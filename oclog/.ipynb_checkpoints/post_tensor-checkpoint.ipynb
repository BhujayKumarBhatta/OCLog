{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0878493-0b36-4b8c-ae1f-5ac4dbf06da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from char_hdfs import HDFSLog\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.random.set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2bfb5f8-9173-425a-a32e-7c2d4ac43054",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_in_line = 256\n",
    "lines_in_seq = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8b84613-5a39-439f-b1f6-14f1f93822e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of lines in the log file: 11175629\n",
      "starting training the tokenizer:\n",
      "ending tokenizer training: 229.5795021057129\n",
      "vocabulary size: 51\n",
      "vocabulary size: 51\n",
      "starting text to number conversion\n",
      "ending text to number conversion: 216.70576786994934\n",
      "ending padding characters: 62.46872520446777\n",
      "padded_txt_to_num shape: (11175629, 256)\n",
      "completed:  0\n",
      "ending blk sequencing: 0.0\n",
      "completed:  1000000\n",
      "ending blk sequencing: 1.9932920932769775\n",
      "completed:  2000000\n",
      "ending blk sequencing: 4.052656173706055\n",
      "completed:  3000000\n",
      "ending blk sequencing: 6.296718597412109\n",
      "completed:  4000000\n",
      "ending blk sequencing: 8.382265567779541\n",
      "completed:  5000000\n",
      "ending blk sequencing: 10.591729640960693\n",
      "completed:  6000000\n",
      "ending blk sequencing: 12.716314554214478\n",
      "completed:  7000000\n",
      "ending blk sequencing: 14.874132871627808\n",
      "completed:  8000000\n",
      "ending blk sequencing: 17.110179901123047\n",
      "completed:  9000000\n",
      "ending blk sequencing: 19.21608829498291\n",
      "completed:  10000000\n",
      "ending blk sequencing: 21.3917076587677\n",
      "completed:  11000000\n",
      "ending blk sequencing: 23.450777292251587\n",
      "15154 1684\n",
      "length of train  sequence original 4\n",
      "length of train  sequence original 2\n",
      "length of train sequence original 64\n",
      "length of train sequence original 64\n",
      "len of test seq after padding 64\n",
      "len of test seq after padding 64\n"
     ]
    }
   ],
   "source": [
    "hdfslog = HDFSLog(padded_char_len=chars_in_line, \n",
    "                  padded_seq_len=lines_in_seq,\n",
    "                 train_ratio=0.9)\n",
    "x_train, y_train, x_test, y_test, tk = hdfslog.get_train_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01cfe279-44ab-4517-b0e3-2377450a0832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 51\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tk.word_index)\n",
    "print(f'vocab_size: {vocab_size}')\n",
    "char_onehot = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c5e5732-11ad-4162-8f5e-f18dda044068",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = []\n",
    "embedding_weights.append(np.zeros(vocab_size))\n",
    "for char, i in tk.word_index.items(): # from 1 to 51\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[i-1] = 1\n",
    "    embedding_weights.append(onehot)\n",
    "embedding_weights = np.array(embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42f51055-8374-4c3c-83e1-e722cdc55f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 51)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(embedding_weights.shape) # first row all 0 for PAD and last row for UNK\n",
    "embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9bd6c39-050d-4605-b8ff-751ba94a0200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_weights[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8479e9da-a5b5-42eb-bdfa-56eb26f425ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = [64, 256]\n",
    "embedding_size = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "60699e05-457a-4a9c-98fc-925f93de7832",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(vocab_size+1,\n",
    "                                            embedding_size,\n",
    "                                            input_length=input_size,\n",
    "                                            weights = [embedding_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "840e782b-73e1-4675-a645-ec075e940234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.embeddings.Embedding at 0x1760ea3fe20>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd684e09-e95a-4157-bb1b-08e221b31e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c0023ab-5c04-4b1f-863a-22611bf3daee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30308, 64, 256)\n",
      "(30308,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d63cc77e-6f00-43fe-bf0a-413c7c2488b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: [[ 4 12  3 ...  0  0  0]\n",
      " [ 4 12  3 ...  0  0  0]\n",
      " [ 4 12  3 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]], label: 1\n"
     ]
    }
   ],
   "source": [
    "print(f'features: {x_train[0]}, label: {y_train[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ec36c92b-eb23-4006-ab13-7ebae8d93313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((64, 256), ()), types: (tf.int32, tf.int64)>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflogs = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "tflogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "08e4be12-77a4-4584-b57e-d6aa4dd6e61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((64, 256, 51), ()), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tflogs = tflogs.map(lambda x, y: (tf.one_hot(x, depth=char_onehot), y))\n",
    "# tflogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ebc8ca87-a60f-45b9-a742-1207011a4491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tflogs = tflogs.map(lambda x, y: (tf.reshape(x, (64, 13056)), y))\n",
    "# tflogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91bc7f6-4dec-4aa7-95dd-37978edfddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tflogs = tflogs.map(lambda x_batch, y_batch: (tf.one_hot(x_batch, depth=char_onehot), y_batch))\n",
    "# tflogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "84a239d0-8270-49a7-9ba1-278692ec8ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 64, 256), (None,)), types: (tf.int32, tf.int64)>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "tflogs = tflogs.batch(batch_size)\n",
    "tflogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "78a1696c-1dc7-4a64-8a36-68e1e6b10060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tflogs= tflogs.shuffle(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd628c4d-5c7d-4c0c-be43-6b96e269ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tflogs = tflogs.map(lambda x_batch, y_batch: (tf.one_hot(x_batch, depth=51), y_batch))\n",
    "# tflogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e8c27ae6-2b86-4581-82d9-8fa7cfad2fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 64, 256) (32,)\n"
     ]
    }
   ],
   "source": [
    "for x in tflogs.take(1):\n",
    "    print(x[0].shape,  x[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dc40f6-ef77-4036-aebb-2bb4cadeb25d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39aed7-324e-4384-895f-7dfe15909374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63ae04-8bf1-472b-9841-6a7395c1fbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3cd7fa-b6a4-46bc-879e-90e4fb28c8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "cc6df525-9045-4501-9b77-e500b0f2de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tflogs = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "tflogs = tflogs.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2f2fcec2-fbfe-4b7f-ad70-13df7a2e3fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data with char embedding: - (ùêµ,ùêøùë†,ùêøùëô,ùê∂ùëí) (None, 64, 256, 51)\n",
      "after conv : -  (ùêµ,ùêøùë†,ùêøùëô,ùê∂ùëô) (None, 64, 256, 64)\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(64,256), dtype='float64' )\n",
    "x = tf.keras.layers.Embedding(input_dim=vocab_size+1,\n",
    "                                output_dim=embedding_size,\n",
    "                                input_length=256,\n",
    "                                weights = [embedding_weights],\n",
    "                                )(inputs)\n",
    "#At first, OneLog embeds all characters into an arbitraryvector space with the dimension ofùê∂ùëí, which is the character em-bedding matrix‚Äôs dimensions, producing an intermediate tensorshape of(ùêµ,ùêøùë†,ùêøùëô,ùê∂ùëí).\n",
    "print('input data with char embedding: - (ùêµ,ùêøùë†,ùêøùëô,ùê∂ùëí)', x.shape)\n",
    "x = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='same')(x)\n",
    "print('after conv : -  (ùêµ,ùêøùë†,ùêøùëô,ùê∂ùëô)',x.shape)\n",
    "\n",
    "# The output is then aggregated by taking the maximums in the third dimension,delivering the intermediate tensor shape of(ùêµ,ùêøùë†,ùê∂ùëô)\n",
    "#(ùêµ,ùêøùë†,ùê∂ùëô)\n",
    "# x = tf.keras.layers.Maximum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "31078b86-ae52-4f74-ad38-ab06b55de7e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\BHUJAY~1\\AppData\\Local\\Temp/ipykernel_13620/2022838141.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m ])\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# print(model.summary())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mevent_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m# model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#               optimizer='adam', metrics=['accuracy'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    980\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 982\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2615\u001b[0m     \u001b[1;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2616\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2617\u001b[1;33m       input_spec.assert_input_compatibility(\n\u001b[0m\u001b[0;32m   2618\u001b[0m           self.input_spec, inputs, self.name)\n\u001b[0;32m   2619\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         spec.max_ndim is not None):\n\u001b[1;32m--> 166\u001b[1;33m       \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[0;32m    168\u001b[0m                          \u001b[0mlayer_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' is incompatible with the layer: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "emb = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size+1,\n",
    "                                embedding_size,\n",
    "                                input_length=256,\n",
    "                                weights = [embedding_weights],\n",
    "                                input_shape=[64, 256 ]),    \n",
    "])\n",
    "# print(model.summary())\n",
    "event_emb = tf.keras.layers.Conv1D(filters=64, kernel_size=3)(emb)\n",
    "# model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "#               optimizer='adam', metrics=['accuracy'])\n",
    "# model.fit(tflogs, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8a7cedbf-83e9-48b1-8794-66d18698a52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_30 (Embedding)     (None, 64, 256, 51)       2652      \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 64, 256, 64)       9856      \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 1048576)           0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 1048577   \n",
      "=================================================================\n",
      "Total params: 1,061,085\n",
      "Trainable params: 1,061,085\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# input_shape=[57, 230, 51])\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size+1,\n",
    "                                embedding_size,\n",
    "                                input_length=256,\n",
    "                                weights = [embedding_weights],\n",
    "                                input_shape=[64, 256 ]),\n",
    "    # tf.keras.layers.Conv1D(64, 3, padding='same',  input_shape=[64, 256, 51]),\n",
    "    tf.keras.layers.Conv1D(64, 3, padding='same', ),\n",
    "    # tf.keras.layers.Maximum(),\n",
    "    # tf.keras.layers.MaxPooling1D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "print(model.summary())\n",
    "# model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "#               optimizer='adam', metrics=['accuracy'])\n",
    "# model.fit(tflogs, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae598b8-fdcd-43af-a660-c11d8709d61b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4330d0db-97b0-4cd2-be9f-920e2125238a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948/948 [==============================] - 266s 280ms/step - loss: 7.6250 - accuracy: 0.4997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1761a36e640>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tflogs, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e7acb-e75b-4537-9361-de708ceb33af",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=8, kernel_size=64, strides=1, padding='causal',\n",
    "                           input_shape=[lines_in_seq, chars_in_line]), \n",
    "    # tf.keras.layers.Conv1D(filters=16, kernel_size=3, strides=1, padding='causal',\n",
    "    #                        ),\n",
    "    # tf.keras.layers.Conv1D(filters=32, kernel_size=2, strides=1, padding='causal',\n",
    "                           # ),\n",
    "    tf.keras.layers.Flatten(), \n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "print(m2.summary())\n",
    "m2.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "history = m2.fit(tflogs, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2708da-ae18-4762-b53a-667174816cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c7d30-0d10-40d9-8ef7-25f81c7ad7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d6d1ff-ea7d-4c85-8bb4-b125fdf233b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa95a0-9221-4ea6-b029-4e362defab29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ccf168-9bb7-4a81-915b-cc420531b162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec3c217-d510-4bb1-a5c3-34014c0d4cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=8, kernel_size=6,  input_shape=[lines_in_seq, chars_in_line]),\n",
    "    # tf.keras.layers.Conv1D(filters=256, kernel_size=2, padding='same', strides=1, activation='relu'),\n",
    "    # # tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    # tf.keras.layers.MaxPooling1D(),\n",
    "    # tf.keras.layers.Conv1D(filters=256, kernel_size=2, padding='same', strides=1, activation='relu'),\n",
    "    # tf.keras.layers.Conv1D(filters=1024, kernel_size=2,  input_shape=[lines_in_seq, chars_in_line]),\n",
    "    # tf.keras.layers.Conv1D(filters=1024, kernel_size=2,  input_shape=[lines_in_seq, chars_in_line]),\n",
    "    # tf.keras.layers.Conv1D(filters=1024, kernel_size=2,  input_shape=[lines_in_seq, chars_in_line]),\n",
    "    # tf.keras.layers.Conv1D(filters=1024, kernel_size=2,  input_shape=[lines_in_seq, chars_in_line]),\n",
    "    # tf.keras.layers.MaxPooling1D(),\n",
    "    tf.keras.layers.Flatten(),      \n",
    "    # tf.keras.layers.Dense(100,),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "print(model_1.summary())\n",
    "model_1.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "history = model_1.fit(tflogs, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab7dad3-d125-4df7-a483-afb8e5eda753",
   "metadata": {},
   "source": [
    "WARNING:tensorflow:Model was constructed with shape (None, 57, 231, 51) for input Tensor(\"conv1d_10_input:0\", shape=(None, 57, 231, 51), dtype=float32), but it was called on an input with incompatible shape (None, 57, 230, 51).\n",
    "WARNING:tensorflow:Model was constructed with shape (None, 57, 231, 51) for input Tensor(\"conv1d_10_input:0\", shape=(None, 57, 231, 51), dtype=float32), but it was called on an input with incompatible shape (None, 57, 230, 51).\n",
    "\n",
    "InvalidArgumentError:  Incompatible shapes: [32,57,229,1] vs. [32,1]\n",
    "\n",
    "ValueError: logits and labels must have the same shape ((None, 57, 229, 1) vs (None, 1))\n",
    "\n",
    "InvalidArgumentError:  Incompatible shapes: [32,1] vs. [32,57,229,1]\n",
    "\n",
    "ValueError: Input 0 of layer max_pooling1d is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, 64, 254, 128]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
