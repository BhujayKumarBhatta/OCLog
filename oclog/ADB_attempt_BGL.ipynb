{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec764c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bglog import BGLog, get_embedding_layer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "469335ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "bglog = BGLog(save_padded_num_sequences=False, load_from_pkl=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a4dce86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded_num_seq_df loaded from data\\bgl_padded_num_seq_df.pkl\n",
      "trained tokenizer, tk, loaded from data\\bgltk.pkl\n",
      "train_0:, 800\n",
      "test_0:, 200\n",
      "train_1:, 800\n",
      "test_1:, 200\n",
      "train_2:, 800\n",
      "test_2:, 200\n",
      "train_3:, 800\n",
      "test_3:, 102\n",
      "4 class does not have 800 records, it has only 628 records\n",
      "test_4:, 0\n",
      "5 class does not have 800 records, it has only 165 records\n",
      "5 class does not have 200 records, it has only 165 records\n",
      "6 class does not have 800 records, it has only 75 records\n",
      "6 class does not have 200 records, it has only 75 records\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "<BatchDataset shapes: ((32, 32, 64), (32, 4)), types: (tf.int32, tf.float32)>\n",
      "<BatchDataset shapes: ((32, 32, 64), (32, 4)), types: (tf.int32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "train_test = bglog.get_tensor_train_test(ablation=1000)\n",
    "train_data, test_data = train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfd0a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(conv1d_set1 = 3, conv1d_set2 = 3, dense_neurons=2048, filters=64,\n",
    "            kernel_size=3,maxpool_1=True,epochs=25, dense_activation='relu'):\n",
    "    embedding_weights, vocab_size, char_onehot = get_embedding_layer(bglog)\n",
    "    B = train_data.element_spec[0].shape[0]\n",
    "    inputs = tf.keras.layers.Input(batch_shape=(B, train_data.element_spec[0].shape[1], train_data.element_spec[0].shape[2]), dtype='float64' )\n",
    "    x = tf.keras.layers.Embedding(input_dim=vocab_size+1,\n",
    "                                    output_dim=vocab_size,\n",
    "                                    input_length=train_data.element_spec[0].shape[2],\n",
    "                                    weights = [embedding_weights],\n",
    "                                    )(inputs)\n",
    "    for _ in range(conv1d_set1):\n",
    "        x = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
    "    if maxpool_1:\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(1, train_data.element_spec[0].shape[2]))(x)\n",
    "        x = tf.reshape(x, (B, train_data.element_spec[0].shape[1], filters))        \n",
    "        for _ in range(conv1d_set2):\n",
    "            x = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size=(train_data.element_spec[0].shape[1]) )(x)\n",
    "        x = tf.reshape(x, (B, filters))\n",
    "    if not maxpool_1:\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "    if dense_activation is None:\n",
    "        x = tf.keras.layers.Dense(dense_neurons)(x)\n",
    "    else:\n",
    "        x = tf.keras.layers.Dense(dense_neurons, activation=dense_activation)(x)\n",
    "    outputs = tf.keras.layers.Dense(train_data.element_spec[1].shape[1], activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    print(model.summary())\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "    hist = model.fit(train_data, validation_data=test_data, epochs=epochs) \n",
    "    return model, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b20edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we  feed  xi  to  a dense layer h to get the log-sequence representation zi∈RD:\n",
    "#     zi= h(xi) =σ(Whxi+bh) ............................(2)\n",
    "# in our case zi can be obtained from the dense layer before the softmax\n",
    "# Lets see how to ger it from the train mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dffc57c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 50\n",
      "Model: \"functional_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(32, 32, 64)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (32, 32, 64, 50)          2550      \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (32, 32, 64, 64)          9664      \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (32, 32, 64, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (32, 32, 64, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (32, 32, 1, 64)           0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (Tenso [(32, 32, 64)]            0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (32, 32, 64)              12352     \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (32, 32, 64)              12352     \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (32, 32, 64)              12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (32, 1, 64)               0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (Tenso [(32, 64)]                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (32, 2048)                133120    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (32, 4)                   8196      \n",
      "=================================================================\n",
      "Total params: 215,290\n",
      "Trainable params: 215,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/6\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 0.2985 - accuracy: 0.8628 - precision_3: 0.9211 - recall_3: 0.8141 - val_loss: 0.0304 - val_accuracy: 0.9836 - val_precision_3: 0.9836 - val_recall_3: 0.9836\n",
      "Epoch 2/6\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 0.0492 - accuracy: 0.9822 - precision_3: 0.9822 - recall_3: 0.9819 - val_loss: 0.0232 - val_accuracy: 0.9896 - val_precision_3: 0.9896 - val_recall_3: 0.9896\n",
      "Epoch 3/6\n",
      "100/100 [==============================] - 9s 90ms/step - loss: 0.0288 - accuracy: 0.9903 - precision_3: 0.9909 - recall_3: 0.9903 - val_loss: 0.0139 - val_accuracy: 0.9970 - val_precision_3: 0.9970 - val_recall_3: 0.9955\n",
      "Epoch 4/6\n",
      "100/100 [==============================] - 9s 93ms/step - loss: 0.0168 - accuracy: 0.9944 - precision_3: 0.9944 - recall_3: 0.9944 - val_loss: 0.0160 - val_accuracy: 0.9955 - val_precision_3: 0.9955 - val_recall_3: 0.9955\n",
      "Epoch 5/6\n",
      "100/100 [==============================] - 9s 89ms/step - loss: 0.0204 - accuracy: 0.9941 - precision_3: 0.9941 - recall_3: 0.9941 - val_loss: 0.0147 - val_accuracy: 0.9955 - val_precision_3: 0.9985 - val_recall_3: 0.9955\n",
      "Epoch 6/6\n",
      "100/100 [==============================] - 9s 89ms/step - loss: 0.0158 - accuracy: 0.9944 - precision_3: 0.9947 - recall_3: 0.9944 - val_loss: 0.0127 - val_accuracy: 0.9970 - val_precision_3: 0.9985 - val_recall_3: 0.9970\n"
     ]
    }
   ],
   "source": [
    "# we pre-train the model with labeled known intent samples. \n",
    "# In order to better reflect the effectiveness of the learned decision boundary, \n",
    "# we learn the feature representation zi with the simple softmax loss Ls to perform classification:\n",
    "\n",
    "trained_model, hist = model(epochs=6,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea902bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn the decision boundary of each class constraining the known labels within a ball area\n",
    "# how to get zi and how to know that zi belongs to which yi ?\n",
    "# from there we will have to calculate the Ck , centroid for the class k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21815bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x1b2cb5147c0>,\n",
       " <tensorflow.python.keras.layers.embeddings.Embedding at 0x1b2d74eb850>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv1D at 0x1b2cb50bb20>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv1D at 0x1b2d74ebfa0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv1D at 0x1b2d74dacd0>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1b2d74b9460>,\n",
       " <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x1b2b10bc790>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv1D at 0x1b2d74ee160>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv1D at 0x1b2b10bc400>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv1D at 0x1b2b10c2e80>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling1D at 0x1b2b10c7580>,\n",
       " <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer at 0x1b2b10dcd00>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1b2b10d7c10>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1b2b10dcfa0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76a80bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.layers.core.Dense object at 0x000001B2B10D7C10>\n"
     ]
    }
   ],
   "source": [
    "dense_6 = trained_model.get_layer('dense_6')\n",
    "print(dense_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c8fc57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_6/Relu:0' shape=(32, 2048) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_6.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfbdbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we use the pre-trained model to extract intent features for \n",
    "# learning the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae80d1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_metric_torch(a, b):\n",
    "    n = a.shape[0]\n",
    "    m = b.shape[0]\n",
    "    a = a.unsqueeze(1).expand(n, m, -1)\n",
    "    b = b.unsqueeze(0).expand(n, m, -1)\n",
    "    logits = -((a - b)**2).sum(dim=2)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b26e8813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [[0 1 2]\n",
      " [3 4 5]]\n",
      "a.shape (2, 3)\n",
      "b [ 8  9 10 11 12 13 14 15]\n",
      "b [[ 8  9]\n",
      " [10 11]\n",
      " [12 13]\n",
      " [14 15]]\n",
      "b.shape: (4, 2)\n",
      "tfa tf.Tensor(\n",
      "[[0 1 2]\n",
      " [3 4 5]], shape=(2, 3), dtype=int32)\n",
      "tfb tf.Tensor(\n",
      "[[ 8  9]\n",
      " [10 11]\n",
      " [12 13]\n",
      " [14 15]], shape=(4, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "a = np.arange(6)\n",
    "a = a.reshape((2, -1))\n",
    "print('a:', a)\n",
    "print('a.shape', a.shape)\n",
    "b = np.arange(8, 16)\n",
    "print('b',b)\n",
    "b = np.reshape(b, (4, -1))\n",
    "print('b',b)\n",
    "print('b.shape:',b.shape)\n",
    "tfa = tf.constant(a)\n",
    "tfb = tf.constant(b)\n",
    "print('tfa',tfa)\n",
    "print('tfb',tfb)\n",
    "# n = tfa.shape[0]\n",
    "# m = b.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c48ec212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.expand_dims(tfa, 0) : tf.Tensor(\n",
      "[[[0 1 2]\n",
      "  [3 4 5]]], shape=(1, 2, 3), dtype=int32)\n",
      "\n",
      "tf.expand_dims(tfa, 1) : tf.Tensor(\n",
      "[[[0 1 2]]\n",
      "\n",
      " [[3 4 5]]], shape=(2, 1, 3), dtype=int32)\n",
      "\n",
      "tf.expand_dims(tfa, 1) : tf.Tensor(\n",
      "[[[0]\n",
      "  [1]\n",
      "  [2]]\n",
      "\n",
      " [[3]\n",
      "  [4]\n",
      "  [5]]], shape=(2, 3, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print('tf.expand_dims(tfa, 0) :',tf.expand_dims(tfa, 0))\n",
    "print()\n",
    "print('tf.expand_dims(tfa, 1) :',tf.expand_dims(tfa, 1))\n",
    "print()\n",
    "print('tf.expand_dims(tfa, 1) :',tf.expand_dims(tfa, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d23ee17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.shape(tfa): [2 1 3]\n",
      "tf.shape(tfb): [1 4 2]\n"
     ]
    }
   ],
   "source": [
    "tfa = tf.expand_dims(tfa, 1)\n",
    "print(f'tf.shape(tfa): {tf.shape(tfa)}')\n",
    "tfb = tf.expand_dims(tfb, 0)\n",
    "print(f'tf.shape(tfb): {tf.shape(tfb)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f5ff6d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfa_broadcast tf.Tensor(\n",
      "[[[0 1 2]\n",
      "  [0 1 2]\n",
      "  [0 1 2]\n",
      "  [0 1 2]]\n",
      "\n",
      " [[3 4 5]\n",
      "  [3 4 5]\n",
      "  [3 4 5]\n",
      "  [3 4 5]]], shape=(2, 4, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "n = a.shape[0]\n",
    "m = b.shape[0]\n",
    "tfa_broadcast = tf.broadcast_to(tfa, [2, 4, 3])\n",
    "tf.shape(tfa_broadcast)\n",
    "print('tfa_broadcast',tfa_broadcast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c14f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "\n",
    "# The equivalent function for pytorch expand is tensorflow tf.broadcast_to\n",
    "\n",
    "# Docs: https://www.tensorflow.org/api_docs/python/tf/broadcast_to\n",
    "\n",
    "# Share\n",
    "# Follow\n",
    "# edited Oct 23, 2021 at 18:22\n",
    "\n",
    "# M.Innat\n",
    "# 12.2k66 gold badges3434 silver badges6767 bronze badges\n",
    "# answered Jan 4, 2019 at 9:12\n",
    "\n",
    "# funkyyyyyy\n",
    "# 6111 silver badge22 bronze badges\n",
    "# Add a comment\n",
    "\n",
    "# 0\n",
    "\n",
    "# Tensorflow automatically broadcasts, so in general you don't need to do any of this. Suppose you have a y' of shape 6x2x3 and your x is of shape 2x3, then you can already do y'*x or y'+x will already behave as if you had expanded it. But if for some other reason you really need to do it, then the command in tensorflow is tile:\n",
    "\n",
    "# y = tf.tile(tf.reshape(x, (1,2,3)), multiples=(6,1,1))\n",
    "# Docs: https://www.tensorflow.org/api_docs/python/tf/tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876ebf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_metric(a, b):\n",
    "    n = a.shape[0]\n",
    "    m = b.shape[0]\n",
    "    a = tf.expand_dims(a, 1)\n",
    "    b = tf.expand_dims(b, 0)\n",
    "    logits = -((a - b)**2).sum(dim=2)\n",
    "    return logits  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2c6a44d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenSet:\n",
    "    def __init__(self, data, pretrained_model):\n",
    "        \n",
    "        self.model = pretrained_model\n",
    "        self.best_eval_score = 0\n",
    "        self.delta = None\n",
    "        self.delta_points = []\n",
    "        self.centroids = None\n",
    "        self.test_results = None\n",
    "        self.predictions = None\n",
    "        self.true_labels = None\n",
    "        \n",
    "    def centroids_cal(self):\n",
    "        centriods = np.zeros(train_data.element_spec[1].shape[1], embedding_size)\n",
    "        total_labels = np.empty(0, dtype=longdouble)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd05e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2608c89c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ccb90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a2dcae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0580230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In context of deep learning the logits layer means the layer that feeds in to softmax (or other such normalization). The output of the softmax are the probabilities for the classification task and its input is logits layer. The logits layer typically produces values from -infinity to +infinity and the softmax layer transforms it to values from 0 to 1.\n",
    "\n",
    "# Historical Context\n",
    "\n",
    "# Where does this term comes from? In 1930s and 40s, several people were trying to adapt linear regression to the problem of predicting probabilities. However linear regression produces output from -infinity to +infinity while for probabilities our desired output is 0 to 1. One way to do this is by somehow mapping the probabilities 0 to 1 to -infinity to +infinity and then use linear regression as usual. One such mapping is cumulative normal distribution that was used by Chester Ittner Bliss in 1934 and he called this \"probit\" model, short for \"probability unit\". However this function is computationally expensive while lacking some of the desirable properties for multi-class classification. In 1944 Joseph Berkson used the function log(p/(1-p)) to do this mapping and called it logit, short for \"logistic unit\". The term logistic regression derived from this as well.\n",
    "\n",
    "# The Confusion\n",
    "\n",
    "# Unfortunately the term logits is abused in deep learning. From pure mathematical perspective logit is a function that performs above mapping. In deep learning people started calling the layer \"logits layer\" that feeds in to logit function. Then people started calling the output values of this layer \"logit\" creating the confusion with logit the function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
