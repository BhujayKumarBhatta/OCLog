{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "824fd43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import psutil\n",
    "import time\n",
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from itertools  import groupby\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91c63592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "total loglines: 4747963\n"
     ]
    }
   ],
   "source": [
    "Logpath='C:\\ML_data\\Logs'\n",
    "labelpath='C:\\ML_data\\Logs'\n",
    "# print(os.listdir(Logpath))\n",
    "logfilename='BGL.log'\n",
    "bglfile = os.path.join(Logpath, logfilename)\n",
    "print(os.path.exists(bglfile))\n",
    "with open(bglfile, 'r',  encoding='utf8') as f:\n",
    "    bglraw = f.readlines()\n",
    "print(f'total loglines: {len(bglraw)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f534c249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alerts 4747963\n",
      "unique_alerts: {'ERROR', 'FATAL', 'INFO', '0x00544eb8,', 'microseconds', 'SEVERE', 'Kill', 'WARNING', 'single', 'FAILURE'}\n",
      "148373\n",
      "elapsed time: 3.3453094959259033\n",
      "INFO       114115\n",
      "FATAL       28439\n",
      "ERROR        4049\n",
      "WARNING       902\n",
      "SEVERE        628\n",
      "Kill          165\n",
      "FAILURE        75\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# get all the alerts in the list. total number of alerts should match with the \n",
    "#  total number of loglines\n",
    "alerts =  [l.split()[8] for l in bglraw]\n",
    "print('alerts',len(alerts))\n",
    "unique_alerts = set(alerts)\n",
    "print(f'unique_alerts: {unique_alerts}')\n",
    "negative_alerts = ['FATAL', 'SEVERE', 'WARNING', 'Kill', 'FAILURE', 'ERROR']\n",
    "sequence_len = 32\n",
    "sequences = [bglraw[i * sequence_len:(i + 1) * sequence_len] for i in range((len(bglraw)) // sequence_len )] \n",
    "print(len(sequences))\n",
    "\n",
    "stime = time.time()\n",
    "labelled_sequences = []\n",
    "for seq in sequences:    \n",
    "    label = 'INFO'\n",
    "    for s in seq:\n",
    "        if s.split()[8] in negative_alerts:\n",
    "            label = s.split()[8]\n",
    "    labelled_sequences.append((seq, label))          \n",
    "etime = time.time()\n",
    "print(f'elapsed time: {etime - stime}')\n",
    "df = pd.DataFrame(labelled_sequences, columns=['sequence', 'label'])\n",
    "print(df.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f52380ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_bgl(txt_line, clean_part_1=True, clean_part_2=True, clean_time_1=True, clean_part_4=True, clean_time_2=True, clean_part_6=True):\n",
    "    part_1 = ''\n",
    "    part_2 = ''\n",
    "    time_1 = ''\n",
    "    part_4 = ''\n",
    "    time_2 = ''\n",
    "    part_6 = ''\n",
    "    if clean_part_1:\n",
    "        part_1 = '^-\\s|^\\w+\\s'\n",
    "    if clean_part_2:\n",
    "        part_2 = '\\d{10}\\s'\n",
    "    if clean_time_1:\n",
    "        time_1 = '\\d{4}\\.\\d{2}\\.\\d{2}\\s'\n",
    "    if clean_part_4:\n",
    "        part_4 = '\\w\\d{2}-\\w\\d-\\w{2}-\\w:\\w\\d{2}-\\w\\d{2}\\s'\n",
    "    if clean_time_2:\n",
    "        time_2 = '\\d{4}-\\d{2}-\\d{2}-\\d{2}\\.\\d{2}\\.\\d{2}\\.\\d{6}\\s'\n",
    "    if clean_part_6:\n",
    "        part_6 = 'RAS'\n",
    "    part_7 = '[\\n]'\n",
    "    signs_n_punctuations = '\\]|\\[|\\)|\\(|\\=|\\,|\\;|\\/|\\{|\\}[$]|[@]|[#]|[%]|[_]|[*]|[&]|[ï]|[ã]|[`]|[ð]|[-]|[\\x0f]|[\\x00]|[\\x10]|[\\x98]|[ç]|[:]|\\''\n",
    "#     signs_n_punctuations = '\\]|\\[|\\)|\\(|\\=|\\,|\\;|\\/|\\{|\\}\\$\\@\\#\\%\\_\\*\\&\\|ï|ã|`|ð|\\-'\n",
    "#     signs_n_punctuations = '\\]|\\[|\\)|\\(|\\=|\\,|\\;|\\/|\\{|\\}[$]|[@]|[#]|[%]|[_]|[*]|[&]|[ï]|[ã]|`|ð|[-]|[\\x0f]|[\\x00]|[\\x10]|[\\x98]|[ç]|[:]'\n",
    "    white_space = '\\s'\n",
    "    multiple_dots = '\\.+?'\n",
    "    pat =f'{part_1}|{part_2}|{time_1}|{part_4}|{time_2}|{part_6}\\s|{part_7}|{signs_n_punctuations}|{white_space}|{multiple_dots}'\n",
    "#     signs_n_punctuations = '\\]|\\[|\\)|\\(|\\=|\\,|\\;|\\/'\n",
    "#     pat =f'{part_1}|{part_2}|{time_1}|{part_4}|{time_2}|{part_6}\\s|{part_7}|{signs_n_punctuations}'\n",
    "    s = re.sub(pat, '', txt_line)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "290ed332",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_labelled_sequences = []\n",
    "for sequence, label in labelled_sequences:\n",
    "    cleaned_seq = []\n",
    "    for line in sequence:\n",
    "        cleaned_line = clean_bgl(line)\n",
    "        cleaned_line = cleaned_line.lower()\n",
    "        cleaned_seq.append(cleaned_line)\n",
    "    cleaned_labelled_sequences.append((cleaned_seq, label)) \n",
    "# cleaned_labelled_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f55cff23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character vocabulary 50\n"
     ]
    }
   ],
   "source": [
    "whole_text_for_training = [line for sequence, _ in cleaned_labelled_sequences for line in sequence]\n",
    "len(whole_text_for_training)\n",
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "tk.fit_on_texts(whole_text_for_training)\n",
    "print('character vocabulary', len(tk.word_index))\n",
    "# print(tk.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c3b1c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_char_len=64\n",
    "padding_style='post'\n",
    "truncating='pre'\n",
    "num_sequences = []\n",
    "for seq, label in cleaned_labelled_sequences:\n",
    "    num_seq = []\n",
    "    for sline in seq:\n",
    "        try:        \n",
    "            num_line = tk.texts_to_sequences([sline])\n",
    "            padded_num_line = pad_sequences(num_line, maxlen=padded_char_len, \n",
    "                                                  padding=padding_style, truncating=truncating)\n",
    "            num_seq.append(padded_num_line[0])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('line:', sline)   \n",
    "            break\n",
    "    num_sequences.append((num_seq, label)) \n",
    "# num_sequences[0]\n",
    "print(len(num_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aea4a843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    114115\n",
      "1     28439\n",
      "2      4049\n",
      "3       902\n",
      "4       628\n",
      "5       165\n",
      "6        75\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "numdf = pd.DataFrame(num_sequences, columns=['seq', 'label'])\n",
    "# numdf.head()\n",
    "numdf[\"label\"].replace({\"INFO\": \"0\", \"FATAL\": \"1\", \"ERROR\": \"2\", \n",
    "                     \"WARNING\": \"3\", \"SEVERE\": \"4\", \"Kill\": \"5\",\n",
    "                    \"FAILURE\": \"6\"}, inplace=True)\n",
    "print(numdf.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74b93305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_class(bgldf, ablation = 4000, train_test_ratio = 0.8, label='NORMALBGL'):\n",
    "    train_data=None\n",
    "    test_data=None\n",
    "    train_cnt = round(ablation * train_test_ratio)\n",
    "    test_cnt = round(ablation * (1 - train_test_ratio))\n",
    "\n",
    "    if train_cnt <= bgldf[bgldf.label==label].count()[0] :\n",
    "      train_data = bgldf[bgldf.label==label][0:train_cnt]\n",
    "    else:\n",
    "        print(f'{label} class does not have {train_cnt} records, it has only {bgldf[bgldf.label==label].count()[0]} records')\n",
    "    if test_cnt <= bgldf[bgldf.label==label].count()[0] :\n",
    "      test_data = bgldf[bgldf.label==label][train_cnt:ablation]\n",
    "    else:\n",
    "        print(f'{label} class does not have {test_cnt} records, it has only {bgldf[bgldf.label==label].count()[0]} records')\n",
    "    if train_data is not None:\n",
    "        print(f'train_{label}:, {train_data.count()[0]}')\n",
    "    if test_data is not None:\n",
    "        print(f'test_{label}:, {test_data.count()[0]}')\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "# classes = ['NORMALBGL', 'FATALBGL', 'ERRORBGL', 'WARNINGBGL','SEVEREBGL', 'KillBGL', 'FAILUREBGL', ] \n",
    "classes = ['0', '1', '2', '3','4', '5', '6', ] \n",
    "def train_test_multi_class(bgldf, ablation=4000, train_test_ratio=0.8, classes=classes):\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    for class_name in classes:\n",
    "            trdata, tsdata = train_test_split_class(bgldf, ablation=ablation, \n",
    "                                                    train_test_ratio=train_test_ratio, \n",
    "                                                    label=class_name)\n",
    "            if trdata is not None: train_data.append(trdata)\n",
    "            if tsdata is not None: test_data.append(tsdata)\n",
    "    \n",
    "    train_df = pd.concat(train_data)\n",
    "    test_df = pd.concat(test_data)\n",
    "    return train_df, test_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e92514f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_0:, 80\n",
      "test_0:, 20\n",
      "train_1:, 80\n",
      "test_1:, 20\n",
      "train_2:, 80\n",
      "test_2:, 20\n",
      "train_3:, 80\n",
      "test_3:, 20\n",
      "train_4:, 80\n",
      "test_4:, 20\n",
      "train_5:, 80\n",
      "test_5:, 20\n",
      "6 class does not have 80 records, it has only 75 records\n",
      "test_6:, 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    80\n",
       "1    80\n",
       "2    80\n",
       "3    80\n",
       "4    80\n",
       "5    80\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_multi_class(numdf, ablation=100)\n",
    "train_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ee515c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x_train = list(train_df.seq.values)\n",
    "y_train = list(train_df.label.values)\n",
    "y_train = to_categorical(y_train)\n",
    "print(y_train[:2])\n",
    "x_test = list(test_df.seq.values)\n",
    "y_test = list(test_df.label.values)\n",
    "y_test = to_categorical(y_test)\n",
    "print(y_test[:2])\n",
    "print(y_train[80:82])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "858b8c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 50\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tk.word_index)\n",
    "vocab_size = len(tk.word_index)\n",
    "print(f'vocab_size: {vocab_size}')\n",
    "char_onehot = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "247e25b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((32, 32, 64), (32, 6)), types: (tf.int32, tf.float32)>\n",
      "<BatchDataset shapes: ((32, 32, 64), (32, 6)), types: (tf.int32, tf.float32)>\n",
      "64\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "B=32\n",
    "# train_data = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.shuffle(buffer_size=y_train.shape[0]).batch(B, drop_remainder=True)\n",
    "print(train_data)\n",
    "    \n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_data = test_data.shuffle(buffer_size=y_test.shape[0]).batch(B, drop_remainder=True)\n",
    "print(test_data)\n",
    "\n",
    "print(train_data.element_spec[0].shape[2])\n",
    "print(train_data.element_spec[1].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84123239",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = []\n",
    "embedding_weights.append(np.zeros(vocab_size))\n",
    "for char, i in tk.word_index.items(): # from 1 to 51\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[i-1] = 1\n",
    "    embedding_weights.append(onehot)\n",
    "embedding_weights = np.array(embedding_weights)\n",
    "    \n",
    "input_size =[ train_data.element_spec[0].shape[1], train_data.element_spec[0].shape[2]]\n",
    "embedding_size = vocab_size\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(vocab_size+1,\n",
    "                                                embedding_size,\n",
    "                                                input_length=input_size,\n",
    "                                                weights = [embedding_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc84b824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(32, 32, 64)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (32, 32, 64, 50)          2550      \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (32, 32, 64, 64)          9664      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (32, 32, 64, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (32, 32, 64, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (32, 32, 1, 64)           0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorF [(32, 32, 64)]            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (32, 32, 64)              12352     \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (32, 32, 64)              12352     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (32, 32, 64)              12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (32, 1, 64)               0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (Tenso [(32, 64)]                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (32, 2048)                133120    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (32, 6)                   12294     \n",
      "=================================================================\n",
      "Total params: 219,388\n",
      "Trainable params: 219,388\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/25\n",
      "15/15 [==============================] - 2s 159ms/step - loss: 1.2600 - accuracy: 0.5125 - precision: 0.9020 - recall: 0.1917 - val_loss: 0.9994 - val_accuracy: 0.4792 - val_precision: 0.8837 - val_recall: 0.3958\n",
      "Epoch 2/25\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.4072 - accuracy: 0.8604 - precision: 0.9102 - recall: 0.8021 - val_loss: 0.4142 - val_accuracy: 0.8229 - val_precision: 0.8352 - val_recall: 0.7917\n",
      "Epoch 3/25\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.1961 - accuracy: 0.9292 - precision: 0.9343 - recall: 0.9187 - val_loss: 0.7150 - val_accuracy: 0.6979 - val_precision: 0.6979 - val_recall: 0.6979\n",
      "Epoch 4/25\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.1616 - accuracy: 0.9542 - precision: 0.9542 - recall: 0.9542 - val_loss: 1.0126 - val_accuracy: 0.7604 - val_precision: 0.7692 - val_recall: 0.7292\n",
      "Epoch 5/25\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 0.0846 - accuracy: 0.9729 - precision: 0.9749 - recall: 0.9708 - val_loss: 1.3103 - val_accuracy: 0.7604 - val_precision: 0.7604 - val_recall: 0.7604\n",
      "Epoch 6/25\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.0599 - accuracy: 0.9896 - precision: 0.9916 - recall: 0.9896 - val_loss: 0.3904 - val_accuracy: 0.7917 - val_precision: 0.8000 - val_recall: 0.7917\n",
      "Epoch 7/25\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.0245 - accuracy: 0.9917 - precision: 0.9958 - recall: 0.9917 - val_loss: 0.4045 - val_accuracy: 0.8750 - val_precision: 0.8750 - val_recall: 0.8750\n",
      "Epoch 8/25\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.0139 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 1.2492 - val_accuracy: 0.8021 - val_precision: 0.8021 - val_recall: 0.8021\n",
      "Epoch 9/25\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.0571 - accuracy: 0.9812 - precision: 0.9812 - recall: 0.9812 - val_loss: 0.3459 - val_accuracy: 0.8750 - val_precision: 0.8750 - val_recall: 0.8750\n",
      "Epoch 10/25\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.0096 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.4893 - val_accuracy: 0.8542 - val_precision: 0.8542 - val_recall: 0.8542\n",
      "Epoch 11/25\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.0050 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.5750 - val_accuracy: 0.8438 - val_precision: 0.8438 - val_recall: 0.8438\n",
      "Epoch 12/25\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.0016 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.4583 - val_accuracy: 0.8750 - val_precision: 0.8750 - val_recall: 0.8750\n",
      "Epoch 13/25\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.0012 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.5308 - val_accuracy: 0.8750 - val_precision: 0.8750 - val_recall: 0.8750\n",
      "Epoch 14/25\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 9.0220e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.5435 - val_accuracy: 0.8542 - val_precision: 0.8542 - val_recall: 0.8542\n",
      "Epoch 15/25\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 7.2698e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.4568 - val_accuracy: 0.8646 - val_precision: 0.8646 - val_recall: 0.8646\n",
      "Epoch 16/25\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 5.8570e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.6280 - val_accuracy: 0.8438 - val_precision: 0.8438 - val_recall: 0.8438\n",
      "Epoch 17/25\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 5.1227e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.5320 - val_accuracy: 0.8542 - val_precision: 0.8542 - val_recall: 0.8542\n",
      "Epoch 18/25\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 4.5684e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.6408 - val_accuracy: 0.8333 - val_precision: 0.8333 - val_recall: 0.8333\n",
      "Epoch 19/25\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 4.0608e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.5048 - val_accuracy: 0.8750 - val_precision: 0.8750 - val_recall: 0.8750\n",
      "Epoch 20/25\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 3.6699e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.5714 - val_accuracy: 0.8646 - val_precision: 0.8646 - val_recall: 0.8646\n",
      "Epoch 21/25\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 3.2802e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.4225 - val_accuracy: 0.8750 - val_precision: 0.8750 - val_recall: 0.8750\n",
      "Epoch 22/25\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 3.0451e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.5266 - val_accuracy: 0.8750 - val_precision: 0.8750 - val_recall: 0.8750\n",
      "Epoch 23/25\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 2.7485e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.4986 - val_accuracy: 0.8854 - val_precision: 0.8854 - val_recall: 0.8854\n",
      "Epoch 24/25\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 2.5597e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.4942 - val_accuracy: 0.8750 - val_precision: 0.8750 - val_recall: 0.8750\n",
      "Epoch 25/25\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 2.3482e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.6269 - val_accuracy: 0.8542 - val_precision: 0.8542 - val_recall: 0.8542\n"
     ]
    }
   ],
   "source": [
    "conv1d_set1 = 3\n",
    "conv1d_set2 = 3\n",
    "dense_neurons=2048\n",
    "filters=64\n",
    "kernel_size=3\n",
    "maxpool_1=True\n",
    "epochs=25\n",
    "\n",
    "inputs = tf.keras.layers.Input(batch_shape=(B, train_data.element_spec[0].shape[1], train_data.element_spec[0].shape[2]), dtype='float64' )\n",
    "x = tf.keras.layers.Embedding(input_dim=vocab_size+1,\n",
    "                                output_dim=embedding_size,\n",
    "                                input_length=train_data.element_spec[0].shape[2],\n",
    "                                weights = [embedding_weights],\n",
    "                                )(inputs)\n",
    "for _ in range(conv1d_set1):\n",
    "    x = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
    "if maxpool_1:\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(1, train_data.element_spec[0].shape[2]))(x)\n",
    "    x = tf.reshape(x, (B, train_data.element_spec[0].shape[1], filters))        \n",
    "    for _ in range(conv1d_set2):\n",
    "        x = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=(train_data.element_spec[0].shape[1]) )(x)\n",
    "    x = tf.reshape(x, (B, filters))\n",
    "if not maxpool_1:\n",
    "    x = tf.keras.layers.Flatten()(x)       \n",
    "x = tf.keras.layers.Dense(dense_neurons)(x)\n",
    "outputs = tf.keras.layers.Dense(train_data.element_spec[1].shape[1], activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "print(model.summary())\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy',\n",
    "          metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "hist = model.fit(train_data, validation_data=test_data, epochs=epochs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a85e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb1d482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f5db42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_0:, 800\n",
      "test_0:, 200\n",
      "train_1:, 800\n",
      "test_1:, 200\n",
      "train_2:, 800\n",
      "test_2:, 200\n",
      "train_3:, 800\n",
      "test_3:, 102\n",
      "4 class does not have 800 records, it has only 628 records\n",
      "test_4:, 0\n",
      "5 class does not have 800 records, it has only 165 records\n",
      "5 class does not have 200 records, it has only 165 records\n",
      "6 class does not have 800 records, it has only 75 records\n",
      "6 class does not have 200 records, it has only 75 records\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    800\n",
       "1    800\n",
       "2    800\n",
       "3    800\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_multi_class(numdf, ablation=1000)\n",
    "train_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b274b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x_train = list(train_df.seq.values)\n",
    "y_train = list(train_df.label.values)\n",
    "y_train = to_categorical(y_train)\n",
    "print(y_train[:2])\n",
    "x_test = list(test_df.seq.values)\n",
    "y_test = list(test_df.label.values)\n",
    "y_test = to_categorical(y_test)\n",
    "print(y_test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e973945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((32, 32, 64), (32, 4)), types: (tf.int32, tf.float32)>\n",
      "<BatchDataset shapes: ((32, 32, 64), (32, 4)), types: (tf.int32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "B=32\n",
    "# train_data = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.shuffle(buffer_size=y_train.shape[0]).batch(B, drop_remainder=True)\n",
    "print(train_data)\n",
    "    \n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_data = test_data.shuffle(buffer_size=y_test.shape[0]).batch(B, drop_remainder=True)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc158039",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = []\n",
    "embedding_weights.append(np.zeros(vocab_size))\n",
    "for char, i in tk.word_index.items(): # from 1 to 51\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[i-1] = 1\n",
    "    embedding_weights.append(onehot)\n",
    "embedding_weights = np.array(embedding_weights)\n",
    "    \n",
    "input_size =[ train_data.element_spec[0].shape[1], train_data.element_spec[0].shape[2]]\n",
    "embedding_size = vocab_size\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(vocab_size+1,\n",
    "                                                embedding_size,\n",
    "                                                input_length=input_size,\n",
    "                                                weights = [embedding_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d937d3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(32, 32, 64)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (32, 32, 64, 50)          2550      \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (32, 32, 64, 64)          9664      \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (32, 32, 64, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (32, 32, 64, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (32, 32, 1, 64)           0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (Tenso [(32, 32, 64)]            0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (32, 32, 64)              12352     \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (32, 32, 64)              12352     \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (32, 32, 64)              12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (32, 1, 64)               0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (Tenso [(32, 64)]                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (32, 2048)                133120    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (32, 4)                   8196      \n",
      "=================================================================\n",
      "Total params: 215,290\n",
      "Trainable params: 215,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/25\n",
      "100/100 [==============================] - 9s 88ms/step - loss: 0.2665 - accuracy: 0.8894 - precision_1: 0.9302 - recall_1: 0.8450 - val_loss: 0.0213 - val_accuracy: 0.9911 - val_precision_1: 0.9925 - val_recall_1: 0.9896\n",
      "Epoch 2/25\n",
      "100/100 [==============================] - 8s 84ms/step - loss: 0.0344 - accuracy: 0.9869 - precision_1: 0.9875 - recall_1: 0.9866 - val_loss: 0.0182 - val_accuracy: 0.9911 - val_precision_1: 0.9940 - val_recall_1: 0.9911\n",
      "Epoch 3/25\n",
      "100/100 [==============================] - 9s 86ms/step - loss: 0.0218 - accuracy: 0.9922 - precision_1: 0.9928 - recall_1: 0.9922 - val_loss: 0.0074 - val_accuracy: 0.9955 - val_precision_1: 0.9985 - val_recall_1: 0.9955\n",
      "Epoch 4/25\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 0.0353 - accuracy: 0.9884 - precision_1: 0.9884 - recall_1: 0.9884 - val_loss: 0.0519 - val_accuracy: 0.9836 - val_precision_1: 0.9836 - val_recall_1: 0.9836\n",
      "Epoch 5/25\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 0.0325 - accuracy: 0.9903 - precision_1: 0.9903 - recall_1: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9911 - val_precision_1: 0.9911 - val_recall_1: 0.9911\n",
      "Epoch 6/25\n",
      "100/100 [==============================] - 10s 104ms/step - loss: 0.0247 - accuracy: 0.9931 - precision_1: 0.9931 - recall_1: 0.9928 - val_loss: 0.0076 - val_accuracy: 0.9970 - val_precision_1: 0.9970 - val_recall_1: 0.9970\n",
      "Epoch 7/25\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 0.0065 - accuracy: 0.9981 - precision_1: 0.9981 - recall_1: 0.9981 - val_loss: 0.0090 - val_accuracy: 0.9970 - val_precision_1: 0.9970 - val_recall_1: 0.9970\n",
      "Epoch 8/25\n",
      "100/100 [==============================] - 9s 89ms/step - loss: 0.0136 - accuracy: 0.9962 - precision_1: 0.9962 - recall_1: 0.9962 - val_loss: 0.0018 - val_accuracy: 1.0000 - val_precision_1: 1.0000 - val_recall_1: 1.0000\n",
      "Epoch 9/25\n",
      "100/100 [==============================] - 9s 91ms/step - loss: 0.0120 - accuracy: 0.9972 - precision_1: 0.9972 - recall_1: 0.9972 - val_loss: 0.0355 - val_accuracy: 0.9926 - val_precision_1: 0.9926 - val_recall_1: 0.9926\n",
      "Epoch 10/25\n",
      "100/100 [==============================] - 9s 90ms/step - loss: 0.0135 - accuracy: 0.9950 - precision_1: 0.9956 - recall_1: 0.9950 - val_loss: 0.0025 - val_accuracy: 0.9985 - val_precision_1: 0.9985 - val_recall_1: 0.9985\n",
      "Epoch 11/25\n",
      "100/100 [==============================] - 8s 84ms/step - loss: 0.0081 - accuracy: 0.9975 - precision_1: 0.9975 - recall_1: 0.9975 - val_loss: 0.0087 - val_accuracy: 0.9955 - val_precision_1: 0.9970 - val_recall_1: 0.9955\n",
      "Epoch 12/25\n",
      "100/100 [==============================] - 9s 88ms/step - loss: 3.2770e-04 - accuracy: 1.0000 - precision_1: 1.0000 - recall_1: 1.0000 - val_loss: 0.0044 - val_accuracy: 0.9970 - val_precision_1: 0.9970 - val_recall_1: 0.9970\n",
      "Epoch 13/25\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 6.5345e-05 - accuracy: 1.0000 - precision_1: 1.0000 - recall_1: 1.0000 - val_loss: 0.0052 - val_accuracy: 0.9970 - val_precision_1: 0.9970 - val_recall_1: 0.9970\n",
      "Epoch 14/25\n",
      "100/100 [==============================] - 8s 85ms/step - loss: 3.6316e-05 - accuracy: 1.0000 - precision_1: 1.0000 - recall_1: 1.0000 - val_loss: 0.0055 - val_accuracy: 0.9970 - val_precision_1: 0.9970 - val_recall_1: 0.9970\n",
      "Epoch 15/25\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 2.6916e-05 - accuracy: 1.0000 - precision_1: 1.0000 - recall_1: 1.0000 - val_loss: 0.0055 - val_accuracy: 0.9970 - val_precision_1: 0.9970 - val_recall_1: 0.9970\n",
      "Epoch 16/25\n",
      "100/100 [==============================] - 10s 104ms/step - loss: 2.1235e-05 - accuracy: 1.0000 - precision_1: 1.0000 - recall_1: 1.0000 - val_loss: 0.0057 - val_accuracy: 0.9970 - val_precision_1: 0.9970 - val_recall_1: 0.9970\n",
      "Epoch 17/25\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 1.7897e-05 - accuracy: 1.0000 - precision_1: 1.0000 - recall_1: 1.0000 - val_loss: 0.0059 - val_accuracy: 0.9970 - val_precision_1: 0.9970 - val_recall_1: 0.9970\n",
      "Epoch 18/25\n",
      "100/100 [==============================] - 9s 86ms/step - loss: 1.4939e-05 - accuracy: 1.0000 - precision_1: 1.0000 - recall_1: 1.0000 - val_loss: 0.0058 - val_accuracy: 0.9970 - val_precision_1: 0.9970 - val_recall_1: 0.9970\n",
      "Epoch 19/25\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 1.3100e-05 - accuracy: 1.0000 - precision_1: 1.0000 - recall_1: 1.0000 - val_loss: 0.0059 - val_accuracy: 0.9970 - val_precision_1: 0.9970 - val_recall_1: 0.9970\n",
      "Epoch 20/25\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 1.1465e-05 - accuracy: 1.0000 - precision_1: 1.0000 - recall_1: 1.0000 - val_loss: 0.0059 - val_accuracy: 0.9970 - val_precision_1: 0.9970 - val_recall_1: 0.9970\n",
      "Epoch 21/25\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.0087e-05 - accuracy: 1.0000 - precision_1: 1.0000 - recall_1: 1.0000 - val_loss: 0.0060 - val_accuracy: 0.9970 - val_precision_1: 0.9970 - val_recall_1: 0.9970\n",
      "Epoch 22/25\n",
      "100/100 [==============================] - 9s 90ms/step - loss: 8.9078e-06 - accuracy: 1.0000 - precision_1: 1.0000 - recall_1: 1.0000 - val_loss: 0.0061 - val_accuracy: 0.9970 - val_precision_1: 0.9970 - val_recall_1: 0.9970\n",
      "Epoch 23/25\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 7.9775e-06 - accuracy: 1.0000 - precision_1: 1.0000 - recall_1: 1.0000 - val_loss: 0.0062 - val_accuracy: 0.9970 - val_precision_1: 0.9970 - val_recall_1: 0.9970\n",
      "Epoch 24/25\n",
      "100/100 [==============================] - 9s 86ms/step - loss: 7.1532e-06 - accuracy: 1.0000 - precision_1: 1.0000 - recall_1: 1.0000 - val_loss: 0.0063 - val_accuracy: 0.9970 - val_precision_1: 0.9970 - val_recall_1: 0.9970\n",
      "Epoch 25/25\n",
      "100/100 [==============================] - 9s 86ms/step - loss: 6.4790e-06 - accuracy: 1.0000 - precision_1: 1.0000 - recall_1: 1.0000 - val_loss: 0.0063 - val_accuracy: 0.9970 - val_precision_1: 0.9970 - val_recall_1: 0.9970\n"
     ]
    }
   ],
   "source": [
    "conv1d_set1 = 3\n",
    "conv1d_set2 = 3\n",
    "dense_neurons=2048\n",
    "filters=64\n",
    "kernel_size=3\n",
    "maxpool_1=True\n",
    "epochs=25\n",
    "\n",
    "inputs = tf.keras.layers.Input(batch_shape=(B, train_data.element_spec[0].shape[1], train_data.element_spec[0].shape[2]), dtype='float64' )\n",
    "x = tf.keras.layers.Embedding(input_dim=vocab_size+1,\n",
    "                                output_dim=embedding_size,\n",
    "                                input_length=train_data.element_spec[0].shape[2],\n",
    "                                weights = [embedding_weights],\n",
    "                                )(inputs)\n",
    "for _ in range(conv1d_set1):\n",
    "    x = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
    "if maxpool_1:\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(1, train_data.element_spec[0].shape[2]))(x)\n",
    "    x = tf.reshape(x, (B, train_data.element_spec[0].shape[1], filters))        \n",
    "    for _ in range(conv1d_set2):\n",
    "        x = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=(train_data.element_spec[0].shape[1]) )(x)\n",
    "    x = tf.reshape(x, (B, filters))\n",
    "if not maxpool_1:\n",
    "    x = tf.keras.layers.Flatten()(x)       \n",
    "x = tf.keras.layers.Dense(dense_neurons)(x)\n",
    "outputs = tf.keras.layers.Dense(train_data.element_spec[1].shape[1], activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "print(model.summary())\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy',\n",
    "          metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "hist = model.fit(train_data, validation_data=test_data, epochs=epochs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0f96b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(Logpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
