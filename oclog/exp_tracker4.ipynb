{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da63acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(ablation=1000, B=32, epochs=3, filters=64, kernel_size=3, dense_neurons=2048, \n",
    "              conv1d_set1=1, conv1d_set2=1, maxpool_1=False):\n",
    "    from hdflogv1 import HDFSLogv1\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    tf.random.set_seed(123)\n",
    "    import pickle\n",
    "    with open('../data/hdfs_log_obj.pkl', 'rb') as f:\n",
    "        hdfslogs = pickle.load(f)\n",
    "    ablation_data = hdfslogs.get_train_test_data(ablation=ablation)\n",
    "    x_train, y_train, x_test, y_test = hdfslogs.get_padded_train_test_data(ablation=ablation)\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_data = train_data.shuffle(buffer_size=y_train.shape[0]).batch(B, drop_remainder=True)\n",
    "    print(train_data)\n",
    "    \n",
    "    test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    test_data = test_data.shuffle(buffer_size=y_test.shape[0]).batch(B, drop_remainder=True)\n",
    "    print(test_data)\n",
    "    \n",
    "    \n",
    "    tk = hdfslogs.tk    \n",
    "    vocab_size = len(tk.word_index)\n",
    "    print(f'vocab_size: {vocab_size}')\n",
    "    char_onehot = vocab_size\n",
    "    \n",
    "    embedding_weights = []\n",
    "    embedding_weights.append(np.zeros(vocab_size))\n",
    "    for char, i in tk.word_index.items(): # from 1 to 51\n",
    "        onehot = np.zeros(vocab_size)\n",
    "        onehot[i-1] = 1\n",
    "        embedding_weights.append(onehot)\n",
    "    embedding_weights = np.array(embedding_weights)\n",
    "    \n",
    "    input_size = [x_train.shape[1], x_train.shape[2]]\n",
    "    embedding_size = vocab_size\n",
    "\n",
    "    embedding_layer = tf.keras.layers.Embedding(vocab_size+1,\n",
    "                                                embedding_size,\n",
    "                                                input_length=input_size,\n",
    "                                                weights = [embedding_weights])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    inputs = tf.keras.layers.Input(batch_shape=(B, x_train.shape[1], x_train.shape[2]), dtype='float64' )\n",
    "    x = tf.keras.layers.Embedding(input_dim=vocab_size+1,\n",
    "                                    output_dim=embedding_size,\n",
    "                                    input_length=x_train.shape[2],\n",
    "                                    weights = [embedding_weights],\n",
    "                                    )(inputs)\n",
    "    for _ in range(conv1d_set1):\n",
    "        x = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
    "    if maxpool_1:\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(1, x_train.shape[2]))(x)\n",
    "        x = tf.reshape(x, (B, x_train.shape[1], filters))        \n",
    "        for _ in range(conv1d_set2):\n",
    "            x = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size=(x_train.shape[1]) )(x)    \n",
    "    if not maxpool_1:\n",
    "        x = tf.keras.layers.Flatten()(x)       \n",
    "    x = tf.keras.layers.Dense(dense_neurons)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    print(model.summary())\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    hist = model.fit(train_data, validation_data=test_data, epochs=epochs) \n",
    "    return model, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03882f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting ablation data: 4000\n",
      "4000 12838\n",
      "train_test_data done: 0.019000768661499023\n",
      "RAM usage train_test_data:  72\n",
      "length of train  sequence original 4\n",
      "length of train  sequence original 25\n",
      "length of train  sequence original 33\n",
      "length of train  sequence original 21\n",
      "length of train  sequence original 2\n",
      "length of train sequence padded 32\n",
      "length of train sequence padded 32\n",
      "length of train sequence padded 32\n",
      "length of train sequence padded 32\n",
      "length of train sequence padded 32\n",
      "len of test seq after padding 32\n",
      "len of test seq after padding 32\n",
      "len of test seq after padding 32\n",
      "len of test seq after padding 32\n",
      "len of test seq after padding 32\n",
      "padded_train_test_data done: 6.683482885360718\n",
      "RAM usage padded_train_test_data:  72\n",
      "<BatchDataset shapes: ((250, 32, 64), (250,)), types: (tf.int32, tf.int64)>\n",
      "<BatchDataset shapes: ((250, 32, 64), (250,)), types: (tf.int32, tf.int64)>\n",
      "vocab_size: 42\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(250, 32, 64)]           0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (250, 32, 64, 42)         1806      \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (250, 32, 64, 64)         8128      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (250, 32, 64, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (250, 32, 64, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (250, 32, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorF [(250, 32, 64)]           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (250, 32, 64)             12352     \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (250, 32, 64)             12352     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (250, 32, 64)             12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (250, 1, 64)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (250, 1, 2048)            133120    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (250, 1, 1)               2049      \n",
      "=================================================================\n",
      "Total params: 206,863\n",
      "Trainable params: 206,863\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/16\n",
      "32/32 [==============================] - 712s 22s/step - loss: 0.6240 - accuracy: 0.6898 - val_loss: 0.7026 - val_accuracy: 0.9611\n",
      "Epoch 2/16\n",
      "32/32 [==============================] - 3677s 115s/step - loss: 0.5825 - accuracy: 0.8062 - val_loss: 0.7120 - val_accuracy: 0.9506\n",
      "Epoch 3/16\n",
      "32/32 [==============================] - 3982s 124s/step - loss: 0.5825 - accuracy: 0.8087 - val_loss: 0.7111 - val_accuracy: 0.9524\n",
      "Epoch 4/16\n",
      "32/32 [==============================] - 4990s 156s/step - loss: 0.5730 - accuracy: 0.8328 - val_loss: 0.7111 - val_accuracy: 0.9523\n",
      "Epoch 5/16\n",
      "32/32 [==============================] - 5390s 168s/step - loss: 0.5784 - accuracy: 0.8065 - val_loss: 0.6927 - val_accuracy: 0.9818\n",
      "Epoch 6/16\n",
      "32/32 [==============================] - 4993s 156s/step - loss: 0.5686 - accuracy: 0.8300 - val_loss: 0.6987 - val_accuracy: 0.9732\n",
      "Epoch 7/16\n",
      "32/32 [==============================] - 7698s 241s/step - loss: 0.5363 - accuracy: 0.9185 - val_loss: 0.6934 - val_accuracy: 0.9876\n",
      "Epoch 8/16\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5296 - accuracy: 0.9342"
     ]
    }
   ],
   "source": [
    "test_model(ablation=4000, B=250, kernel_size=3, epochs=16, dense_neurons=2048, conv1d_set1=3,conv1d_set2=3, maxpool_1=True )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
