{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56818695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# from hdflogv2 import HDFSLogv2\n",
    "from hdflogv3 import HDFSLogv3\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dc750d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hlog_meta_176_32 = HDFSLogv3( debug=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96f26d41-3d98-4539-9061-2d3a8a661f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# labelled_num_seq_df = hlog_meta_176_32.get_labelled_num_seq_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c544fa1b-270d-4c4e-b629-6fa36dfeeca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of lines in the log file: 11175629\n",
      "RAM usage:  100232088\n",
      "loaded logs in memory in time: 5.0926172733306885\n",
      "loaded cleaned logs with blk_id  in memory: 226.67128086090088\n",
      "RAM usage:  100232088\n",
      "loaded cleaned logs without blkid in memory: 0.5442483425140381\n",
      "RAM usage:  100232088\n",
      "starting training the tokenizer:\n",
      "ending tokenizer training: 106.86853504180908\n",
      "RAM usage:  48\n",
      "vocabulary size: 42\n",
      "starting text to number conversion\n",
      "completed:  0\n",
      "time : 0.0\n",
      "completed:  1000000\n",
      "time : 17.132976055145264\n",
      "completed:  2000000\n",
      "time : 33.53411293029785\n",
      "completed:  3000000\n",
      "time : 50.03786635398865\n",
      "completed:  4000000\n",
      "time : 67.04238796234131\n",
      "completed:  5000000\n",
      "time : 84.18424129486084\n",
      "completed:  6000000\n",
      "time : 100.92993378639221\n",
      "completed:  7000000\n",
      "time : 118.3236563205719\n",
      "completed:  8000000\n",
      "time : 134.9677083492279\n",
      "completed:  9000000\n",
      "time : 151.82281732559204\n",
      "completed:  10000000\n",
      "time : 168.5296995639801\n",
      "completed:  11000000\n",
      "time : 185.52457857131958\n",
      "ending text to number conversion: 188.495374917984\n",
      "RAM usage:  100232088\n",
      "ending num_sequence_by_blkid conversion: 15.881837129592896\n",
      "RAM usage:  20971608\n",
      "ending labelled_num_seq_df conversion: 12.741499185562134\n",
      "RAM usage:  117329426\n",
      "len of pos_df 16838\n",
      "len of neg_df 558223\n",
      "eq_neg_pos_df shape: (33676, 2)\n",
      "train_hdfs_anomaly:, 800, val_hdfs_anomaly:, 100, test_hdfs_anomaly:, 100, train_hdfs_normal:, 800, val_hdfs_normal:, 100, test_hdfs_normal:, 100, train: hdfs_anomaly    800\n",
      "hdfs_normal     800\n",
      "Name: label, dtype: int64\n",
      "val: hdfs_anomaly    100\n",
      "hdfs_normal     100\n",
      "Name: label, dtype: int64\n",
      "test: hdfs_anomaly    100\n",
      "hdfs_normal     100\n",
      "Name: label, dtype: int64\n",
      "train_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "val_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "test_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "char in lines, train_data.element_spec[0].shape[2] 176\n",
      "num classes, train_data.element_spec[1].shape:  (32,)\n",
      "length of val_data: 6\n",
      "length of train_data - (num_seq_per_cls * num_class)// batch size: 50\n",
      "CPU times: total: 9min 25s\n",
      "Wall time: 9min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = hlog_meta_176_32.get_tensor_train_val_test(train_ratio=0.8, padded_char_len=176, padded_seq_len=32,\n",
    "                hdfs_rm_time_stamp=False, hdfs_rm_ip_address=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aafc186-3a1a-4e86-a06d-a309f5f82360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>,\n",
       " <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>,\n",
       " <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "260c1bb6-5fef-4b62-9aac-79884ebffc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved hdfs object as data\\hdfsobj_32_176_time_ip.pkl\n",
      "CPU times: total: 922 ms\n",
      "Wall time: 913 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data\\\\hdfsobj_32_176_time_ip.pkl'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "hlog_meta_176_32.save_hdfs_log_obj(hdfs_obj_save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2e118fb-f928-4c2f-8e96-0099915b1f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hlog_meta_176_32.hdfs_rm_time_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "848cb20d-d5c3-4883-93c4-b930fc0cb877",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the saved obj - done\n",
    "## check the df - done\n",
    "## check differnt ablation - done\n",
    "\n",
    "\n",
    "## create without meta - matching bglog (bgl all seq are 32, two variants of char were used , 64 and 176)\n",
    "## run two models to see the impact on accuracy\n",
    "\n",
    "## mix the data within bglog\n",
    "## pretrain impact - centroid visualization\n",
    "## oset result with this data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0532ee12-15a8-44a3-ab94-6ec10911ec97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\hdfsobj_32_176_time_ip.pkl'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hlog_meta_176_32.hdfs_saved_obj_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "224dd3d3-7ba7-44de-9318-c860936d4c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data\\\\hdfsobj_32_176_time_ip.pkl' , 'rb') as f:\n",
    "    loaded_hdfs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "020af201-d7f7-4a3c-b565-7bb6df28c554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[[3, 14, 2, 2, 2, 2, 3, 14, 2, 27, 2, 20, 6, 2...</td>\n",
       "      <td>hdfs_anomaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>[[3, 14, 2, 2, 3, 28, 6, 2, 2, 22, 11, 27, 2, ...</td>\n",
       "      <td>hdfs_anomaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>[[3, 14, 2, 2, 2, 3, 3, 2, 2, 20, 6, 20, 11, 6...</td>\n",
       "      <td>hdfs_anomaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>[[3, 14, 2, 2, 2, 3, 3, 3, 6, 14, 3, 22, 21, 2...</td>\n",
       "      <td>hdfs_anomaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>[[3, 14, 2, 2, 2, 3, 6, 6, 21, 14, 11, 6, 2, 2...</td>\n",
       "      <td>hdfs_anomaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17163</th>\n",
       "      <td>[[3, 14, 2, 2, 2, 2, 2, 3, 3, 2, 6, 21, 6, 22,...</td>\n",
       "      <td>hdfs_normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17164</th>\n",
       "      <td>[[3, 14, 2, 2, 2, 3, 2, 6, 6, 14, 11, 14, 2, 2...</td>\n",
       "      <td>hdfs_normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17165</th>\n",
       "      <td>[[3, 14, 2, 2, 2, 2, 3, 14, 2, 2, 21, 27, 6, 2...</td>\n",
       "      <td>hdfs_normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17166</th>\n",
       "      <td>[[3, 14, 2, 2, 2, 2, 3, 11, 11, 11, 2, 20, 6, ...</td>\n",
       "      <td>hdfs_normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17167</th>\n",
       "      <td>[[3, 14, 2, 2, 2, 3, 6, 2, 2, 3, 11, 22, 2, 20...</td>\n",
       "      <td>hdfs_normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33676 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     seq         label\n",
       "33     [[3, 14, 2, 2, 2, 2, 3, 14, 2, 27, 2, 20, 6, 2...  hdfs_anomaly\n",
       "53     [[3, 14, 2, 2, 3, 28, 6, 2, 2, 22, 11, 27, 2, ...  hdfs_anomaly\n",
       "304    [[3, 14, 2, 2, 2, 3, 3, 2, 2, 20, 6, 20, 11, 6...  hdfs_anomaly\n",
       "305    [[3, 14, 2, 2, 2, 3, 3, 3, 6, 14, 3, 22, 21, 2...  hdfs_anomaly\n",
       "383    [[3, 14, 2, 2, 2, 3, 6, 6, 21, 14, 11, 6, 2, 2...  hdfs_anomaly\n",
       "...                                                  ...           ...\n",
       "17163  [[3, 14, 2, 2, 2, 2, 2, 3, 3, 2, 6, 21, 6, 22,...   hdfs_normal\n",
       "17164  [[3, 14, 2, 2, 2, 3, 2, 6, 6, 14, 11, 14, 2, 2...   hdfs_normal\n",
       "17165  [[3, 14, 2, 2, 2, 2, 3, 14, 2, 2, 21, 27, 6, 2...   hdfs_normal\n",
       "17166  [[3, 14, 2, 2, 2, 2, 3, 11, 11, 11, 2, 20, 6, ...   hdfs_normal\n",
       "17167  [[3, 14, 2, 2, 2, 3, 6, 2, 2, 3, 11, 22, 2, 20...   hdfs_normal\n",
       "\n",
       "[33676 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_hdfs.lebeled_num_seq_df_epn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f762144e-f2f0-4375-8bfa-c0e5d63b8aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_hdfs_anomaly:, 1600, val_hdfs_anomaly:, 200, test_hdfs_anomaly:, 200, train_hdfs_normal:, 1600, val_hdfs_normal:, 200, test_hdfs_normal:, 200, train: hdfs_anomaly    1600\n",
      "hdfs_normal     1600\n",
      "Name: label, dtype: int64\n",
      "val: hdfs_anomaly    200\n",
      "hdfs_normal     200\n",
      "Name: label, dtype: int64\n",
      "test: hdfs_anomaly    200\n",
      "hdfs_normal     200\n",
      "Name: label, dtype: int64\n",
      "train_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "val_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "test_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "char in lines, train_data.element_spec[0].shape[2] 176\n",
      "num classes, train_data.element_spec[1].shape:  (32,)\n",
      "length of val_data: 12\n",
      "length of train_data - (num_seq_per_cls * num_class)// batch size: 100\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data = loaded_hdfs.get_tensor_train_val_test(ablation=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19586c30-9fe3-4137-899e-6d769246e70e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a516e2b7-780d-400a-bd2c-bbb4504ee6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e4a690-f238-48f8-a340-9f8aceb023c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77693601-e0da-41af-b98f-72ff5d1890d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of lines in the log file: 11175629\n",
      "RAM usage:  100232088\n",
      "loaded logs in memory in time: 4.928619623184204\n",
      "loaded cleaned logs with blk_id  in memory: 85.18003249168396\n",
      "RAM usage:  100232088\n",
      "loaded cleaned logs without blkid in memory: 0.5200395584106445\n",
      "RAM usage:  100232088\n",
      "starting training the tokenizer:\n",
      "ending tokenizer training: 69.21796894073486\n",
      "RAM usage:  48\n",
      "vocabulary size: 42\n",
      "starting text to number conversion\n",
      "completed:  0\n",
      "time : 0.0\n",
      "completed:  1000000\n",
      "time : 12.502689599990845\n",
      "completed:  2000000\n",
      "time : 23.854435682296753\n",
      "completed:  3000000\n",
      "time : 36.653728008270264\n",
      "completed:  4000000\n",
      "time : 48.96333718299866\n",
      "completed:  5000000\n",
      "time : 61.36536693572998\n",
      "completed:  6000000\n",
      "time : 73.81578421592712\n",
      "completed:  7000000\n",
      "time : 86.69364356994629\n",
      "completed:  8000000\n",
      "time : 99.46407985687256\n",
      "completed:  9000000\n",
      "time : 112.18711376190186\n",
      "completed:  10000000\n",
      "time : 124.97475719451904\n",
      "completed:  11000000\n",
      "time : 138.5403594970703\n",
      "ending text to number conversion: 140.9813268184662\n",
      "RAM usage:  100232088\n",
      "ending num_sequence_by_blkid conversion: 14.481579780578613\n",
      "RAM usage:  20971608\n",
      "ending labelled_num_seq_df conversion: 9.928815841674805\n",
      "RAM usage:  117329426\n",
      "len of pos_df 16838\n",
      "len of neg_df 558223\n",
      "eq_neg_pos_df shape: (33676, 2)\n",
      "train_hdfs_anomaly:, 800, val_hdfs_anomaly:, 100, test_hdfs_anomaly:, 100, train_hdfs_normal:, 800, val_hdfs_normal:, 100, test_hdfs_normal:, 100, train: hdfs_anomaly    800\n",
      "hdfs_normal     800\n",
      "Name: label, dtype: int64\n",
      "val: hdfs_anomaly    100\n",
      "hdfs_normal     100\n",
      "Name: label, dtype: int64\n",
      "test: hdfs_anomaly    100\n",
      "hdfs_normal     100\n",
      "Name: label, dtype: int64\n",
      "train_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 64), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "val_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 64), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "test_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 64), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "char in lines, train_data.element_spec[0].shape[2] 64\n",
      "num classes, train_data.element_spec[1].shape:  (32,)\n",
      "length of val_data: 6\n",
      "length of train_data - (num_seq_per_cls * num_class)// batch size: 50\n"
     ]
    }
   ],
   "source": [
    "hlog_meta_32_64_no_meta = HDFSLogv3( debug=True )\n",
    "res_32_64_no_meta = hlog_meta_32_64_no_meta.get_tensor_train_val_test(train_ratio=0.8, padded_char_len=64, padded_seq_len=32,\n",
    "                hdfs_rm_time_stamp=True, hdfs_rm_ip_address=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ecd661a-405b-4804-b248-e0e21e491b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved hdfs object as data\\hdfsobj_32_64_no_meta.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data\\\\hdfsobj_32_64_no_meta.pkl'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hlog_meta_32_64_no_meta.save_hdfs_log_obj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113bfb22-5b72-4cab-ae82-5aa261b7d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run two models to see the impact on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e0f064a-2154-46c9-9ba8-7e2a9bb0275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(ablation=1000, B=32, epochs=3, filters=64, kernel_size=3, dense_neurons=2048, \n",
    "              conv1d_set1=1, conv1d_set2=1, maxpool_1=False, hdfs_obJ_full_name=None):\n",
    "    # from hdflogv2 import HDFSLogv2\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    tf.random.set_seed(123)\n",
    "    import pickle\n",
    "    with open(hdfs_obJ_full_name, 'rb') as f:\n",
    "        hdfslogs = pickle.load(f)    \n",
    "    # x_train, y_train, x_val, y_val, x_test, y_test \n",
    "    train_data, val_data, test_data = hdfslogs.get_tensor_train_val_test(ablation=ablation)    \n",
    "    print(train_data)   \n",
    "    B = train_data.element_spec[0].shape[0]\n",
    "    seq_len = train_data.element_spec[0].shape[1]\n",
    "    char_len = train_data.element_spec[0].shape[2]\n",
    "    \n",
    "    tk = hdfslogs.tk    \n",
    "    vocab_size = len(tk.word_index)\n",
    "    print(f'vocab_size: {vocab_size}')\n",
    "    char_onehot = vocab_size\n",
    "    \n",
    "    embedding_weights = []\n",
    "    embedding_weights.append(np.zeros(vocab_size))\n",
    "    for char, i in tk.word_index.items(): # from 1 to 51\n",
    "        onehot = np.zeros(vocab_size)\n",
    "        onehot[i-1] = 1\n",
    "        embedding_weights.append(onehot)\n",
    "    embedding_weights = np.array(embedding_weights)\n",
    "    \n",
    "    \n",
    "    \n",
    "    input_size = [seq_len, char_len]\n",
    "    embedding_size = vocab_size\n",
    "\n",
    "    embedding_layer = tf.keras.layers.Embedding(vocab_size+1,\n",
    "                                                embedding_size,\n",
    "                                                input_length=input_size,\n",
    "                                                weights = [embedding_weights])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    inputs = tf.keras.layers.Input(batch_shape=(B, seq_len, char_len), dtype='float64' )\n",
    "    x = tf.keras.layers.Embedding(input_dim=vocab_size+1,\n",
    "                                    output_dim=embedding_size,\n",
    "                                    input_length=char_len,\n",
    "                                    weights = [embedding_weights],\n",
    "                                    )(inputs)\n",
    "    for _ in range(conv1d_set1):\n",
    "        x = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
    "    if maxpool_1:\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(1, char_len))(x)\n",
    "        x = tf.reshape(x, (B, seq_len, filters))        \n",
    "        for _ in range(conv1d_set2):\n",
    "            x = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size=(seq_len) )(x)    \n",
    "    if not maxpool_1:\n",
    "        x = tf.keras.layers.Flatten()(x)       \n",
    "    x = tf.keras.layers.Dense(dense_neurons)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    outputs = tf.reshape(outputs, (B, 1))\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    print(model.summary())\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    hist = model.fit(train_data, validation_data=test_data, epochs=epochs) \n",
    "    return model, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aaf44afe-7f7c-497e-bc51-b4415e511fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_hdfs_anomaly:, 3200, val_hdfs_anomaly:, 400, test_hdfs_anomaly:, 400, train_hdfs_normal:, 3200, val_hdfs_normal:, 400, test_hdfs_normal:, 400, train: hdfs_anomaly    3200\n",
      "hdfs_normal     3200\n",
      "Name: label, dtype: int64\n",
      "val: hdfs_anomaly    400\n",
      "hdfs_normal     400\n",
      "Name: label, dtype: int64\n",
      "test: hdfs_anomaly    400\n",
      "hdfs_normal     400\n",
      "Name: label, dtype: int64\n",
      "train_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 64), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "val_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 64), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "test_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 64), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "char in lines, train_data.element_spec[0].shape[2] 64\n",
      "num classes, train_data.element_spec[1].shape:  (32,)\n",
      "length of val_data: 25\n",
      "length of train_data - (num_seq_per_cls * num_class)// batch size: 200\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(32, 32, 64), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "vocab_size: 42\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(32, 32, 64)]            0         \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (32, 32, 64, 42)          1806      \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (32, 32, 64, 64)          8128      \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (32, 32, 64, 64)          12352     \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (32, 32, 64, 64)          12352     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (32, 32, 1, 64)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " tf.reshape_1 (TFOpLambda)   (32, 32, 64)              0         \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           (32, 32, 64)              12352     \n",
      "                                                                 \n",
      " conv1d_10 (Conv1D)          (32, 32, 64)              12352     \n",
      "                                                                 \n",
      " conv1d_11 (Conv1D)          (32, 32, 64)              12352     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (32, 1, 64)              0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dense_2 (Dense)             (32, 1, 2048)             133120    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (32, 1, 1)                2049      \n",
      "                                                                 \n",
      " tf.reshape_2 (TFOpLambda)   (32, 1)                   0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 206,863\n",
      "Trainable params: 206,863\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/16\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 0.6300 - accuracy: 0.7161 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 2/16\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 3/16\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 4/16\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 5/16\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 6/16\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 7/16\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 8/16\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 9/16\n",
      "200/200 [==============================] - 20s 102ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 10/16\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 11/16\n",
      "200/200 [==============================] - 19s 96ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 12/16\n",
      "200/200 [==============================] - 20s 102ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 13/16\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 14/16\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 15/16\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 16/16\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.engine.functional.Functional at 0x17a4cb49430>,\n",
       " <keras.callbacks.History at 0x17a503a4880>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### same setting as exp5_tracker where accuracy was 99.69 \n",
    "### only change is dataset - first with no_meta\n",
    "test_model(ablation=4000, B=250, kernel_size=3, epochs=16, dense_neurons=2048, conv1d_set1=3,conv1d_set2=3, maxpool_1=True,\n",
    "          hdfs_obJ_full_name='data\\\\hdfsobj_32_64_no_meta.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98076aba-15a9-4e1d-8096-951a1d399a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_hdfs_anomaly:, 3200, val_hdfs_anomaly:, 400, test_hdfs_anomaly:, 400, train_hdfs_normal:, 3200, val_hdfs_normal:, 400, test_hdfs_normal:, 400, train: hdfs_anomaly    3200\n",
      "hdfs_normal     3200\n",
      "Name: label, dtype: int64\n",
      "val: hdfs_anomaly    400\n",
      "hdfs_normal     400\n",
      "Name: label, dtype: int64\n",
      "test: hdfs_anomaly    400\n",
      "hdfs_normal     400\n",
      "Name: label, dtype: int64\n",
      "train_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "val_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "test_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "char in lines, train_data.element_spec[0].shape[2] 176\n",
      "num classes, train_data.element_spec[1].shape:  (32,)\n",
      "length of val_data: 25\n",
      "length of train_data - (num_seq_per_cls * num_class)// batch size: 200\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "vocab_size: 42\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(32, 32, 176)]           0         \n",
      "                                                                 \n",
      " embedding_5 (Embedding)     (32, 32, 176, 42)         1806      \n",
      "                                                                 \n",
      " conv1d_12 (Conv1D)          (32, 32, 176, 64)         8128      \n",
      "                                                                 \n",
      " conv1d_13 (Conv1D)          (32, 32, 176, 64)         12352     \n",
      "                                                                 \n",
      " conv1d_14 (Conv1D)          (32, 32, 176, 64)         12352     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (32, 32, 1, 64)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " tf.reshape_3 (TFOpLambda)   (32, 32, 64)              0         \n",
      "                                                                 \n",
      " conv1d_15 (Conv1D)          (32, 32, 64)              12352     \n",
      "                                                                 \n",
      " conv1d_16 (Conv1D)          (32, 32, 64)              12352     \n",
      "                                                                 \n",
      " conv1d_17 (Conv1D)          (32, 32, 64)              12352     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (32, 1, 64)              0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dense_4 (Dense)             (32, 1, 2048)             133120    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (32, 1, 1)                2049      \n",
      "                                                                 \n",
      " tf.reshape_4 (TFOpLambda)   (32, 1)                   0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 206,863\n",
      "Trainable params: 206,863\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/16\n",
      "200/200 [==============================] - 48s 239ms/step - loss: 0.5947 - accuracy: 0.7667 - val_loss: 0.5798 - val_accuracy: 0.8012\n",
      "Epoch 2/16\n",
      "200/200 [==============================] - 47s 237ms/step - loss: 0.5835 - accuracy: 0.7923 - val_loss: 0.5780 - val_accuracy: 0.8062\n",
      "Epoch 3/16\n",
      "200/200 [==============================] - 48s 241ms/step - loss: 0.5836 - accuracy: 0.7969 - val_loss: 0.5811 - val_accuracy: 0.8075\n",
      "Epoch 4/16\n",
      "200/200 [==============================] - 48s 241ms/step - loss: 0.5849 - accuracy: 0.7966 - val_loss: 0.5811 - val_accuracy: 0.8075\n",
      "Epoch 5/16\n",
      "200/200 [==============================] - 48s 239ms/step - loss: 0.5844 - accuracy: 0.7933 - val_loss: 0.5794 - val_accuracy: 0.8025\n",
      "Epoch 6/16\n",
      "200/200 [==============================] - 48s 239ms/step - loss: 0.5838 - accuracy: 0.7917 - val_loss: 0.5794 - val_accuracy: 0.8025\n",
      "Epoch 7/16\n",
      "200/200 [==============================] - 48s 240ms/step - loss: 0.5838 - accuracy: 0.7917 - val_loss: 0.5794 - val_accuracy: 0.8025\n",
      "Epoch 8/16\n",
      "200/200 [==============================] - 48s 240ms/step - loss: 0.5838 - accuracy: 0.7917 - val_loss: 0.5794 - val_accuracy: 0.8025\n",
      "Epoch 9/16\n",
      "200/200 [==============================] - 48s 242ms/step - loss: 0.5838 - accuracy: 0.7917 - val_loss: 0.5794 - val_accuracy: 0.8025\n",
      "Epoch 10/16\n",
      "200/200 [==============================] - 49s 245ms/step - loss: 0.5838 - accuracy: 0.7917 - val_loss: 0.5794 - val_accuracy: 0.8025\n",
      "Epoch 11/16\n",
      "200/200 [==============================] - 48s 242ms/step - loss: 0.5838 - accuracy: 0.7917 - val_loss: 0.5794 - val_accuracy: 0.8025\n",
      "Epoch 12/16\n",
      "200/200 [==============================] - 48s 240ms/step - loss: 0.5838 - accuracy: 0.7917 - val_loss: 0.5794 - val_accuracy: 0.8025\n",
      "Epoch 13/16\n",
      "200/200 [==============================] - 48s 240ms/step - loss: 0.5902 - accuracy: 0.7748 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 14/16\n",
      "200/200 [==============================] - 48s 240ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 15/16\n",
      "200/200 [==============================] - 48s 242ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 16/16\n",
      "200/200 [==============================] - 48s 241ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.engine.functional.Functional at 0x17d6a44e4f0>,\n",
       " <keras.callbacks.History at 0x17d6a44e2b0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### so with 32 seq  and 64 char len the model fails to learn . lets see with seq 32 and char 176 and meta reatianed\n",
    "\n",
    "test_model(ablation=4000, B=250, kernel_size=3, epochs=16, dense_neurons=2048, conv1d_set1=3,conv1d_set2=3, maxpool_1=True,\n",
    "          hdfs_obJ_full_name='data\\\\hdfsobj_32_176_time_ip.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db7278f1-f226-4e6d-94a5-25071d256f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of lines in the log file: 11175629\n",
      "RAM usage:  100232088\n",
      "loaded logs in memory in time: 4.932230234146118\n",
      "loaded cleaned logs with blk_id  in memory: 89.37497878074646\n",
      "RAM usage:  100232088\n",
      "loaded cleaned logs without blkid in memory: 0.6140058040618896\n",
      "RAM usage:  100232088\n",
      "starting training the tokenizer:\n",
      "ending tokenizer training: 69.91967988014221\n",
      "RAM usage:  48\n",
      "vocabulary size: 42\n",
      "starting text to number conversion\n",
      "completed:  0\n",
      "time : 0.0\n",
      "completed:  1000000\n",
      "time : 12.986462831497192\n",
      "completed:  2000000\n",
      "time : 24.733755350112915\n",
      "completed:  3000000\n",
      "time : 37.73969531059265\n",
      "completed:  4000000\n",
      "time : 50.31769037246704\n",
      "completed:  5000000\n",
      "time : 63.09116792678833\n",
      "completed:  6000000\n",
      "time : 75.72368383407593\n",
      "completed:  7000000\n",
      "time : 88.78753399848938\n",
      "completed:  8000000\n",
      "time : 101.75811624526978\n",
      "completed:  9000000\n",
      "time : 114.99517154693604\n",
      "completed:  10000000\n",
      "time : 127.98333668708801\n",
      "completed:  11000000\n",
      "time : 140.96529078483582\n",
      "ending text to number conversion: 143.329909324646\n",
      "RAM usage:  100232088\n",
      "ending num_sequence_by_blkid conversion: 14.918161630630493\n",
      "RAM usage:  20971608\n",
      "ending labelled_num_seq_df conversion: 12.176108360290527\n",
      "RAM usage:  117329426\n",
      "len of pos_df 16838\n",
      "len of neg_df 558223\n",
      "eq_neg_pos_df shape: (33676, 2)\n",
      "train_hdfs_anomaly:, 800, val_hdfs_anomaly:, 100, test_hdfs_anomaly:, 100, train_hdfs_normal:, 800, val_hdfs_normal:, 100, test_hdfs_normal:, 100, train: hdfs_anomaly    800\n",
      "hdfs_normal     800\n",
      "Name: label, dtype: int64\n",
      "val: hdfs_anomaly    100\n",
      "hdfs_normal     100\n",
      "Name: label, dtype: int64\n",
      "test: hdfs_anomaly    100\n",
      "hdfs_normal     100\n",
      "Name: label, dtype: int64\n",
      "train_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "val_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "test_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "char in lines, train_data.element_spec[0].shape[2] 176\n",
      "num classes, train_data.element_spec[1].shape:  (32,)\n",
      "length of val_data: 6\n",
      "length of train_data - (num_seq_per_cls * num_class)// batch size: 50\n"
     ]
    }
   ],
   "source": [
    "hlog_meta_32_176_no_meta = HDFSLogv3(debug=True )\n",
    "res_32_176_no_meta = hlog_meta_32_176_no_meta.get_tensor_train_val_test(train_ratio=0.8, padded_char_len=176, padded_seq_len=32,\n",
    "                hdfs_rm_time_stamp=True, hdfs_rm_ip_address=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc4f51ce-ff8e-47e1-b8f6-9a499656a902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved hdfs object as data\\hdfsobj_32_176_no_meta.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data\\\\hdfsobj_32_176_no_meta.pkl'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hlog_meta_32_176_no_meta.save_hdfs_log_obj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72d2c045-010c-4b69-a9db-c8b2d5b555ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_hdfs_anomaly:, 3200, val_hdfs_anomaly:, 400, test_hdfs_anomaly:, 400, train_hdfs_normal:, 3200, val_hdfs_normal:, 400, test_hdfs_normal:, 400, train: hdfs_anomaly    3200\n",
      "hdfs_normal     3200\n",
      "Name: label, dtype: int64\n",
      "val: hdfs_anomaly    400\n",
      "hdfs_normal     400\n",
      "Name: label, dtype: int64\n",
      "test: hdfs_anomaly    400\n",
      "hdfs_normal     400\n",
      "Name: label, dtype: int64\n",
      "train_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "val_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "test_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "char in lines, train_data.element_spec[0].shape[2] 176\n",
      "num classes, train_data.element_spec[1].shape:  (32,)\n",
      "length of val_data: 25\n",
      "length of train_data - (num_seq_per_cls * num_class)// batch size: 200\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "vocab_size: 42\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(32, 32, 176)]           0         \n",
      "                                                                 \n",
      " embedding_7 (Embedding)     (32, 32, 176, 42)         1806      \n",
      "                                                                 \n",
      " conv1d_18 (Conv1D)          (32, 32, 176, 64)         8128      \n",
      "                                                                 \n",
      " conv1d_19 (Conv1D)          (32, 32, 176, 64)         12352     \n",
      "                                                                 \n",
      " conv1d_20 (Conv1D)          (32, 32, 176, 64)         12352     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (32, 32, 1, 64)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " tf.reshape_5 (TFOpLambda)   (32, 32, 64)              0         \n",
      "                                                                 \n",
      " conv1d_21 (Conv1D)          (32, 32, 64)              12352     \n",
      "                                                                 \n",
      " conv1d_22 (Conv1D)          (32, 32, 64)              12352     \n",
      "                                                                 \n",
      " conv1d_23 (Conv1D)          (32, 32, 64)              12352     \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (32, 1, 64)              0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dense_6 (Dense)             (32, 1, 2048)             133120    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (32, 1, 1)                2049      \n",
      "                                                                 \n",
      " tf.reshape_6 (TFOpLambda)   (32, 1)                   0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 206,863\n",
      "Trainable params: 206,863\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/16\n",
      "200/200 [==============================] - 48s 239ms/step - loss: 0.5916 - accuracy: 0.7709 - val_loss: 0.5790 - val_accuracy: 0.8037\n",
      "Epoch 2/16\n",
      "200/200 [==============================] - 48s 239ms/step - loss: 0.6304 - accuracy: 0.6692 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 3/16\n",
      "200/200 [==============================] - 50s 249ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 4/16\n",
      "200/200 [==============================] - 50s 247ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 5/16\n",
      "200/200 [==============================] - 50s 249ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 6/16\n",
      "200/200 [==============================] - 49s 247ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 7/16\n",
      "200/200 [==============================] - 52s 258ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 8/16\n",
      "200/200 [==============================] - 50s 250ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 9/16\n",
      "200/200 [==============================] - 50s 250ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 10/16\n",
      "200/200 [==============================] - 51s 256ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 11/16\n",
      "200/200 [==============================] - 52s 262ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 12/16\n",
      "200/200 [==============================] - 49s 247ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 13/16\n",
      "200/200 [==============================] - 49s 244ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 14/16\n",
      "200/200 [==============================] - 49s 244ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 15/16\n",
      "200/200 [==============================] - 48s 242ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 16/16\n",
      "200/200 [==============================] - 51s 254ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.engine.functional.Functional at 0x181e5e6be50>,\n",
       " <keras.callbacks.History at 0x181e4a5c310>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(ablation=4000, B=250, kernel_size=3, epochs=16, dense_neurons=2048, conv1d_set1=3,conv1d_set2=3, maxpool_1=True,\n",
    "          hdfs_obJ_full_name='data\\\\hdfsobj_32_176_no_meta.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "870a7c90-e86f-4c89-a57c-3a1db56c409a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of lines in the log file: 11175629\n",
      "RAM usage:  100232088\n",
      "loaded logs in memory in time: 4.903720140457153\n",
      "loaded cleaned logs with blk_id  in memory: 213.37509274482727\n",
      "RAM usage:  100232088\n",
      "loaded cleaned logs without blkid in memory: 0.5786683559417725\n",
      "RAM usage:  100232088\n",
      "starting training the tokenizer:\n",
      "ending tokenizer training: 111.32306838035583\n",
      "RAM usage:  48\n",
      "vocabulary size: 42\n",
      "starting text to number conversion\n",
      "completed:  0\n",
      "time : 0.0\n",
      "completed:  1000000\n",
      "time : 16.463935613632202\n",
      "completed:  2000000\n",
      "time : 32.234508752822876\n",
      "completed:  3000000\n",
      "time : 48.29206371307373\n",
      "completed:  4000000\n",
      "time : 64.79974889755249\n",
      "completed:  5000000\n",
      "time : 81.01934289932251\n",
      "completed:  6000000\n",
      "time : 97.86089825630188\n",
      "completed:  7000000\n",
      "time : 114.2381522655487\n",
      "completed:  8000000\n",
      "time : 130.31518006324768\n",
      "completed:  9000000\n",
      "time : 146.71768498420715\n",
      "completed:  10000000\n",
      "time : 163.1870379447937\n",
      "completed:  11000000\n",
      "time : 179.55471873283386\n",
      "ending text to number conversion: 182.3069133758545\n",
      "RAM usage:  100232088\n",
      "ending num_sequence_by_blkid conversion: 13.52832841873169\n",
      "RAM usage:  20971608\n",
      "ending labelled_num_seq_df conversion: 9.670776844024658\n",
      "RAM usage:  117329426\n",
      "len of pos_df 16838\n",
      "len of neg_df 558223\n",
      "eq_neg_pos_df shape: (33676, 2)\n",
      "train_hdfs_anomaly:, 800, val_hdfs_anomaly:, 100, test_hdfs_anomaly:, 100, train_hdfs_normal:, 800, val_hdfs_normal:, 100, test_hdfs_normal:, 100, train: hdfs_anomaly    800\n",
      "hdfs_normal     800\n",
      "Name: label, dtype: int64\n",
      "val: hdfs_anomaly    100\n",
      "hdfs_normal     100\n",
      "Name: label, dtype: int64\n",
      "test: hdfs_anomaly    100\n",
      "hdfs_normal     100\n",
      "Name: label, dtype: int64\n",
      "train_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 64), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "val_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 64), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "test_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 64), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "char in lines, train_data.element_spec[0].shape[2] 64\n",
      "num classes, train_data.element_spec[1].shape:  (32,)\n",
      "length of val_data: 6\n",
      "length of train_data - (num_seq_per_cls * num_class)// batch size: 50\n",
      "saved hdfs object as data\\hdfsobj_32_64_time_ip.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data\\\\hdfsobj_32_64_time_ip.pkl'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hlog_meta_32_64_time_ip = HDFSLogv3(debug=True )\n",
    "res_32_64_time_ip = hlog_meta_32_64_time_ip.get_tensor_train_val_test(train_ratio=0.8, padded_char_len=64, padded_seq_len=32,\n",
    "                hdfs_rm_time_stamp=False, hdfs_rm_ip_address=False,)\n",
    "hlog_meta_32_64_time_ip.save_hdfs_log_obj()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec565458-64ab-4a43-bddc-1257a1c5a9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_hdfs_anomaly:, 3200, val_hdfs_anomaly:, 400, test_hdfs_anomaly:, 400, train_hdfs_normal:, 3200, val_hdfs_normal:, 400, test_hdfs_normal:, 400, train: hdfs_anomaly    3200\n",
      "hdfs_normal     3200\n",
      "Name: label, dtype: int64\n",
      "val: hdfs_anomaly    400\n",
      "hdfs_normal     400\n",
      "Name: label, dtype: int64\n",
      "test: hdfs_anomaly    400\n",
      "hdfs_normal     400\n",
      "Name: label, dtype: int64\n",
      "train_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 64), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "val_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 64), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "test_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 64), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "char in lines, train_data.element_spec[0].shape[2] 64\n",
      "num classes, train_data.element_spec[1].shape:  (32,)\n",
      "length of val_data: 25\n",
      "length of train_data - (num_seq_per_cls * num_class)// batch size: 200\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(32, 32, 64), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "vocab_size: 42\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(32, 32, 64)]            0         \n",
      "                                                                 \n",
      " embedding_9 (Embedding)     (32, 32, 64, 42)          1806      \n",
      "                                                                 \n",
      " conv1d_24 (Conv1D)          (32, 32, 64, 64)          8128      \n",
      "                                                                 \n",
      " conv1d_25 (Conv1D)          (32, 32, 64, 64)          12352     \n",
      "                                                                 \n",
      " conv1d_26 (Conv1D)          (32, 32, 64, 64)          12352     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (32, 32, 1, 64)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " tf.reshape_7 (TFOpLambda)   (32, 32, 64)              0         \n",
      "                                                                 \n",
      " conv1d_27 (Conv1D)          (32, 32, 64)              12352     \n",
      "                                                                 \n",
      " conv1d_28 (Conv1D)          (32, 32, 64)              12352     \n",
      "                                                                 \n",
      " conv1d_29 (Conv1D)          (32, 32, 64)              12352     \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (32, 1, 64)              0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dense_8 (Dense)             (32, 1, 2048)             133120    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (32, 1, 1)                2049      \n",
      "                                                                 \n",
      " tf.reshape_8 (TFOpLambda)   (32, 1)                   0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 206,863\n",
      "Trainable params: 206,863\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/16\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 0.5909 - accuracy: 0.7763 - val_loss: 0.5799 - val_accuracy: 0.8012\n",
      "Epoch 2/16\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 0.5774 - accuracy: 0.8152 - val_loss: 0.5759 - val_accuracy: 0.8213\n",
      "Epoch 3/16\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 0.5790 - accuracy: 0.8134 - val_loss: 0.5759 - val_accuracy: 0.8213\n",
      "Epoch 4/16\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 0.5790 - accuracy: 0.8134 - val_loss: 0.5759 - val_accuracy: 0.8213\n",
      "Epoch 5/16\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 0.5790 - accuracy: 0.8134 - val_loss: 0.5759 - val_accuracy: 0.8213\n",
      "Epoch 6/16\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 0.5790 - accuracy: 0.8134 - val_loss: 0.5759 - val_accuracy: 0.8213\n",
      "Epoch 7/16\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 0.5790 - accuracy: 0.8134 - val_loss: 0.5759 - val_accuracy: 0.8213\n",
      "Epoch 8/16\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 0.5790 - accuracy: 0.8134 - val_loss: 0.5759 - val_accuracy: 0.8213\n",
      "Epoch 9/16\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 0.5790 - accuracy: 0.8134 - val_loss: 0.5759 - val_accuracy: 0.8213\n",
      "Epoch 10/16\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 0.5790 - accuracy: 0.8134 - val_loss: 0.5759 - val_accuracy: 0.8213\n",
      "Epoch 11/16\n",
      "200/200 [==============================] - 19s 96ms/step - loss: 0.5790 - accuracy: 0.8134 - val_loss: 0.5759 - val_accuracy: 0.8213\n",
      "Epoch 12/16\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 0.5790 - accuracy: 0.8134 - val_loss: 0.5759 - val_accuracy: 0.8213\n",
      "Epoch 13/16\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 0.5790 - accuracy: 0.8134 - val_loss: 0.5759 - val_accuracy: 0.8213\n",
      "Epoch 14/16\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 0.5790 - accuracy: 0.8134 - val_loss: 0.5759 - val_accuracy: 0.8213\n",
      "Epoch 15/16\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 0.5790 - accuracy: 0.8134 - val_loss: 0.5759 - val_accuracy: 0.8213\n",
      "Epoch 16/16\n",
      "200/200 [==============================] - 21s 103ms/step - loss: 0.5790 - accuracy: 0.8134 - val_loss: 0.5759 - val_accuracy: 0.8213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.engine.functional.Functional at 0x18318383940>,\n",
       " <keras.callbacks.History at 0x1831850b4c0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(ablation=4000, B=250, kernel_size=3, epochs=16, dense_neurons=2048, conv1d_set1=3,conv1d_set2=3, maxpool_1=True,\n",
    "          hdfs_obJ_full_name='data\\\\hdfsobj_32_64_time_ip.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22a4b840-90ed-4f36-a183-7e4855643ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of lines in the log file: 11175629\n",
      "RAM usage:  100232088\n",
      "loaded logs in memory in time: 4.975688219070435\n",
      "loaded cleaned logs with blk_id  in memory: 207.84787678718567\n",
      "RAM usage:  100232088\n",
      "loaded cleaned logs without blkid in memory: 0.4926776885986328\n",
      "RAM usage:  100232088\n",
      "starting training the tokenizer:\n",
      "ending tokenizer training: 109.17058253288269\n",
      "RAM usage:  48\n",
      "vocabulary size: 42\n",
      "starting text to number conversion\n",
      "completed:  0\n",
      "time : 0.0\n",
      "completed:  1000000\n",
      "time : 17.362428188323975\n",
      "completed:  2000000\n",
      "time : 34.01002812385559\n",
      "completed:  3000000\n",
      "time : 50.87336492538452\n",
      "completed:  4000000\n",
      "time : 68.24760246276855\n",
      "completed:  5000000\n",
      "time : 85.03499841690063\n",
      "completed:  6000000\n",
      "time : 102.18086194992065\n",
      "completed:  7000000\n",
      "time : 119.44902229309082\n",
      "completed:  8000000\n",
      "time : 136.55510306358337\n",
      "completed:  9000000\n",
      "time : 153.98451733589172\n",
      "completed:  10000000\n",
      "time : 171.1959855556488\n",
      "completed:  11000000\n",
      "time : 188.76276683807373\n",
      "ending text to number conversion: 191.69426035881042\n",
      "RAM usage:  100232088\n",
      "ending num_sequence_by_blkid conversion: 14.976683378219604\n",
      "RAM usage:  20971608\n",
      "ending labelled_num_seq_df conversion: 17.130167961120605\n",
      "RAM usage:  117329426\n",
      "len of pos_df 16838\n",
      "len of neg_df 558223\n",
      "eq_neg_pos_df shape: (33676, 2)\n",
      "train_hdfs_anomaly:, 800, val_hdfs_anomaly:, 100, test_hdfs_anomaly:, 100, train_hdfs_normal:, 800, val_hdfs_normal:, 100, test_hdfs_normal:, 100, train: hdfs_anomaly    800\n",
      "hdfs_normal     800\n",
      "Name: label, dtype: int64\n",
      "val: hdfs_anomaly    100\n",
      "hdfs_normal     100\n",
      "Name: label, dtype: int64\n",
      "test: hdfs_anomaly    100\n",
      "hdfs_normal     100\n",
      "Name: label, dtype: int64\n",
      "train_data <BatchDataset element_spec=(TensorSpec(shape=(32, 64, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "val_data <BatchDataset element_spec=(TensorSpec(shape=(32, 64, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "test_data <BatchDataset element_spec=(TensorSpec(shape=(32, 64, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "char in lines, train_data.element_spec[0].shape[2] 176\n",
      "num classes, train_data.element_spec[1].shape:  (32,)\n",
      "length of val_data: 6\n",
      "length of train_data - (num_seq_per_cls * num_class)// batch size: 50\n",
      "saved hdfs object as data\\hdfsobj_64_176_time_ip.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data\\\\hdfsobj_64_176_time_ip.pkl'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 64 seq len 176 \n",
    "hlog_meta_64_176_time_ip = HDFSLogv3(debug=True )\n",
    "res_64_176_time_ip = hlog_meta_64_176_time_ip.get_tensor_train_val_test(train_ratio=0.8, padded_char_len=176, \n",
    "                                                                        padded_seq_len=64,\n",
    "                hdfs_rm_time_stamp=False, hdfs_rm_ip_address=False,)\n",
    "hlog_meta_64_176_time_ip.save_hdfs_log_obj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a169016f-79a3-4be5-be85-3e9a76d225c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_hdfs_anomaly:, 3200, val_hdfs_anomaly:, 400, test_hdfs_anomaly:, 400, train_hdfs_normal:, 3200, val_hdfs_normal:, 400, test_hdfs_normal:, 400, train: hdfs_anomaly    3200\n",
      "hdfs_normal     3200\n",
      "Name: label, dtype: int64\n",
      "val: hdfs_anomaly    400\n",
      "hdfs_normal     400\n",
      "Name: label, dtype: int64\n",
      "test: hdfs_anomaly    400\n",
      "hdfs_normal     400\n",
      "Name: label, dtype: int64\n",
      "train_data <BatchDataset element_spec=(TensorSpec(shape=(32, 64, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "val_data <BatchDataset element_spec=(TensorSpec(shape=(32, 64, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "test_data <BatchDataset element_spec=(TensorSpec(shape=(32, 64, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "char in lines, train_data.element_spec[0].shape[2] 176\n",
      "num classes, train_data.element_spec[1].shape:  (32,)\n",
      "length of val_data: 25\n",
      "length of train_data - (num_seq_per_cls * num_class)// batch size: 200\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(32, 64, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "vocab_size: 42\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(32, 64, 176)]           0         \n",
      "                                                                 \n",
      " embedding_11 (Embedding)    (32, 64, 176, 42)         1806      \n",
      "                                                                 \n",
      " conv1d_30 (Conv1D)          (32, 64, 176, 64)         8128      \n",
      "                                                                 \n",
      " conv1d_31 (Conv1D)          (32, 64, 176, 64)         12352     \n",
      "                                                                 \n",
      " conv1d_32 (Conv1D)          (32, 64, 176, 64)         12352     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (32, 64, 1, 64)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " tf.reshape_9 (TFOpLambda)   (32, 64, 64)              0         \n",
      "                                                                 \n",
      " conv1d_33 (Conv1D)          (32, 64, 64)              12352     \n",
      "                                                                 \n",
      " conv1d_34 (Conv1D)          (32, 64, 64)              12352     \n",
      "                                                                 \n",
      " conv1d_35 (Conv1D)          (32, 64, 64)              12352     \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (32, 1, 64)              0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dense_10 (Dense)            (32, 1, 2048)             133120    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (32, 1, 1)                2049      \n",
      "                                                                 \n",
      " tf.reshape_10 (TFOpLambda)  (32, 1)                   0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 206,863\n",
      "Trainable params: 206,863\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/16\n",
      "200/200 [==============================] - 95s 472ms/step - loss: 0.6080 - accuracy: 0.7231 - val_loss: 0.5991 - val_accuracy: 0.7475\n",
      "Epoch 2/16\n",
      "200/200 [==============================] - 94s 471ms/step - loss: 0.6008 - accuracy: 0.7430 - val_loss: 0.5991 - val_accuracy: 0.7475\n",
      "Epoch 3/16\n",
      "200/200 [==============================] - 94s 469ms/step - loss: 0.6008 - accuracy: 0.7430 - val_loss: 0.5991 - val_accuracy: 0.7475\n",
      "Epoch 4/16\n",
      "200/200 [==============================] - 93s 464ms/step - loss: 0.6008 - accuracy: 0.7430 - val_loss: 0.5991 - val_accuracy: 0.7475\n",
      "Epoch 5/16\n",
      "200/200 [==============================] - 93s 467ms/step - loss: 0.6008 - accuracy: 0.7430 - val_loss: 0.5991 - val_accuracy: 0.7475\n",
      "Epoch 6/16\n",
      "200/200 [==============================] - 94s 468ms/step - loss: 0.6008 - accuracy: 0.7430 - val_loss: 0.5991 - val_accuracy: 0.7475\n",
      "Epoch 7/16\n",
      "200/200 [==============================] - 93s 465ms/step - loss: 0.6008 - accuracy: 0.7430 - val_loss: 0.5991 - val_accuracy: 0.7475\n",
      "Epoch 8/16\n",
      "200/200 [==============================] - 93s 467ms/step - loss: 0.6008 - accuracy: 0.7430 - val_loss: 0.5991 - val_accuracy: 0.7475\n",
      "Epoch 9/16\n",
      "200/200 [==============================] - 94s 469ms/step - loss: 0.6008 - accuracy: 0.7430 - val_loss: 0.5991 - val_accuracy: 0.7475\n",
      "Epoch 10/16\n",
      "200/200 [==============================] - 93s 465ms/step - loss: 0.6008 - accuracy: 0.7430 - val_loss: 0.5991 - val_accuracy: 0.7475\n",
      "Epoch 11/16\n",
      "200/200 [==============================] - 93s 465ms/step - loss: 0.6008 - accuracy: 0.7430 - val_loss: 0.5991 - val_accuracy: 0.7475\n",
      "Epoch 12/16\n",
      "200/200 [==============================] - 93s 466ms/step - loss: 0.6008 - accuracy: 0.7430 - val_loss: 0.5991 - val_accuracy: 0.7475\n",
      "Epoch 13/16\n",
      "200/200 [==============================] - 93s 466ms/step - loss: 0.6008 - accuracy: 0.7430 - val_loss: 0.5991 - val_accuracy: 0.7475\n",
      "Epoch 14/16\n",
      "200/200 [==============================] - 93s 465ms/step - loss: 0.7369 - accuracy: 0.5875 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 15/16\n",
      "200/200 [==============================] - 95s 473ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 16/16\n",
      "200/200 [==============================] - 93s 465ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.engine.functional.Functional at 0x189239f51f0>,\n",
       " <keras.callbacks.History at 0x18923605430>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(ablation=4000, B=250, kernel_size=3, epochs=16, dense_neurons=2048, conv1d_set1=3,conv1d_set2=3, maxpool_1=True,\n",
    "          hdfs_obJ_full_name='data\\\\hdfsobj_64_176_time_ip.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5391fc14-d798-41a4-8733-c797a307b4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of lines in the log file: 11175629\n",
      "RAM usage:  100232088\n",
      "loaded logs in memory in time: 5.232716798782349\n",
      "loaded cleaned logs with blk_id  in memory: 86.41884183883667\n",
      "RAM usage:  100232088\n",
      "loaded cleaned logs without blkid in memory: 0.5340242385864258\n",
      "RAM usage:  100232088\n",
      "starting training the tokenizer:\n",
      "ending tokenizer training: 71.64438652992249\n",
      "RAM usage:  48\n",
      "vocabulary size: 42\n",
      "starting text to number conversion\n",
      "completed:  0\n",
      "time : 0.0010061264038085938\n",
      "completed:  1000000\n",
      "time : 12.6530282497406\n",
      "completed:  2000000\n",
      "time : 24.142647981643677\n",
      "completed:  3000000\n",
      "time : 37.025145053863525\n",
      "completed:  4000000\n",
      "time : 49.55745768547058\n",
      "completed:  5000000\n",
      "time : 62.06629419326782\n",
      "completed:  6000000\n",
      "time : 74.66933369636536\n",
      "completed:  7000000\n",
      "time : 87.49184608459473\n",
      "completed:  8000000\n",
      "time : 100.31672430038452\n",
      "completed:  9000000\n",
      "time : 113.1926257610321\n",
      "completed:  10000000\n",
      "time : 126.06050968170166\n",
      "completed:  11000000\n",
      "time : 139.07091450691223\n",
      "ending text to number conversion: 141.4505958557129\n",
      "RAM usage:  100232088\n",
      "ending num_sequence_by_blkid conversion: 12.454441547393799\n",
      "RAM usage:  20971608\n",
      "ending labelled_num_seq_df conversion: 17.524921655654907\n",
      "RAM usage:  117329426\n",
      "len of pos_df 16838\n",
      "len of neg_df 558223\n",
      "eq_neg_pos_df shape: (33676, 2)\n",
      "train_hdfs_anomaly:, 800, val_hdfs_anomaly:, 100, test_hdfs_anomaly:, 100, train_hdfs_normal:, 800, val_hdfs_normal:, 100, test_hdfs_normal:, 100, train: hdfs_anomaly    800\n",
      "hdfs_normal     800\n",
      "Name: label, dtype: int64\n",
      "val: hdfs_anomaly    100\n",
      "hdfs_normal     100\n",
      "Name: label, dtype: int64\n",
      "test: hdfs_anomaly    100\n",
      "hdfs_normal     100\n",
      "Name: label, dtype: int64\n",
      "train_data <BatchDataset element_spec=(TensorSpec(shape=(32, 64, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "val_data <BatchDataset element_spec=(TensorSpec(shape=(32, 64, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "test_data <BatchDataset element_spec=(TensorSpec(shape=(32, 64, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "char in lines, train_data.element_spec[0].shape[2] 176\n",
      "num classes, train_data.element_spec[1].shape:  (32,)\n",
      "length of val_data: 6\n",
      "length of train_data - (num_seq_per_cls * num_class)// batch size: 50\n",
      "saved hdfs object as data\\hdfsobj_64_176_no_meta.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data\\\\hdfsobj_64_176_no_meta.pkl'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hlog_meta_64_176_no_meta = HDFSLogv3(debug=True )\n",
    "res_64_176_no_meta = hlog_meta_64_176_no_meta.get_tensor_train_val_test(train_ratio=0.8, padded_char_len=176, \n",
    "                                                                        padded_seq_len=64,\n",
    "                hdfs_rm_time_stamp=True, hdfs_rm_ip_address=True,)\n",
    "hlog_meta_64_176_no_meta.save_hdfs_log_obj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e09ce9bb-5bb0-4cdf-9da4-f44459c24682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_hdfs_anomaly:, 3200, val_hdfs_anomaly:, 400, test_hdfs_anomaly:, 400, train_hdfs_normal:, 3200, val_hdfs_normal:, 400, test_hdfs_normal:, 400, train: hdfs_anomaly    3200\n",
      "hdfs_normal     3200\n",
      "Name: label, dtype: int64\n",
      "val: hdfs_anomaly    400\n",
      "hdfs_normal     400\n",
      "Name: label, dtype: int64\n",
      "test: hdfs_anomaly    400\n",
      "hdfs_normal     400\n",
      "Name: label, dtype: int64\n",
      "train_data <BatchDataset element_spec=(TensorSpec(shape=(32, 64, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "val_data <BatchDataset element_spec=(TensorSpec(shape=(32, 64, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "test_data <BatchDataset element_spec=(TensorSpec(shape=(32, 64, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "char in lines, train_data.element_spec[0].shape[2] 176\n",
      "num classes, train_data.element_spec[1].shape:  (32,)\n",
      "length of val_data: 25\n",
      "length of train_data - (num_seq_per_cls * num_class)// batch size: 200\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(32, 64, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "vocab_size: 42\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(32, 64, 176)]           0         \n",
      "                                                                 \n",
      " embedding_13 (Embedding)    (32, 64, 176, 42)         1806      \n",
      "                                                                 \n",
      " conv1d_36 (Conv1D)          (32, 64, 176, 64)         8128      \n",
      "                                                                 \n",
      " conv1d_37 (Conv1D)          (32, 64, 176, 64)         12352     \n",
      "                                                                 \n",
      " conv1d_38 (Conv1D)          (32, 64, 176, 64)         12352     \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (32, 64, 1, 64)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " tf.reshape_11 (TFOpLambda)  (32, 64, 64)              0         \n",
      "                                                                 \n",
      " conv1d_39 (Conv1D)          (32, 64, 64)              12352     \n",
      "                                                                 \n",
      " conv1d_40 (Conv1D)          (32, 64, 64)              12352     \n",
      "                                                                 \n",
      " conv1d_41 (Conv1D)          (32, 64, 64)              12352     \n",
      "                                                                 \n",
      " max_pooling1d_6 (MaxPooling  (32, 1, 64)              0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dense_12 (Dense)            (32, 1, 2048)             133120    \n",
      "                                                                 \n",
      " dense_13 (Dense)            (32, 1, 1)                2049      \n",
      "                                                                 \n",
      " tf.reshape_12 (TFOpLambda)  (32, 1)                   0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 206,863\n",
      "Trainable params: 206,863\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/16\n",
      "200/200 [==============================] - 93s 461ms/step - loss: 0.6070 - accuracy: 0.7316 - val_loss: 0.5991 - val_accuracy: 0.7475\n",
      "Epoch 2/16\n",
      "200/200 [==============================] - 93s 466ms/step - loss: 0.5988 - accuracy: 0.7509 - val_loss: 0.5996 - val_accuracy: 0.7462\n",
      "Epoch 3/16\n",
      "200/200 [==============================] - 93s 465ms/step - loss: 0.6692 - accuracy: 0.6662 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 4/16\n",
      "200/200 [==============================] - 93s 466ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 5/16\n",
      "200/200 [==============================] - 94s 472ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 6/16\n",
      "200/200 [==============================] - 93s 465ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 7/16\n",
      "200/200 [==============================] - 93s 466ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 8/16\n",
      "200/200 [==============================] - 94s 468ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 9/16\n",
      "200/200 [==============================] - 93s 465ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 10/16\n",
      "200/200 [==============================] - 93s 465ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 11/16\n",
      "200/200 [==============================] - 93s 466ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 12/16\n",
      "200/200 [==============================] - 93s 466ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 13/16\n",
      "200/200 [==============================] - 93s 467ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 14/16\n",
      "200/200 [==============================] - 26358s 132s/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 15/16\n",
      "200/200 [==============================] - 100s 501ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n",
      "Epoch 16/16\n",
      "200/200 [==============================] - 97s 486ms/step - loss: 0.8133 - accuracy: 0.5000 - val_loss: 0.8133 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.engine.functional.Functional at 0x17ece5a1790>,\n",
       " <keras.callbacks.History at 0x181eae29550>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(ablation=4000, B=250, kernel_size=3, epochs=16, dense_neurons=2048, conv1d_set1=3,conv1d_set2=3, maxpool_1=True,\n",
    "          hdfs_obJ_full_name='data\\\\hdfsobj_64_176_no_meta.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d306fb34-d01e-4911-8ea7-19f7b12b680a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_hdfs_anomaly:, 6400, val_hdfs_anomaly:, 800, test_hdfs_anomaly:, 800, train_hdfs_normal:, 6400, val_hdfs_normal:, 800, test_hdfs_normal:, 800, train: hdfs_anomaly    6400\n",
      "hdfs_normal     6400\n",
      "Name: label, dtype: int64\n",
      "val: hdfs_anomaly    800\n",
      "hdfs_normal     800\n",
      "Name: label, dtype: int64\n",
      "test: hdfs_anomaly    800\n",
      "hdfs_normal     800\n",
      "Name: label, dtype: int64\n",
      "train_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "val_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "test_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "char in lines, train_data.element_spec[0].shape[2] 176\n",
      "num classes, train_data.element_spec[1].shape:  (32,)\n",
      "length of val_data: 50\n",
      "length of train_data - (num_seq_per_cls * num_class)// batch size: 400\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "vocab_size: 42\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(32, 32, 176)]           0         \n",
      "                                                                 \n",
      " embedding_15 (Embedding)    (32, 32, 176, 42)         1806      \n",
      "                                                                 \n",
      " conv1d_42 (Conv1D)          (32, 32, 176, 64)         8128      \n",
      "                                                                 \n",
      " conv1d_43 (Conv1D)          (32, 32, 176, 64)         12352     \n",
      "                                                                 \n",
      " conv1d_44 (Conv1D)          (32, 32, 176, 64)         12352     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (32, 32, 1, 64)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " tf.reshape_13 (TFOpLambda)  (32, 32, 64)              0         \n",
      "                                                                 \n",
      " conv1d_45 (Conv1D)          (32, 32, 64)              12352     \n",
      "                                                                 \n",
      " conv1d_46 (Conv1D)          (32, 32, 64)              12352     \n",
      "                                                                 \n",
      " conv1d_47 (Conv1D)          (32, 32, 64)              12352     \n",
      "                                                                 \n",
      " max_pooling1d_7 (MaxPooling  (32, 1, 64)              0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dense_14 (Dense)            (32, 1, 2048)             133120    \n",
      "                                                                 \n",
      " dense_15 (Dense)            (32, 1, 1)                2049      \n",
      "                                                                 \n",
      " tf.reshape_14 (TFOpLambda)  (32, 1)                   0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 206,863\n",
      "Trainable params: 206,863\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/16\n",
      "400/400 [==============================] - 95s 236ms/step - loss: 0.5885 - accuracy: 0.7859 - val_loss: 0.5784 - val_accuracy: 0.8150\n",
      "Epoch 2/16\n",
      "400/400 [==============================] - 94s 236ms/step - loss: 0.5781 - accuracy: 0.8160 - val_loss: 0.5784 - val_accuracy: 0.8150\n",
      "Epoch 3/16\n",
      "400/400 [==============================] - 94s 236ms/step - loss: 0.5780 - accuracy: 0.8163 - val_loss: 0.5784 - val_accuracy: 0.8150\n",
      "Epoch 4/16\n",
      "400/400 [==============================] - 96s 241ms/step - loss: 0.5780 - accuracy: 0.8163 - val_loss: 0.5784 - val_accuracy: 0.8150\n",
      "Epoch 5/16\n",
      "400/400 [==============================] - 94s 235ms/step - loss: 0.5780 - accuracy: 0.8163 - val_loss: 0.5784 - val_accuracy: 0.8150\n",
      "Epoch 6/16\n",
      "400/400 [==============================] - 96s 241ms/step - loss: 0.5780 - accuracy: 0.8163 - val_loss: 0.5784 - val_accuracy: 0.8150\n",
      "Epoch 7/16\n",
      "400/400 [==============================] - 95s 238ms/step - loss: 0.5780 - accuracy: 0.8163 - val_loss: 0.5784 - val_accuracy: 0.8150\n",
      "Epoch 8/16\n",
      "400/400 [==============================] - 97s 241ms/step - loss: 0.5780 - accuracy: 0.8163 - val_loss: 0.5784 - val_accuracy: 0.8150\n",
      "Epoch 9/16\n",
      "400/400 [==============================] - 96s 240ms/step - loss: 0.5780 - accuracy: 0.8163 - val_loss: 0.5784 - val_accuracy: 0.8150\n",
      "Epoch 10/16\n",
      "400/400 [==============================] - 96s 240ms/step - loss: 0.5780 - accuracy: 0.8163 - val_loss: 0.5784 - val_accuracy: 0.8150\n",
      "Epoch 11/16\n",
      "400/400 [==============================] - 95s 238ms/step - loss: 0.5780 - accuracy: 0.8163 - val_loss: 0.5784 - val_accuracy: 0.8150\n",
      "Epoch 12/16\n",
      "400/400 [==============================] - 95s 238ms/step - loss: 0.5780 - accuracy: 0.8163 - val_loss: 0.5784 - val_accuracy: 0.8150\n",
      "Epoch 13/16\n",
      "400/400 [==============================] - 99s 247ms/step - loss: 0.5780 - accuracy: 0.8163 - val_loss: 0.5784 - val_accuracy: 0.8150\n",
      "Epoch 14/16\n",
      "400/400 [==============================] - 100s 250ms/step - loss: 0.5780 - accuracy: 0.8163 - val_loss: 0.5784 - val_accuracy: 0.8150\n",
      "Epoch 15/16\n",
      "400/400 [==============================] - 98s 245ms/step - loss: 0.5780 - accuracy: 0.8163 - val_loss: 0.5784 - val_accuracy: 0.8150\n",
      "Epoch 16/16\n",
      "400/400 [==============================] - 95s 238ms/step - loss: 0.5780 - accuracy: 0.8163 - val_loss: 0.5784 - val_accuracy: 0.8150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.engine.functional.Functional at 0x181e9e320d0>,\n",
       " <keras.callbacks.History at 0x181e9e2b400>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(ablation=8000, B=250, kernel_size=3, epochs=16, dense_neurons=2048, conv1d_set1=3,conv1d_set2=3, maxpool_1=True,\n",
    "          hdfs_obJ_full_name='data\\\\hdfsobj_32_176_time_ip.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5bec4aa3-deec-4d1f-aa21-fe6f6132d1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_hdfs_anomaly:, 12800, val_hdfs_anomaly:, 1600, test_hdfs_anomaly:, 1600, train_hdfs_normal:, 12800, val_hdfs_normal:, 1600, test_hdfs_normal:, 1600, train: hdfs_anomaly    12800\n",
      "hdfs_normal     12800\n",
      "Name: label, dtype: int64\n",
      "val: hdfs_anomaly    1600\n",
      "hdfs_normal     1600\n",
      "Name: label, dtype: int64\n",
      "test: hdfs_anomaly    1600\n",
      "hdfs_normal     1600\n",
      "Name: label, dtype: int64\n",
      "train_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 64), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "val_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 64), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "test_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 64), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "char in lines, train_data.element_spec[0].shape[2] 64\n",
      "num classes, train_data.element_spec[1].shape:  (32,)\n",
      "length of val_data: 100\n",
      "length of train_data - (num_seq_per_cls * num_class)// batch size: 800\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(32, 32, 64), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "vocab_size: 42\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(32, 32, 64)]            0         \n",
      "                                                                 \n",
      " embedding_19 (Embedding)    (32, 32, 64, 42)          1806      \n",
      "                                                                 \n",
      " conv1d_54 (Conv1D)          (32, 32, 64, 64)          8128      \n",
      "                                                                 \n",
      " conv1d_55 (Conv1D)          (32, 32, 64, 64)          12352     \n",
      "                                                                 \n",
      " conv1d_56 (Conv1D)          (32, 32, 64, 64)          12352     \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (32, 32, 1, 64)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " tf.reshape_17 (TFOpLambda)  (32, 32, 64)              0         \n",
      "                                                                 \n",
      " conv1d_57 (Conv1D)          (32, 32, 64)              12352     \n",
      "                                                                 \n",
      " conv1d_58 (Conv1D)          (32, 32, 64)              12352     \n",
      "                                                                 \n",
      " conv1d_59 (Conv1D)          (32, 32, 64)              12352     \n",
      "                                                                 \n",
      " max_pooling1d_9 (MaxPooling  (32, 1, 64)              0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dense_18 (Dense)            (32, 1, 2048)             133120    \n",
      "                                                                 \n",
      " dense_19 (Dense)            (32, 1, 1)                2049      \n",
      "                                                                 \n",
      " tf.reshape_18 (TFOpLambda)  (32, 1)                   0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 206,863\n",
      "Trainable params: 206,863\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "800/800 [==============================] - 75s 93ms/step - loss: 0.6104 - accuracy: 0.7197 - val_loss: 0.6649 - val_accuracy: 0.5744\n",
      "Epoch 2/5\n",
      "800/800 [==============================] - 74s 93ms/step - loss: 0.6110 - accuracy: 0.7162 - val_loss: 0.6649 - val_accuracy: 0.5744\n",
      "Epoch 3/5\n",
      "800/800 [==============================] - 74s 92ms/step - loss: 0.6110 - accuracy: 0.7162 - val_loss: 0.6649 - val_accuracy: 0.5744\n",
      "Epoch 4/5\n",
      "800/800 [==============================] - 77s 96ms/step - loss: 0.6110 - accuracy: 0.7162 - val_loss: 0.6649 - val_accuracy: 0.5744\n",
      "Epoch 5/5\n",
      "800/800 [==============================] - 78s 97ms/step - loss: 0.6110 - accuracy: 0.7162 - val_loss: 0.6649 - val_accuracy: 0.5744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.engine.functional.Functional at 0x181fa0f1c70>,\n",
       " <keras.callbacks.History at 0x181fa0f95b0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 16K  with meta\n",
    "test_model(ablation=16000, B=32, kernel_size=3, epochs=5, dense_neurons=2048, conv1d_set1=3,conv1d_set2=3, maxpool_1=True,\n",
    "          hdfs_obJ_full_name='data\\\\hdfsobj_32_64_time_ip.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb043838-82bf-4b73-8ab8-a1822b8edd26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_hdfs_anomaly:, 12800, val_hdfs_anomaly:, 1600, test_hdfs_anomaly:, 1600, train_hdfs_normal:, 12800, val_hdfs_normal:, 1600, test_hdfs_normal:, 1600, train: hdfs_anomaly    12800\n",
      "hdfs_normal     12800\n",
      "Name: label, dtype: int64\n",
      "val: hdfs_anomaly    1600\n",
      "hdfs_normal     1600\n",
      "Name: label, dtype: int64\n",
      "test: hdfs_anomaly    1600\n",
      "hdfs_normal     1600\n",
      "Name: label, dtype: int64\n",
      "train_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "val_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "test_data <BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "char in lines, train_data.element_spec[0].shape[2] 176\n",
      "num classes, train_data.element_spec[1].shape:  (32,)\n",
      "length of val_data: 100\n",
      "length of train_data - (num_seq_per_cls * num_class)// batch size: 800\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(32, 32, 176), dtype=tf.int32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "vocab_size: 42\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(32, 32, 176)]           0         \n",
      "                                                                 \n",
      " embedding_21 (Embedding)    (32, 32, 176, 42)         1806      \n",
      "                                                                 \n",
      " conv1d_60 (Conv1D)          (32, 32, 176, 64)         8128      \n",
      "                                                                 \n",
      " conv1d_61 (Conv1D)          (32, 32, 176, 64)         12352     \n",
      "                                                                 \n",
      " conv1d_62 (Conv1D)          (32, 32, 176, 64)         12352     \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (32, 32, 1, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " tf.reshape_19 (TFOpLambda)  (32, 32, 64)              0         \n",
      "                                                                 \n",
      " conv1d_63 (Conv1D)          (32, 32, 64)              12352     \n",
      "                                                                 \n",
      " conv1d_64 (Conv1D)          (32, 32, 64)              12352     \n",
      "                                                                 \n",
      " conv1d_65 (Conv1D)          (32, 32, 64)              12352     \n",
      "                                                                 \n",
      " max_pooling1d_10 (MaxPoolin  (32, 1, 64)              0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dense_20 (Dense)            (32, 1, 2048)             133120    \n",
      "                                                                 \n",
      " dense_21 (Dense)            (32, 1, 1)                2049      \n",
      "                                                                 \n",
      " tf.reshape_20 (TFOpLambda)  (32, 1)                   0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 206,863\n",
      "Trainable params: 206,863\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "800/800 [==============================] - 191s 239ms/step - loss: 0.6064 - accuracy: 0.7333 - val_loss: 0.6645 - val_accuracy: 0.5756\n",
      "Epoch 2/5\n",
      "800/800 [==============================] - 191s 238ms/step - loss: 0.6293 - accuracy: 0.6687 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "800/800 [==============================] - 192s 240ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 4/5\n",
      "800/800 [==============================] - 191s 238ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "800/800 [==============================] - 194s 242ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.engine.functional.Functional at 0x181f9f02430>,\n",
       " <keras.callbacks.History at 0x1892273a640>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(ablation=16000, B=32, kernel_size=3, epochs=5, dense_neurons=2048, conv1d_set1=3,conv1d_set2=3, maxpool_1=True,\n",
    "          hdfs_obJ_full_name='data\\\\hdfsobj_32_176_time_ip.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3230f31e-1dd7-4d17-923c-bf595393e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(ablation=10000, B=32, kernel_size=3, epochs=16, dense_neurons=2048, conv1d_set1=3,conv1d_set2=3, maxpool_1=True,\n",
    "          hdfs_obJ_full_name='data\\\\hdfsobj_32_176_time_ip.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f7e47-3cc2-4815-8513-d822ea5179f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd61cb3-03d1-4548-8249-61bf8f0746af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fe7e7f-3e31-4d87-8c46-f419d8cd78fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d11534-7137-42c2-978f-82d73169cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 16K no meta\n",
    "test_model(ablation=16000, B=32, kernel_size=3, epochs=5, dense_neurons=2048, conv1d_set1=3,conv1d_set2=3, maxpool_1=True,\n",
    "          hdfs_obJ_full_name='data\\\\hdfsobj_32_64_no_meta.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791c715e-fd03-47df-9e8a-7b615ad2d0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e658c94e-712c-4fb5-b8bc-37f2d86bf4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4025111-1455-42b7-941a-aeebfb427db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d67c48e-0616-47ab-bb41-ad06a5b17297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
