{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56818695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# from hdflogv2 import HDFSLogv2\n",
    "from hdflogv3 import HDFSLogv3\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dc750d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "creating an instance of the hdfslogs, we will have to inspect the sequence and the loglines later to \n",
    "tune the values of sequence length and the characters in line. \n",
    "hlog_meta_176_32 = HDFSLogv3(train_ratio=0.8, padded_char_len=176, padded_seq_len=32,\n",
    "                rm_time_stamp=False, rm_ip_address=False, debug=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb069f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_train_test_data_text is to see the texts in sequence , for model training get_train_test_data_num will be used\n",
    "# the ablation parameter allows to take only 10 number each from positive and negative sequences\n",
    "x_train, y_train, x_test, y_test = hlogs.get_padded_train_test_data(ablation=1000, save_pkl=True,\n",
    "                                                                   hdfs_pkl_name='hdfsv3_32_176.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b881948c-3b15-450e-850f-822218310167",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlog_no_meta_176_32 = HDFSLogv3(train_ratio=0.8, padded_char_len=176, padded_seq_len=32,\n",
    "                rm_time_stamp=True, rm_ip_address=True, debug=True )\n",
    "x_train, y_train, x_test, y_test = hlogs.get_padded_train_test_data(ablation=1000, save_pkl=True,\n",
    "                                                                   hdfs_pkl_name='hdfsv3_32_176_no_meta.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5827569-2eae-46a6-9e39-bfc4d3dc66d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_data = hlogs.labelled_num_seq_df['LogNumSequence'].values\n",
    "# y_data = hlogs.labelled_num_seq_df['Label'].values\n",
    "# pos_idx = y_data > 0\n",
    "# x_pos = x_data[pos_idx]\n",
    "# y_pos = y_data[pos_idx]\n",
    "# x_neg = x_data[~pos_idx]\n",
    "# y_neg = y_data[~pos_idx]\n",
    "# ablation = 1000\n",
    "# train_ratio = 0.8\n",
    "\n",
    "\n",
    "# if ablation != 0 and x_pos.shape[0] >= ablation:\n",
    "#     x_pos = x_pos[0:ablation]\n",
    "#     y_pos = y_pos[0:ablation]\n",
    "#     x_neg = x_neg[0:ablation]\n",
    "#     y_neg = y_neg[0:ablation]\n",
    "#     print('ablation x_pos:', x_pos.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "878a1432-626b-4b63-b251-dfd13412c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDFS:\n",
    "    \n",
    "    def __init__(self, padded_num_seq_df, ablation=100, train_ratio=0.8, \n",
    "                 val_ratio=None, test_ratio=None, designated_ukc_cls=None, \n",
    "                 debug=True):\n",
    "        self.padded_num_seq_df = padded_num_seq_df\n",
    "        self.ablation = ablation\n",
    "        self.train_ratio = train_ratio\n",
    "        self.val_ratio = val_ratio\n",
    "        self.test_ratio = test_ratio\n",
    "        self.designated_ukc_cls = designated_ukc_cls\n",
    "        self.debug = debug        \n",
    "    \n",
    "    \n",
    "    def get_train_test_split_single_class(self, label=0):\n",
    "        bgldf = self.padded_num_seq_df\n",
    "        bgldf.rename(columns = {'Label':'label', 'LogNumSequence': 'seq'}, inplace = True)\n",
    "        bgldf.drop(['BlockId'], axis = 1, inplace = True)\n",
    "        train_data = None\n",
    "        val_data = None\n",
    "        test_data = None\n",
    "        ukc_data = None ### unknown known - not present in the training data but present in the test data        \n",
    "        \n",
    "        train_cnt = round(self.ablation * self.train_ratio)#### 1000 * 0.7 = 700\n",
    "        remaining_cnt = round(self.ablation *(1 - self.train_ratio)) ### 1000 * (1-0.7) = 300 \n",
    "        if self.val_ratio is None and self.test_ratio is None:\n",
    "            val_cnt = test_cnt = remaining_cnt//2 ### 300/2 = 150 each\n",
    "        else:\n",
    "            val_cnt = round(self.ablation * self.val_ratio) ### 1000 * 0.2 = 200 \n",
    "            test_cnt = round(self.ablation * self.test_ratio) ### 1000 * 0.1 = 100\n",
    "        \n",
    "        cls_data = bgldf[bgldf.label==label]\n",
    "        cls_data_cnt = cls_data.count()[0]\n",
    "        cls_unique_label = int(np.unique(cls_data.label)[0])\n",
    "        #if self.debug: print('cls_unique_label', cls_unique_label)\n",
    "        if self.designated_ukc_cls == cls_unique_label:\n",
    "            if cls_data_cnt < test_cnt:\n",
    "                ukc_data = cls_data[0:cls_data_cnt]\n",
    "            else:\n",
    "                ukc_data = cls_data[0:test_cnt]\n",
    "            print(f'class {cls_unique_label} is added as ukc')\n",
    "        else:\n",
    "            if self.ablation <= cls_data_cnt: ### if 1000 <= 2000            \n",
    "                train_data = cls_data[0:train_cnt] ### cls_data[0:700]\n",
    "                val_data = cls_data[train_cnt:train_cnt+val_cnt] ### cls_data[700:(700+200)]\n",
    "                test_data = cls_data[train_cnt+val_cnt:self.ablation] ### cls_data[900:1000]\n",
    "            elif self.ablation > cls_data_cnt and cls_data_cnt >= train_cnt+val_cnt: ### 1000>950 and 950>(700+200)\n",
    "                train_data = cls_data[0:train_cnt] ### cls_data[0:700]\n",
    "                remaining_for_test = cls_data_cnt - (train_cnt+val_cnt) ### 950 - (700+200) = 50\n",
    "                if remaining_for_test > 0: ### 50 > 0\n",
    "                    val_data = cls_data[train_cnt:train_cnt+val_cnt] ### cls_data[700:(700+200)]\n",
    "                    test_data = cls_data[train_cnt+val_cnt:cls_data_cnt] ### cls_data[900:950]\n",
    "                else: ### cls_data_cnt = 850 or 900\n",
    "                    val_data = cls_data[train_cnt:cls_data_cnt] ### cls_data[700:850]\n",
    "            else:\n",
    "                if self.debug:\n",
    "                    print(f'{cls_data_cnt} data in class {label} not enough to split into train:{train_cnt} and validation:{val_cnt}, adding the entire data as ukc')\n",
    "                if self.designated_ukc_cls is None:    \n",
    "                    if cls_data_cnt < test_cnt:\n",
    "                        ukc_data = cls_data[0:cls_data_cnt]\n",
    "                    else:\n",
    "                        ukc_data = cls_data[0:test_cnt]\n",
    "        # if self.debug:    \n",
    "        if train_data is not None:\n",
    "            print(f'train_{label}:, {train_data.count()[0]}', end=', ')\n",
    "        if val_data is not None:\n",
    "            print(f'val_{label}:, {val_data.count()[0]}', end=', ')\n",
    "        if test_data is not None:\n",
    "            print(f'test_{label}:, {test_data.count()[0]}', end=', ')\n",
    "        if ukc_data is not None:\n",
    "            print(f'ukc_{label}:, {ukc_data.count()[0]}')\n",
    "        return train_data, val_data,test_data, ukc_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "822caf88-6906-4d7e-be3a-3ee6b63819b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# with open('hlog.pkl', 'wb') as f:\n",
    "#     pickle.dump(hlogs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "423a117d-d0fd-477c-a6cb-80c36ecb78d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join('data', 'hlog_no_meta_176_32.pkl')\n",
    "with open(file_name, 'rb') as f:\n",
    "    hlog_no_meta_176_32 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4efde3d-515a-4e72-866a-b194a401c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hlogs.labelled_num_seq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9bf1671-5ac4-4b66-8a22-7aaa65370392",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs = HDFS(hlog_no_meta_176_32.labelled_num_seq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcde6cfd-4854-4cf0-aaad-486dd1ce3d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_0:, 80, val_0:, 10, test_0:, 10, "
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                                  seq  label\n",
       " 0   [[3, 14, 2, 2, 2, 2, 3, 28, 20, 20, 11, 6, 6, ...      0\n",
       " 1   [[3, 14, 2, 2, 2, 3, 3, 6, 3, 2, 3, 22, 21, 2,...      0\n",
       " 2   [[3, 14, 2, 2, 2, 2, 3, 28, 20, 27, 2, 11, 6, ...      0\n",
       " 3   [[3, 14, 2, 2, 2, 3, 3, 2, 11, 2, 2, 27, 11, 1...      0\n",
       " 4   [[3, 14, 2, 2, 2, 3, 6, 6, 2, 20, 21, 2, 2, 11...      0\n",
       " ..                                                ...    ...\n",
       " 77  [[3, 14, 2, 2, 2, 3, 3, 3, 2, 2, 6, 22, 6, 22,...      0\n",
       " 78  [[3, 14, 2, 2, 2, 2, 3, 28, 21, 14, 21, 2, 6, ...      0\n",
       " 79  [[3, 14, 2, 2, 2, 2, 3, 27, 6, 14, 3, 2, 6, 21...      0\n",
       " 80  [[3, 14, 2, 2, 2, 2, 3, 22, 11, 22, 6, 22, 6, ...      0\n",
       " 81  [[3, 14, 2, 2, 2, 3, 3, 3, 21, 11, 21, 28, 6, ...      0\n",
       " \n",
       " [80 rows x 2 columns],\n",
       "                                                   seq  label\n",
       " 82  [[3, 14, 2, 2, 2, 2, 2, 3, 11, 27, 20, 28, 6, ...      0\n",
       " 83  [[3, 14, 2, 2, 2, 2, 3, 28, 2, 11, 3, 6, 6, 11...      0\n",
       " 84  [[3, 14, 2, 2, 3, 28, 6, 21, 11, 2, 3, 3, 21, ...      0\n",
       " 85  [[3, 14, 2, 2, 2, 3, 2, 3, 20, 22, 11, 11, 6, ...      0\n",
       " 86  [[3, 14, 2, 2, 2, 2, 3, 28, 3, 22, 6, 14, 6, 1...      0\n",
       " 87  [[3, 14, 2, 2, 2, 2, 3, 22, 6, 21, 11, 3, 6, 6...      0\n",
       " 88  [[3, 14, 2, 2, 2, 3, 6, 6, 11, 2, 20, 27, 2, 2...      0\n",
       " 89  [[3, 14, 2, 2, 2, 3, 2, 6, 2, 3, 11, 11, 2, 3,...      0\n",
       " 90  [[3, 14, 2, 2, 2, 2, 3, 22, 20, 2, 11, 6, 6, 6...      0\n",
       " 91  [[3, 14, 2, 2, 2, 3, 6, 6, 3, 20, 11, 27, 2, 1...      0,\n",
       "                                                    seq  label\n",
       " 92   [[3, 14, 2, 2, 2, 3, 2, 6, 2, 22, 6, 2, 2, 3, ...      0\n",
       " 93   [[3, 14, 2, 2, 2, 2, 3, 21, 6, 11, 21, 14, 2, ...      0\n",
       " 94   [[3, 14, 2, 2, 2, 2, 2, 3, 2, 2, 2, 14, 6, 22,...      0\n",
       " 95   [[3, 14, 2, 2, 2, 2, 3, 20, 11, 6, 6, 14, 21, ...      0\n",
       " 96   [[3, 14, 2, 2, 3, 28, 6, 2, 21, 6, 2, 2, 6, 21...      0\n",
       " 97   [[3, 14, 2, 2, 2, 2, 3, 22, 3, 22, 6, 20, 6, 2...      0\n",
       " 98   [[3, 14, 2, 2, 2, 2, 3, 20, 6, 14, 2, 14, 2, 2...      0\n",
       " 99   [[3, 14, 2, 2, 2, 3, 3, 2, 3, 11, 11, 2, 21, 2...      0\n",
       " 100  [[3, 14, 2, 2, 2, 3, 6, 2, 11, 2, 3, 20, 2, 11...      0\n",
       " 101  [[3, 14, 2, 2, 3, 28, 6, 2, 3, 20, 6, 21, 2, 6...      0,\n",
       " None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs.get_train_test_split_single_class(label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14c67a1b-f2fe-4d8a-8650-dcbfcf1db383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs log object pks file size in GB:  22.62341594696045\n"
     ]
    }
   ],
   "source": [
    "print('hdfs log object pks file size in GB: ', 23722371/(1024*1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f26d41-3d98-4539-9061-2d3a8a661f68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
