{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "498ea522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdflogv1 import HDFSLogv1\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.random.set_seed(123)\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "973ba6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfslogs = HDFSLogv1(padded_seq_len=32,\n",
    "                         padded_char_len=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b62edf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of lines in the log file: 11175629\n",
      "RAM usage:  91719104\n",
      "ending logs in memory: 5.253721714019775\n",
      "loaded cleaned logs with blk_id  in memory: 89.4391872882843\n",
      "RAM usage:  91719104\n",
      "loaded cleaned logs without blkid in memory: 0.4410388469696045\n",
      "RAM usage:  91719104\n",
      "starting training the tokenizer:\n",
      "ending tokenizer training: 183.61084413528442\n",
      "RAM usage:  48\n",
      "vocabulary size: 42\n",
      "starting text to number conversion\n",
      "completed:  0\n",
      "time : 0.0\n",
      "completed:  1000000\n",
      "time : 16.5333034992218\n",
      "completed:  2000000\n",
      "time : 31.730684280395508\n",
      "completed:  3000000\n",
      "time : 48.962820529937744\n",
      "completed:  4000000\n",
      "time : 65.35843014717102\n",
      "completed:  5000000\n",
      "time : 82.02002739906311\n",
      "completed:  6000000\n",
      "time : 98.68645477294922\n",
      "completed:  7000000\n",
      "time : 115.7013008594513\n",
      "completed:  8000000\n",
      "time : 132.62197065353394\n",
      "completed:  9000000\n",
      "time : 149.62989830970764\n",
      "completed:  10000000\n",
      "time : 166.54741525650024\n",
      "completed:  11000000\n",
      "time : 183.4408519268036\n",
      "ending text to number conversion: 186.51735496520996\n",
      "RAM usage:  91719104\n",
      "ending num_sequence_by_blkid conversion: 385.09153008461\n",
      "RAM usage:  20971608\n",
      "ending labelled_num_seq_df conversion: 387.89361119270325\n",
      "RAM usage:  181824789\n",
      "13470 3368\n",
      "train_test_data done: 388.0506155490875\n",
      "RAM usage train_test_data:  72\n",
      "length of train  sequence original 4\n",
      "length of train  sequence original 25\n",
      "length of train  sequence original 33\n",
      "length of train  sequence original 21\n",
      "length of train  sequence original 2\n",
      "length of train sequence padded 32\n",
      "length of train sequence padded 32\n",
      "length of train sequence padded 32\n",
      "length of train sequence padded 32\n",
      "length of train sequence padded 32\n",
      "len of test seq after padding 32\n",
      "len of test seq after padding 32\n",
      "len of test seq after padding 32\n",
      "len of test seq after padding 32\n",
      "len of test seq after padding 32\n",
      "padded_train_test_data done: 394.6439926624298\n",
      "RAM usage padded_train_test_data:  72\n"
     ]
    }
   ],
   "source": [
    "padded_data = hdfslogs.get_padded_train_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "380f378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = padded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1939309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  7, 13,  3, 10,  2, 11,  2,  4, 23,  4,  7, 24, 15, 12,  3, 11,\n",
       "       14,  8, 10, 11, 19,  5,  2,  8,  6, 19,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "901c0dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i n f o r e c e i v i n g b l o c k s r c : d e s t : UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'r o o t r a n d 7 _ t e m p o r a r y _ t a s k _ 2 0 0 8 1 1 1 0 1 0 2 4 _ 0 0 1 4 _ m _ 0 0 0 6 2 3 _ 0 p a r t - 0 0 6 2 3 .', 'i n f o r e c e i v i n g b l o c k s r c : d e s t : UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'b l o c k r e c e i v e d e x c e p t i o n j a v a . i o . i o e x c e p t i o n : c o u l d n o t r e a d f r o m s t r e a m', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "tk = hdfslogs.tk\n",
    "print(tk.sequences_to_texts(x_train[0]))\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f6140b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i n f o r e c e i v i n g b l o c k s r c : d e s t : UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'i n f o r e c e i v i n g b l o c k s r c : d e s t : UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'i n f o r e c e i v i n g b l o c k s r c : d e s t : UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'r r o o t r a n d _ t e m p o r a r y _ t a s k _ 2 0 0 8 1 1 0 9 2 0 3 0 _ 0 0 0 1 _ m _ 0 0 1 2 0 8 _ 0 p a r t - 0 1 2 0 8 .', 'i n f o p a c k e t r e s p o n d e r 0 f o r b l o c k t e r m i n a t i n g UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'i n f o r e c e i v e d b l o c k o f s i z e 6 7 1 0 8 8 6 4 f r o m UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', '* n a m e s y s t e m . a d d s t o r e d b l o c k : b l o c k m a p u p d a t e d : i s a d d e d t o s i z e 6 7 1 0 8 8 6 4', 'i n f o p a c k e t r e s p o n d e r 1 f o r b l o c k t e r m i n a t i n g UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'i n f o r e c e i v e d b l o c k o f s i z e 6 7 1 0 8 8 6 4 f r o m UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'i n f o p a c k e t r e s p o n d e r 2 f o r b l o c k t e r m i n a t i n g UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'i n f o r e c e i v e d b l o c k o f s i z e 6 7 1 0 8 8 6 4 f r o m UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', '* n a m e s y s t e m . a d d s t o r e d b l o c k : b l o c k m a p u p d a t e d : i s a d d e d t o s i z e 6 7 1 0 8 8 6 4', '* n a m e s y s t e m . a d d s t o r e d b l o c k : b l o c k m a p u p d a t e d : i s a d d e d t o s i z e 6 7 1 0 8 8 6 4', 'w a r n : g o t e x c e p t i o n w h i l e s e r v i n g t o UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'i n f o s e r v e d b l o c k t o UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'w a r n : g o t e x c e p t i o n w h i l e s e r v i n g t o UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'i n f o v e r i f i c a t i o n s u c c e e d e d f o r UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'i n f o b l o c k * n a m e s y s t e m . d e l e t e : i s a d d e d t o i n v a l i d s e t o f UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'i n f o b l o c k * n a m e s y s t e m . d e l e t e : i s a d d e d t o i n v a l i d s e t o f UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'i n f o b l o c k * n a m e s y s t e m . d e l e t e : i s a d d e d t o i n v a l i d s e t o f UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'i n f o d e l e t i n g b l o c k f i l e m n t h a d o o p d f s d a t a c u r r e n t s u b d i r 1 2 UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'i n f o d e l e t i n g b l o c k f i l e m n t h a d o o p d f s d a t a c u r r e n t s u b d i r 3 6 UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'l o c k r e q u e s t r e c e i v e d f o r o n s i z e 6 7 1 0 8 8 6 4 b u t i t d o e s n o t b e l o n g t o a n y f i l e .', '* n a m e s y s t e m . a d d s t o r e d b l o c k : b l o c k m a p u p d a t e d : i s a d d e d t o s i z e 6 7 1 0 8 8 6 4', 'i n f o d e l e t i n g b l o c k f i l e m n t h a d o o p d f s d a t a c u r r e n t s u b d i r 6 1 UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK', 'UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(tk.sequences_to_texts(x_train[1]))\n",
    "print(y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "208cb475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of lines in the log file: 11175629\n",
      "RAM usage:  91719104\n",
      "ending logs in memory: 5.3517234325408936\n",
      "loaded cleaned logs with blk_id  in memory: 87.69729948043823\n",
      "RAM usage:  91719104\n",
      "loaded cleaned logs without blkid in memory: 0.448652982711792\n",
      "RAM usage:  91719104\n",
      "starting training the tokenizer:\n",
      "ending tokenizer training: 184.1769437789917\n",
      "RAM usage:  48\n",
      "vocabulary size: 42\n",
      "starting text to number conversion\n",
      "completed:  0\n",
      "time : 0.0\n",
      "completed:  1000000\n",
      "time : 16.56307053565979\n",
      "completed:  2000000\n",
      "time : 31.8032648563385\n",
      "completed:  3000000\n",
      "time : 48.934873819351196\n",
      "completed:  4000000\n",
      "time : 65.34685850143433\n",
      "completed:  5000000\n",
      "time : 81.88243174552917\n",
      "completed:  6000000\n",
      "time : 98.55234456062317\n",
      "completed:  7000000\n",
      "time : 115.54704546928406\n",
      "completed:  8000000\n",
      "time : 132.530690908432\n",
      "completed:  9000000\n",
      "time : 149.71746969223022\n",
      "completed:  10000000\n",
      "time : 166.7537932395935\n",
      "completed:  11000000\n",
      "time : 183.7383234500885\n",
      "ending text to number conversion: 186.80669689178467\n",
      "RAM usage:  91719104\n",
      "ending num_sequence_by_blkid conversion: 385.940420627594\n",
      "RAM usage:  20971608\n",
      "ending labelled_num_seq_df conversion: 388.715425491333\n",
      "RAM usage:  181824789\n",
      "getting ablation data: 1000\n",
      "1000 15838\n",
      "train_test_data done: 388.8734555244446\n",
      "RAM usage train_test_data:  72\n"
     ]
    }
   ],
   "source": [
    "ablation_data = hdfslogs.get_train_test_data(ablation=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39f5dbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train  sequence original 4\n",
      "length of train  sequence original 25\n",
      "length of train  sequence original 33\n",
      "length of train  sequence original 21\n",
      "length of train  sequence original 2\n",
      "length of train sequence padded 32\n",
      "length of train sequence padded 32\n",
      "length of train sequence padded 32\n",
      "length of train sequence padded 32\n",
      "length of train sequence padded 32\n",
      "len of test seq after padding 32\n",
      "len of test seq after padding 32\n",
      "len of test seq after padding 32\n",
      "len of test seq after padding 32\n",
      "len of test seq after padding 32\n",
      "padded_train_test_data done: 6.604649066925049\n",
      "RAM usage padded_train_test_data:  72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 32, 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = hdfslogs.get_padded_train_test_data()\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7831a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hdfs_log_obj.pkl', 'wb') as f:\n",
    "    pickle.dump(hdfslogs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5f0c1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f41b9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((32, 32, 64), (32,)), types: (tf.int32, tf.int64)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "B = 32\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.shuffle(buffer_size=y_train.shape[0]).batch(B, drop_remainder=True)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c15215c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer conv1d_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Model: \"functional_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(32, 32, 64)]            0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (32, 32, 64)              12352     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (32, 2048)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (32, 2048)                4196352   \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (32, 1)                   2049      \n",
      "=================================================================\n",
      "Total params: 4,210,753\n",
      "Trainable params: 4,210,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input(batch_shape=(B, 32, 64), dtype='float64' )\n",
    "x = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='same')(inputs)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(2048)(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "print(model.summary())\n",
    "model.compile(optimizer='adam', \n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eae8ffb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 0.6956 - accuracy: 0.4970\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.6838 - accuracy: 0.5302\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.6834 - accuracy: 0.5333\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.6931 - accuracy: 0.4995\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.6931 - accuracy: 0.4990\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.6931 - accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.6931 - accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.6931 - accuracy: 0.4995\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.6931 - accuracy: 0.5010\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.6931 - accuracy: 0.5015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a96fcfd4f0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ea9b2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 42\n"
     ]
    }
   ],
   "source": [
    "tk = hdfslogs.tk\n",
    "vocab_size = len(tk.word_index)\n",
    "print(f'vocab_size: {vocab_size}')\n",
    "char_onehot = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "934b8b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embedding_weights = []\n",
    "embedding_weights.append(np.zeros(vocab_size))\n",
    "for char, i in tk.word_index.items(): # from 1 to 51\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[i-1] = 1\n",
    "    embedding_weights.append(onehot)\n",
    "embedding_weights = np.array(embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bbbb0e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43, 42)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(embedding_weights.shape) # first row all 0 for PAD and last row for UNK\n",
    "embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6305c21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.embeddings.Embedding at 0x203aaaaa100>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = [32, 64]\n",
    "embedding_size = 42\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(vocab_size+1,\n",
    "                                            embedding_size,\n",
    "                                            input_length=input_size,\n",
    "                                            weights = [embedding_weights])\n",
    "embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e268af7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        [(32, 32, 64)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (32, 32, 64, 42)          1806      \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (32, 32, 64, 64)          8128      \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (32, 131072)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (32, 2048)                268437504 \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (32, 1)                   2049      \n",
      "=================================================================\n",
      "Total params: 268,449,487\n",
      "Trainable params: 268,449,487\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input(batch_shape=(B, 32, 64), dtype='float64' )\n",
    "x = tf.keras.layers.Embedding(input_dim=vocab_size+1,\n",
    "                                output_dim=embedding_size,\n",
    "                                input_length=64,\n",
    "                                weights = [embedding_weights],\n",
    "                                )(inputs)\n",
    "x = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='same')(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(2048)(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "print(model.summary())\n",
    "model.compile(optimizer='adam', \n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c22c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "01e102e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "62/62 [==============================] - 54s 878ms/step - loss: 0.6554 - accuracy: 0.5988\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 55s 886ms/step - loss: 0.6523 - accuracy: 0.6094\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 54s 878ms/step - loss: 0.6522 - accuracy: 0.6074\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 55s 883ms/step - loss: 0.6523 - accuracy: 0.6099\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 55s 881ms/step - loss: 0.6520 - accuracy: 0.6094\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 56s 901ms/step - loss: 0.6525 - accuracy: 0.6069\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 55s 882ms/step - loss: 0.6525 - accuracy: 0.6089\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 55s 886ms/step - loss: 0.6523 - accuracy: 0.6074\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 55s 888ms/step - loss: 0.6522 - accuracy: 0.6099\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 55s 888ms/step - loss: 0.6522 - accuracy: 0.6084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a9700136a0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a0b0eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(32, 32, 64)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (32, 32, 64, 42)          1806      \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (32, 32, 64, 64)          16192     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (32, 131072)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (32, 2048)                268437504 \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (32, 1)                   2049      \n",
      "=================================================================\n",
      "Total params: 268,457,551\n",
      "Trainable params: 268,457,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input(batch_shape=(B, 32, 64), dtype='float64' )\n",
    "x = tf.keras.layers.Embedding(input_dim=vocab_size+1,\n",
    "                                output_dim=embedding_size,\n",
    "                                input_length=64,\n",
    "                                weights = [embedding_weights],\n",
    "                                )(inputs)\n",
    "x = tf.keras.layers.Conv1D(filters=64, kernel_size=6, padding='same')(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(2048)(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "print(model.summary())\n",
    "model.compile(optimizer='adam', \n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03a44c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "62/62 [==============================] - 56s 899ms/step - loss: 0.6590 - accuracy: 0.5938\n",
      "Epoch 2/3\n",
      "62/62 [==============================] - 57s 915ms/step - loss: 0.6570 - accuracy: 0.5948\n",
      "Epoch 3/3\n",
      "62/62 [==============================] - 56s 898ms/step - loss: 0.6562 - accuracy: 0.5973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2039554a850>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce66310",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
