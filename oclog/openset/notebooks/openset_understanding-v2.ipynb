{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b29a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(123)\n",
    "from BGL.bglog import BGLog, get_embedding_layer\n",
    "from pretraining import LogLineEncoder, LogSeqEncoder, LogClassifier\n",
    "from boundary_loss import euclidean_metric, BoundaryLoss\n",
    "from tqdm import trange, tqdm, tnrange\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08af22b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bglog = BGLog(save_padded_num_sequences=False, load_from_pkl=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2490cf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded_num_seq_df loaded from data\\bgl_padded_num_seq_df.pkl\n",
      "trained tokenizer, tk, loaded from data\\bgltk.pkl\n",
      "train_0:, 800\n",
      "test_0:, 200\n",
      "train_1:, 800\n",
      "test_1:, 200\n",
      "train_2:, 800\n",
      "test_2:, 200\n",
      "train_3:, 800\n",
      "test_3:, 102\n",
      "4 class does not have 800 records, it has only 628 records\n",
      "test_4:, 0\n",
      "5 class does not have 800 records, it has only 165 records\n",
      "5 class does not have 200 records, it has only 165 records\n",
      "6 class does not have 800 records, it has only 75 records\n",
      "6 class does not have 200 records, it has only 75 records\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "<BatchDataset shapes: ((32, 32, 64), (32, 4)), types: (tf.int32, tf.float32)>\n",
      "<BatchDataset shapes: ((32, 32, 64), (32, 4)), types: (tf.int32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "train_test = bglog.get_tensor_train_test(ablation=1000)\n",
    "train_data, test_data = train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3eefa208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 50\n",
      "sample_x_train.shape: (32, 32, 64)\n",
      "loglineEmbedding.shape: (32, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "line_encoder =   LogLineEncoder(bglog, chars_in_line=64)\n",
    "# the model doesn't have a state unless it is called at least once\n",
    "# in order to initialize the model we need a sample data \n",
    "sample_train_data = next(iter(train_data))\n",
    "sample_x_train = sample_train_data[0]\n",
    "print('sample_x_train.shape:', sample_x_train.shape)\n",
    "# now we will initialize the model with the sample data\n",
    "loglineEmbedding = line_encoder(sample_x_train)\n",
    "print('loglineEmbedding.shape:', loglineEmbedding.shape)\n",
    "# Now the model have a state and can be inspected        \n",
    "# line_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b7e28de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logSeqEmbedding.shape: (32, 16)\n"
     ]
    }
   ],
   "source": [
    "logSeqencer =   LogSeqEncoder(line_in_seq=32)\n",
    "# the model doesn't have a state unless it is called at least once\n",
    "logSeqEmbedding = logSeqencer(loglineEmbedding)\n",
    "print('logSeqEmbedding.shape:', logSeqEmbedding.shape)\n",
    "# Now the model have a state and can be inspected        \n",
    "# logSeqencer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1d84f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_classifier = LogClassifier(line_encoder=line_encoder, seq_encoder=logSeqencer, num_classes=4)\n",
    "# log_classifier(sample_x_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "330ca911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09302ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method LogLineEncoder.call of <pretraining.LogLineEncoder object at 0x0000023D300CBAC0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method LogLineEncoder.call of <pretraining.LogLineEncoder object at 0x0000023D300CBAC0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method LogSeqEncoder.call of <pretraining.LogSeqEncoder object at 0x0000023D300CBD30>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method LogSeqEncoder.call of <pretraining.LogSeqEncoder object at 0x0000023D300CBD30>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['log_seq_encoder/conv1d_3/kernel:0', 'log_seq_encoder/conv1d_3/bias:0', 'log_seq_encoder/conv1d_4/kernel:0', 'log_seq_encoder/conv1d_4/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['log_seq_encoder/conv1d_3/kernel:0', 'log_seq_encoder/conv1d_3/bias:0', 'log_seq_encoder/conv1d_4/kernel:0', 'log_seq_encoder/conv1d_4/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['log_seq_encoder/conv1d_3/kernel:0', 'log_seq_encoder/conv1d_3/bias:0', 'log_seq_encoder/conv1d_4/kernel:0', 'log_seq_encoder/conv1d_4/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['log_seq_encoder/conv1d_3/kernel:0', 'log_seq_encoder/conv1d_3/bias:0', 'log_seq_encoder/conv1d_4/kernel:0', 'log_seq_encoder/conv1d_4/bias:0'] when minimizing the loss.\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 0.3961 - accuracy: 0.8656 - precision: 0.9614 - recall: 0.7619 - val_loss: 0.0231 - val_accuracy: 0.9985 - val_precision: 0.9985 - val_recall: 0.9940\n"
     ]
    }
   ],
   "source": [
    "log_classifier.compile(optimizer='adam', \n",
    "                  loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "hist = log_classifier.fit(train_data, validation_data=test_data, epochs=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56f46b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_classifier(sample_x_train)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dae2a584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenSet:\n",
    "    ''' \n",
    "    self.num_labels = number of classes\n",
    "    self.embedding_size = number of neurons in the logits layers of the pretrained model'''\n",
    "    def __init__(self, num_labels, pretrained_model, embedding_size,\n",
    "                lr_boundary):\n",
    "#         super().__init__():\n",
    "        self.model = pretrained_model        \n",
    "        self.centroids = None\n",
    "        self.num_labels = num_labels\n",
    "        self.embedding_size = embedding_size\n",
    "        self.delta = None\n",
    "        self.lr_boundary = lr_boundary\n",
    "        self.delta_points = []\n",
    "        \n",
    "    \n",
    "    def train(self, data_train, epochs=1):\n",
    "        criterion_boundary = BoundaryLoss(num_labels=self.num_labels)\n",
    "        # delta is getting calculated inside the  BoundaryLoss class as well\n",
    "        # however that calculated delta is used for calculating the loss \n",
    "        # that delta is not updating the criterion_boundary.delta which is \n",
    "        # a randomly initialized parameter. \n",
    "        # Hence the following softplus is on randomly initialized trainable parameters\n",
    "        # and not softplus on softplus\n",
    "        self.delta = tf.nn.softplus(criterion_boundary.delta)\n",
    "        self.centroids = self.centroids_cal(data_train)        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.lr_boundary) # does it take criterion_boundary.parameters() ??\n",
    "        wait = 0\n",
    "        best_delta, best_centroids = None, None\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            tr_loss = 0\n",
    "            nb_tr_examples, nb_tr_steps = 0, 0\n",
    "            for batch in data_train:\n",
    "                logseq_batch, label_batch = batch\n",
    "                ## (32, 32, 64), (32, 4)\n",
    "                batch_loss, t_loss = self.train_step(criterion_boundary, \n",
    "                                                     logseq_batch, label_batch, optimizer)\n",
    "                tr_loss += t_loss\n",
    "                nb_tr_steps += 1\n",
    "                \n",
    "            self.delta_points.append(self.delta)\n",
    "            loss = tr_loss / nb_tr_steps\n",
    "            print('train_loss:', loss)\n",
    "            print(self.delta_points)\n",
    "                    \n",
    "#     @tf.function                \n",
    "    def train_step(self, criterion_boundary, logseq_batch, label_batch, optimizer):\n",
    "#         print('within train_step')\n",
    "        tr_loss = 0\n",
    "        with tf.GradientTape() as tape:                \n",
    "            features_batch = self.model(logseq_batch, extract_feature=True)\n",
    "            loss, self.delta = criterion_boundary(features_batch, \n",
    "                                                  self.centroids, \n",
    "                                                  label_batch)\n",
    "        tr_loss += loss\n",
    "        gradients = tape.gradient(loss, [self.delta])\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, [self.delta]))\n",
    "        return loss, tr_loss                    \n",
    "        \n",
    "        \n",
    "        \n",
    "    def centroids_cal(self, data):\n",
    "        centroids = tf.zeros((self.num_labels, self.embedding_size))\n",
    "        total_labels = tf.zeros(self.num_labels)\n",
    "        for batch in data:\n",
    "            logseq_batch, label_batch = batch\n",
    "            ## (32, 32, 64), (32, 4)\n",
    "            features = self.model(logseq_batch, extract_feature=True)\n",
    "            ## (32, 16) features - 32 sequence of line each haaving 64 characrers\n",
    "            ## produces a feaure vector of dimension 16. \n",
    "            for i in range(len(label_batch)): # (32, 4) --> here length is 32\n",
    "                label = label_batch[i] # label looks like [0 0 0 1]\n",
    "                numeric_label = np.argmax(label) # index position of the label = 3 , so it is actually class =3\n",
    "                ##total_labels = [0 0 0 0] each col representing a class \n",
    "                ## count the number for each class\n",
    "                total_labels_lst = tf.unstack(total_labels)\n",
    "                total_labels_lst[numeric_label] += 1 \n",
    "                total_labels = tf.stack(total_labels_lst)\n",
    "                centroids_lst = tf.unstack(centroids)\n",
    "                centroids_lst[numeric_label] += features[i]\n",
    "                centroids = tf.stack(centroids_lst)\n",
    "                # each row index in the centroid array is a class\n",
    "                # we add first identify the feature belonging to which class by the numeric_label\n",
    "                # Then add all the features belonging to the class in the corresponding row of the centroid arr\n",
    "        ### shape of centroids is (4, 16) whereas shape of total_labels is (1, 4)\n",
    "        ### reshape the total_labels as 4,1 ==> [[0], [0], [0], [0]]==> 4 rows \n",
    "        ## so that we can divide the centroids array by the total_labels\n",
    "        total_label_reshaped = tf.reshape(total_labels, (self.num_labels, 1))\n",
    "        centroids /= total_label_reshaped\n",
    "        return centroids  \n",
    "        \n",
    "        def openpredict(self, features):\n",
    "            logits = euclidean_metric(features, self.centroids)\n",
    "            ####original line in pytorch ###probs, preds = F.softmax(logits.detach(), dim = 1).max(dim = 1)\n",
    "            smax = tf.nn.softmax(logits, )\n",
    "            preds = tf.math.argmax(smax, axis=1)\n",
    "            probs = tf.reduce_max(smax, 1)            \n",
    "            #######euc_dis = torch.norm(features - self.centroids[preds], 2, 1).view(-1)\n",
    "            euc_dis = tf.norm(features - self.centroids[preds], ord='euclidean', axis=1) \n",
    "            \n",
    "            \n",
    "            #preds[euc_dis >= self.delta[preds]] = data.unseen_token_id\n",
    "\n",
    "            return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb239f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#while developing openpredict method we need to understand the data shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42a7b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, self.delta = criterion_boundary(features_batch,\n",
    "# logits =  euclidean_metric(features, centroids)\n",
    "# NotImplementedError: Cannot convert a symbolic Tensor (log_classifier/log_seq_encoder/dense/Relu:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported \n",
    "# it looks like the numpy arrays to be converted to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17df908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "oset = OpenSet(4, log_classifier, 16, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12344699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f717d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.apply_gradients(zip(gradients, self.delta))\n",
    "# TypeError: 'IndexedSlices' object is not iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff210672",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_batch = next(iter(train_data))\n",
    "t_batch_x, t_batch_y = t_batch\n",
    "t_batch_x.shape\n",
    "centroids = oset.centroids_cal(train_data)\n",
    "features_batch = log_classifier(t_batch_x, extract_feature=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "868b25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_boundary = BoundaryLoss(num_labels=4)\n",
    "loss, delta = criterion_boundary(features_batch, centroids, t_batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "009d7413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       "array([1.6548097, 1.6548097, 1.6548097, 1.6548097, 1.6931441, 1.6931441,\n",
       "       1.6548097, 1.6362237, 1.6362237, 1.6548097, 1.6548097, 1.6931441,\n",
       "       1.6754997, 1.6754997, 1.6362237, 1.6931441, 1.6362237, 1.6362237,\n",
       "       1.6931441, 1.6548097, 1.6754997, 1.6754997, 1.6931441, 1.6754997,\n",
       "       1.6548097, 1.6548097, 1.6548097, 1.6548097, 1.6754997, 1.6548097,\n",
       "       1.6754997, 1.6362237], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "744de728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'boundary_loss/Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
       "array([[0.6644433],\n",
       "       [0.6851332],\n",
       "       [0.7037192],\n",
       "       [0.6467987]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c462747",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_loss=0\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "with tf.GradientTape() as tape:                \n",
    "    features_batch = log_classifier(t_batch_x, extract_feature=True)\n",
    "    loss, delta = criterion_boundary(features_batch, centroids, t_batch_y)\n",
    "tr_loss += loss\n",
    "gradients = tape.gradient(loss, [delta])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4313c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
       "array([[0.6644433],\n",
       "       [0.6851332],\n",
       "       [0.7037192],\n",
       "       [0.6467987]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = tf.Variable(delta)\n",
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18c34f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.framework.indexed_slices.IndexedSlices at 0x22af9beb970>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "485580ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.minimize(loss, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7105859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.apply_gradients(zip(gradients, delta))\n",
    "#### AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_in_graph_mode'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a49e7bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def check_gradient(t_batch_x, t_batch_y, centroids):\n",
    "    tr_loss=0\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "    with tf.GradientTape() as tape:                \n",
    "        features_batch = log_classifier(t_batch_x, extract_feature=True)\n",
    "        loss, delta = criterion_boundary(features_batch, centroids, t_batch_y)\n",
    "    tr_loss += loss\n",
    "    gradients = tape.gradient(loss, [delta])    \n",
    "    optimizer.apply_gradients(zip(gradients, [delta]))\n",
    "#     optimizer.minimize(gradients, var_list=[delta])\n",
    "    return loss, tr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "867b2048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       " array([1.6548097, 1.6548097, 1.6548097, 1.6548097, 1.6931441, 1.6931441,\n",
       "        1.6548097, 1.6362237, 1.6362237, 1.6548097, 1.6548097, 1.6931441,\n",
       "        1.6754997, 1.6754997, 1.6362237, 1.6931441, 1.6362237, 1.6362237,\n",
       "        1.6931441, 1.6548097, 1.6754997, 1.6754997, 1.6931441, 1.6754997,\n",
       "        1.6548097, 1.6548097, 1.6548097, 1.6548097, 1.6754997, 1.6548097,\n",
       "        1.6754997, 1.6362237], dtype=float32)>,\n",
       " <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       " array([1.6548097, 1.6548097, 1.6548097, 1.6548097, 1.6931441, 1.6931441,\n",
       "        1.6548097, 1.6362237, 1.6362237, 1.6548097, 1.6548097, 1.6931441,\n",
       "        1.6754997, 1.6754997, 1.6362237, 1.6931441, 1.6362237, 1.6362237,\n",
       "        1.6931441, 1.6548097, 1.6754997, 1.6754997, 1.6931441, 1.6754997,\n",
       "        1.6548097, 1.6548097, 1.6548097, 1.6548097, 1.6754997, 1.6548097,\n",
       "        1.6754997, 1.6362237], dtype=float32)>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_gradient(t_batch_x, t_batch_y, centroids)\n",
    "###AttributeError: 'Tensor' object has no attribute '_in_graph_mode'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8af390c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: tf.Tensor(\n",
      "[1.7890495 1.7961293 1.7964641 1.8004185 1.7929924 1.7974381 1.795191\n",
      " 1.7931007 1.7890009 1.7863306 1.7911556 1.7913154 1.7929295 1.7916319\n",
      " 1.793261  1.7944844 1.7919563 1.7936623 1.7925035 1.79865   1.7968493\n",
      " 1.8006352 1.7996067 1.7871317 1.7890861 1.7963542 1.7880127 1.7968636\n",
      " 1.7952195 1.7908095 1.7912313 1.7910193], shape=(32,), dtype=float32)\n",
      "[<tf.Variable 'boundary_loss_1/Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
      "array([[0.71801656],\n",
      "       [0.7913795 ],\n",
      "       [0.697228  ],\n",
      "       [0.76913875]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "oset.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "676c0abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'boundary_loss_1/Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
       " array([[0.71801656],\n",
       "        [0.7913795 ],\n",
       "        [0.697228  ],\n",
       "        [0.76913875]], dtype=float32)>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oset.delta_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f91b0db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ -67.01952     -3.525845  -109.583534  -160.43402  ]\n",
      " [ -79.82242     -2.1222196  -93.719376  -131.56161  ]\n",
      " [-184.88982    -24.641348  -132.53224   -195.33246  ]\n",
      " [ -76.3061      -2.9903688 -107.254425  -152.44286  ]\n",
      " [-119.22389   -140.64474    -50.575027   -47.487274 ]\n",
      " [-222.22762   -185.70552   -101.49874     -6.754195 ]\n",
      " [ -70.32373     -1.5167881  -95.82221   -140.16188  ]\n",
      " [-140.18436    -95.62632     -3.5865545  -59.446598 ]\n",
      " [-119.264595   -92.42897     -4.27982    -86.548355 ]\n",
      " [ -65.43387     -3.5297403 -108.33574   -157.64508  ]\n",
      " [ -65.415955    -3.8230033 -111.58875   -161.621    ]\n",
      " [-180.8993    -161.67134    -77.23272     -0.7322664]\n",
      " [  -2.8020918  -88.394     -161.12628   -210.3276   ]\n",
      " [  -4.171529   -83.65649    -98.34264   -135.52994  ]\n",
      " [-154.31055   -103.04908     -7.6279254  -41.967087 ]\n",
      " [-209.51389   -179.3525     -93.75351     -4.171521 ]\n",
      " [-135.17406    -90.617714    -0.8169305  -77.32392  ]\n",
      " [-141.30368   -106.51258    -15.940049   -23.444386 ]\n",
      " [-202.55185   -177.32387    -95.83757     -2.9559226]\n",
      " [ -65.92119     -3.5019805 -109.425064  -159.5612   ]\n",
      " [  -2.9463463  -88.81685   -162.48105   -211.64554  ]\n",
      " [  -4.171529   -83.65649    -98.34264   -135.52994  ]\n",
      " [-208.68044   -177.49672    -94.12228     -3.7914007]\n",
      " [  -2.938864   -89.27342   -162.53915   -210.3732   ]\n",
      " [-184.88982    -24.641348  -132.53224   -195.33246  ]\n",
      " [ -76.804245    -2.3747244 -112.30319   -162.9024   ]\n",
      " [ -67.77647     -1.7361434  -93.95012   -137.69157  ]\n",
      " [ -64.39933     -3.5298269 -108.14549   -158.5847   ]\n",
      " [  -4.171529   -83.65649    -98.34264   -135.52994  ]\n",
      " [-184.88982    -24.641348  -132.53224   -195.33246  ]\n",
      " [  -4.2661357  -86.24663   -107.086655  -133.88907  ]\n",
      " [-146.59216    -97.25247     -5.1762     -77.54037  ]], shape=(32, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "logits = euclidean_metric(features_batch, oset.centroids)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad8bf596",
   "metadata": {},
   "outputs": [],
   "source": [
    "smax = tf.nn.softmax(logits, )\n",
    "preds = tf.math.argmax(smax, axis=1)\n",
    "probs = tf.reduce_max(smax, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "539ab8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: tf.Tensor([1 1 1 1 3 3 1 2 2 1 1 3 0 0 2 3 2 2 3 1 0 0 3 0 1 1 1 1 0 1 0 2], shape=(32,), dtype=int64)\n",
      "probs: tf.Tensor(\n",
      "[1.         1.         1.         1.         0.9563847  1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99944955\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.        ], shape=(32,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('preds:', preds)\n",
    "print('probs:', probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35566e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oset.centroids: tf.Tensor(\n",
      "[[0.0000000e+00 4.2018835e-02 0.0000000e+00 7.5073738e+00 2.9848788e+00\n",
      "  0.0000000e+00 5.9337478e+00 3.0970883e+00 4.7078938e+00 5.1352601e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.4536023e+01 3.4672394e+00\n",
      "  6.4453125e+00]\n",
      " [0.0000000e+00 6.2599664e+00 0.0000000e+00 1.1224109e+01 2.5894647e+00\n",
      "  0.0000000e+00 6.1635613e+00 2.2290489e-01 5.6645598e+00 3.8178229e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.5476877e+01 6.0169353e+00\n",
      "  3.0685852e+00]\n",
      " [0.0000000e+00 4.1064219e+00 0.0000000e+00 1.0502010e+01 2.1169436e+00\n",
      "  0.0000000e+00 6.6549835e+00 1.9764199e-03 3.4076446e-01 6.0818281e-02\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.7405222e+01 8.4182948e-02\n",
      "  9.9504930e-01]\n",
      " [0.0000000e+00 1.7120099e+00 0.0000000e+00 1.5485305e+01 6.2190514e-02\n",
      "  0.0000000e+00 2.0054388e+00 3.6714032e-02 1.1385172e-01 1.2439347e-01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.7535822e+01 1.5839231e-01\n",
      "  4.3307877e+00]], shape=(4, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('oset.centroids:',oset.centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfc89801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroids are having only 4 rows , whereas labels are rows equivallent to batch\n",
    "        # pick-up the centroid for each class \n",
    "        # label_index from the data set will have all the classes, 32 for a batch\n",
    "        # for each class cetroid[class_index] will give the centroid of the calss\n",
    "        # it is basically : [centroids[class_idx] for class_idx in label_indexes]\n",
    "#         c = centroids[label_indexs]\n",
    "#         c = tf.gather(centroids, indices=label_indexs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff21c9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 0.          6.2599664   0.         11.224109    2.5894647   0.\n",
      "  6.1635613   0.22290489  5.66456     3.817823    0.          0.\n",
      "  0.         15.476877    6.0169353   3.0685852 ], shape=(16,), dtype=float32)\n",
      "(32, 16)\n"
     ]
    }
   ],
   "source": [
    "c = tf.gather(oset.centroids, indices=preds)\n",
    "print(c[0])\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1afd824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[1.8777233 1.456784  4.964005  1.7292683 6.891102  2.5988834 1.2315795\n",
      " 1.89382   2.0687726 1.8787605 1.9552501 0.8557257 1.673945  2.042432\n",
      " 2.76187   2.0424302 0.903842  3.992499  1.7192796 1.8713579 1.7164923\n",
      " 2.042432  1.9471519 1.7143115 4.964005  1.5410141 1.3176279 1.8787833\n",
      " 2.042432  4.964005  2.0654626 2.2751262], shape=(32,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "euc_dis = tf.norm(features_batch - c, ord='euclidean', axis=1) \n",
    "print(euc_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8402e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = tf.gather(oset.delta_points, indices=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f2e35a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 32), dtype=bool, numpy=\n",
       "array([[[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True]]])>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euc_dis >=oset.delta_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1530197e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'boundary_loss_1/Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
       "array([[0.71801656],\n",
       "       [0.7913795 ],\n",
       "       [0.697228  ],\n",
       "       [0.76913875]], dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oset.delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1ed92fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
       "array([1, 1, 1, 1, 3, 3, 1, 2, 2, 1, 1, 3, 0, 0, 2, 3, 2, 2, 3, 1, 0, 0,\n",
       "       3, 0, 1, 1, 1, 1, 0, 1, 0, 2], dtype=int64)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "796e0cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'boundary_loss_1/Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
       "array([[0.71801656],\n",
       "       [0.7913795 ],\n",
       "       [0.697228  ],\n",
       "       [0.76913875]], dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oset.delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f16fe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.7913795 ]\n",
      " [0.7913795 ]\n",
      " [0.7913795 ]\n",
      " [0.7913795 ]\n",
      " [0.76913875]\n",
      " [0.76913875]\n",
      " [0.7913795 ]\n",
      " [0.697228  ]\n",
      " [0.697228  ]\n",
      " [0.7913795 ]\n",
      " [0.7913795 ]\n",
      " [0.76913875]\n",
      " [0.71801656]\n",
      " [0.71801656]\n",
      " [0.697228  ]\n",
      " [0.76913875]\n",
      " [0.697228  ]\n",
      " [0.697228  ]\n",
      " [0.76913875]\n",
      " [0.7913795 ]\n",
      " [0.71801656]\n",
      " [0.71801656]\n",
      " [0.76913875]\n",
      " [0.71801656]\n",
      " [0.7913795 ]\n",
      " [0.7913795 ]\n",
      " [0.7913795 ]\n",
      " [0.7913795 ]\n",
      " [0.71801656]\n",
      " [0.7913795 ]\n",
      " [0.71801656]\n",
      " [0.697228  ]], shape=(32, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "d = tf.gather(oset.delta, indices=preds)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57dccc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = tf.reshape(d, d.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd2f4794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[1.8777233 1.456784  4.964005  1.7292683 6.891102  2.5988834 1.2315795\n",
      " 1.89382   2.0687726 1.8787605 1.9552501 0.8557257 1.673945  2.042432\n",
      " 2.76187   2.0424302 0.903842  3.992499  1.7192796 1.8713579 1.7164923\n",
      " 2.042432  1.9471519 1.7143115 4.964005  1.5410141 1.3176279 1.8787833\n",
      " 2.042432  4.964005  2.0654626 2.2751262], shape=(32,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(euc_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb4629b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=bool, numpy=\n",
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True])>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euc_dis >=d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f31aec42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
       "array([1, 1, 1, 1, 3, 3, 1, 2, 2, 1, 1, 3, 0, 0, 2, 3, 2, 2, 3, 1, 0, 0,\n",
       "       3, 0, 1, 1, 1, 1, 0, 1, 0, 2], dtype=int64)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[euc_dis >=d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b709992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all the data points, the distance is more than the bouundary ???\n",
    "# So either the distance or the radius delta is not right \n",
    "# IS it due to inadequaate training ??? we trained with only one batch and one epochs\n",
    "# Let us increase the batch and epochs one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f8057f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: tf.Tensor(\n",
      "[1.7928886 1.7947131 1.7920505 1.7942716 1.7917563 1.7941245 1.7958467\n",
      " 1.7921882 1.7936943 1.7922795 1.793405  1.7919062 1.792107  1.7950283\n",
      " 1.7954435 1.7912172 1.7934419 1.7937831 1.7903285 1.7914207 1.7894336\n",
      " 1.7927735 1.7915726 1.790076  1.7917013 1.7907062 1.7943268 1.7915584\n",
      " 1.7926401 1.791234  1.7943248 1.7923831], shape=(32,), dtype=float32)\n",
      "[<tf.Variable 'boundary_loss_1/Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
      "array([[0.71801656],\n",
      "       [0.7913795 ],\n",
      "       [0.697228  ],\n",
      "       [0.76913875]], dtype=float32)>, <tf.Variable 'boundary_loss_2/Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
      "array([[0.7494542],\n",
      "       [0.7539585],\n",
      "       [0.7190195],\n",
      "       [0.7566024]], dtype=float32)>]\n",
      "train_loss: tf.Tensor(\n",
      "[1.7907228 1.7922087 1.7961165 1.7906119 1.7898664 1.7928864 1.7935165\n",
      " 1.7935889 1.7927704 1.7943834 1.7917529 1.7919803 1.7951568 1.7928505\n",
      " 1.7938629 1.792004  1.7942433 1.791109  1.793346  1.793889  1.7931705\n",
      " 1.7936989 1.7945917 1.7899562 1.7915053 1.7929764 1.7933426 1.7903692\n",
      " 1.7913687 1.793761  1.7921993 1.7909834], shape=(32,), dtype=float32)\n",
      "[<tf.Variable 'boundary_loss_1/Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
      "array([[0.71801656],\n",
      "       [0.7913795 ],\n",
      "       [0.697228  ],\n",
      "       [0.76913875]], dtype=float32)>, <tf.Variable 'boundary_loss_2/Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
      "array([[0.7494542],\n",
      "       [0.7539585],\n",
      "       [0.7190195],\n",
      "       [0.7566024]], dtype=float32)>, <tf.Variable 'boundary_loss_2/Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
      "array([[0.76799905],\n",
      "       [0.7725034 ],\n",
      "       [0.73756427],\n",
      "       [0.77514726]], dtype=float32)>]\n",
      "train_loss: tf.Tensor(\n",
      "[1.7913891 1.7922337 1.7924324 1.7927116 1.7919748 1.792237  1.7923102\n",
      " 1.7939208 1.7902708 1.7929988 1.7925605 1.793095  1.7913728 1.7934192\n",
      " 1.7932637 1.7929307 1.7911493 1.7935126 1.7936559 1.7939086 1.7921894\n",
      " 1.7933526 1.793665  1.7962531 1.793758  1.7945442 1.7911321 1.7890118\n",
      " 1.7942421 1.7914765 1.7926906 1.7911518], shape=(32,), dtype=float32)\n",
      "[<tf.Variable 'boundary_loss_1/Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
      "array([[0.71801656],\n",
      "       [0.7913795 ],\n",
      "       [0.697228  ],\n",
      "       [0.76913875]], dtype=float32)>, <tf.Variable 'boundary_loss_2/Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
      "array([[0.7494542],\n",
      "       [0.7539585],\n",
      "       [0.7190195],\n",
      "       [0.7566024]], dtype=float32)>, <tf.Variable 'boundary_loss_2/Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
      "array([[0.76799905],\n",
      "       [0.7725034 ],\n",
      "       [0.73756427],\n",
      "       [0.77514726]], dtype=float32)>, <tf.Variable 'boundary_loss_2/Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
      "array([[0.7811786 ],\n",
      "       [0.7856829 ],\n",
      "       [0.75074387],\n",
      "       [0.7883268 ]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "oset.train(train_data, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "596122c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loss is decreaing, however, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e601dfaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c426b3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once the shape of the data has been understood and the distance comparison logic is working\n",
    "# rebuild the  openpredict method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5987b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenSetv1:\n",
    "    ''' \n",
    "    self.num_labels = number of classes\n",
    "    self.embedding_size = number of neurons in the logits layers of the pretrained model'''\n",
    "    def __init__(self, num_labels, pretrained_model, embedding_size,\n",
    "                lr_boundary):\n",
    "#         super().__init__():\n",
    "        self.model = pretrained_model        \n",
    "        self.centroids = None\n",
    "        self.num_labels = num_labels\n",
    "        self.embedding_size = embedding_size\n",
    "        self.delta = None\n",
    "        self.lr_boundary = lr_boundary\n",
    "        self.delta_points = []\n",
    "        \n",
    "    \n",
    "    def train(self, data_train, epochs=1):\n",
    "        criterion_boundary = BoundaryLoss(num_labels=self.num_labels)\n",
    "        # delta is getting calculated inside the  BoundaryLoss class as well\n",
    "        # however that calculated delta is used for calculating the loss \n",
    "        # that delta is not updating the criterion_boundary.delta which is \n",
    "        # a randomly initialized parameter. \n",
    "        # Hence the following softplus is on randomly initialized trainable parameters\n",
    "        # and not softplus on softplus\n",
    "        self.delta = tf.nn.softplus(criterion_boundary.delta)\n",
    "        self.centroids = self.centroids_cal(data_train)        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.lr_boundary) # does it take criterion_boundary.parameters() ??\n",
    "        wait = 0\n",
    "        best_delta, best_centroids = None, None        \n",
    "#         for epoch in tnrange(epochs, desc='Epoch'):\n",
    "        for epoch in range(epochs):\n",
    "            self.model.fit(train_data) \n",
    "            tr_loss = 0\n",
    "            nb_tr_examples, nb_tr_steps = 0, 0\n",
    "#             for batch in tqdm(data_train, desc='Iteration'):   \n",
    "            for batch in data_train:\n",
    "                logseq_batch, label_batch = batch\n",
    "                ## (32, 32, 64), (32, 4)\n",
    "                batch_loss = self.train_step(criterion_boundary, \n",
    "                                                     logseq_batch, label_batch, optimizer)\n",
    "                tr_loss += batch_loss\n",
    "                nb_tr_steps += 1                \n",
    "            self.delta_points.append(self.delta)\n",
    "            loss = tr_loss / nb_tr_steps\n",
    "            print('train_loss:', loss)  \n",
    "                    \n",
    "#     @tf.function                \n",
    "    def train_step(self, criterion_boundary, logseq_batch, label_batch, optimizer):        \n",
    "        with tf.GradientTape() as tape:                \n",
    "            features_batch = self.model(logseq_batch, extract_feature=True)\n",
    "            b_loss, self.delta = criterion_boundary(features_batch, \n",
    "                                                  self.centroids, \n",
    "                                                  label_batch)           \n",
    "            gradients = tape.gradient(b_loss, [self.delta])\n",
    "            optimizer.apply_gradients(zip(gradients, [self.delta]))\n",
    "        return b_loss\n",
    "        \n",
    "    def centroids_cal(self, data):\n",
    "        centroids = tf.zeros((self.num_labels, self.embedding_size))\n",
    "        total_labels = tf.zeros(self.num_labels)\n",
    "        for batch in data:\n",
    "            logseq_batch, label_batch = batch\n",
    "            ## (32, 32, 64), (32, 4)\n",
    "            features = self.model(logseq_batch, extract_feature=True)\n",
    "            ## (32, 16) features - 32 sequence of line each haaving 64 characrers\n",
    "            ## produces a feaure vector of dimension 16. \n",
    "            for i in range(len(label_batch)): # (32, 4) --> here length is 32\n",
    "                label = label_batch[i] # label looks like [0 0 0 1]\n",
    "                numeric_label = np.argmax(label) # index position of the label = 3 , so it is actually class =3\n",
    "                ##total_labels = [0 0 0 0] each col representing a class \n",
    "                ## count the number for each class\n",
    "                total_labels_lst = tf.unstack(total_labels)\n",
    "                total_labels_lst[numeric_label] += 1 \n",
    "                total_labels = tf.stack(total_labels_lst)\n",
    "                centroids_lst = tf.unstack(centroids)\n",
    "                centroids_lst[numeric_label] += features[i]\n",
    "                centroids = tf.stack(centroids_lst)\n",
    "                # each row index in the centroid array is a class\n",
    "                # we add first identify the feature belonging to which class by the numeric_label\n",
    "                # Then add all the features belonging to the class in the corresponding row of the centroid arr\n",
    "        ### shape of centroids is (4, 16) whereas shape of total_labels is (1, 4)\n",
    "        ### reshape the total_labels as 4,1 ==> [[0], [0], [0], [0]]==> 4 rows \n",
    "        ## so that we can divide the centroids array by the total_labels\n",
    "        total_label_reshaped = tf.reshape(total_labels, (self.num_labels, 1))\n",
    "        centroids /= total_label_reshaped\n",
    "        return centroids  \n",
    "        \n",
    "        def openpredict(self, features):\n",
    "            logits = euclidean_metric(features, self.centroids)\n",
    "            ####original line in pytorch ###probs, preds = F.softmax(logits.detach(), dim = 1).max(dim = 1)\n",
    "            smax = tf.nn.softmax(logits, )\n",
    "            preds = tf.math.argmax(smax, axis=1)\n",
    "            probs = tf.reduce_max(smax, 1)            \n",
    "            #######euc_dis = torch.norm(features - self.centroids[preds], 2, 1).view(-1)\n",
    "            pred_centroids = tf.gather(self.centroids, indices=preds)\n",
    "            euc_dis = tf.norm(features - pred_centroids, ord='euclidean', axis=1) \n",
    "            pred_deltas = tf.gather(self.delta, indices=preds)\n",
    "            pred_deltas = tf.reshape(pred_deltas, pred_deltas.shape[0], )\n",
    "            #####preds[euc_dis >= self.delta[preds]] = data.unseen_token_id\n",
    "            euc_dis >= pred_deltas\n",
    "            return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "94baed0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 9s 94ms/step - loss: 3.5516e-05 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[4.666806  4.6673903 4.666464  4.6670804 4.6677337 4.6661906 4.6671405\n",
      " 4.668411  4.667627  4.666809  4.6675744 4.6671104 4.6677637 4.667657\n",
      " 4.6664677 4.6673975 4.667324  4.6663694 4.6671214 4.666889  4.667508\n",
      " 4.6675115 4.6676784 4.666422  4.666159  4.66697   4.6676335 4.666282\n",
      " 4.6664557 4.6671786 4.667373  4.667103 ], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 2.9461e-05 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[4.731694  4.7299166 4.7303667 4.7308073 4.7307396 4.7317424 4.7313333\n",
      " 4.7306833 4.7320766 4.730001  4.731311  4.730979  4.730976  4.7304854\n",
      " 4.730744  4.730394  4.731369  4.7301755 4.730094  4.731062  4.730792\n",
      " 4.730747  4.7305217 4.731491  4.730342  4.730673  4.7303505 4.7311673\n",
      " 4.730837  4.731039  4.7308946 4.7311435], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 2.8375e-05 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[4.8154216 4.815752  4.815854  4.8162746 4.815593  4.815938  4.816299\n",
      " 4.816042  4.8160667 4.8163233 4.8157277 4.8153377 4.8157973 4.8158555\n",
      " 4.815527  4.815689  4.8163643 4.8165836 4.8152494 4.815604  4.81579\n",
      " 4.8154526 4.816154  4.8150725 4.8160496 4.815886  4.815243  4.8152575\n",
      " 4.816055  4.8161297 4.8166833 4.815233 ], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 9s 88ms/step - loss: 2.5974e-05 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[4.9198866 4.9197435 4.9207087 4.9195533 4.920889  4.9201837 4.919562\n",
      " 4.9196978 4.9203987 4.920527  4.92026   4.9202223 4.9190035 4.9207935\n",
      " 4.920592  4.920578  4.9193687 4.919711  4.91981   4.919804  4.920835\n",
      " 4.919597  4.920133  4.920484  4.9195867 4.9201174 4.9201035 4.9208226\n",
      " 4.91947   4.9203515 4.919723  4.9202614], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 9s 90ms/step - loss: 2.4605e-05 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[5.072037  5.072889  5.073074  5.072955  5.0722246 5.0725527 5.0722322\n",
      " 5.07255   5.073184  5.0718985 5.0723705 5.071837  5.0722575 5.072345\n",
      " 5.0735826 5.072035  5.072025  5.07231   5.0717106 5.0723987 5.071983\n",
      " 5.072453  5.0722632 5.0724735 5.0722256 5.0728974 5.072691  5.073324\n",
      " 5.072862  5.0726833 5.072243  5.073141 ], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 9s 86ms/step - loss: 2.2553e-05 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[5.148472  5.1491256 5.149063  5.148603  5.1488743 5.148376  5.1485744\n",
      " 5.1486344 5.147943  5.1484838 5.1488233 5.1491814 5.149054  5.1481156\n",
      " 5.1482887 5.1492934 5.147592  5.148143  5.1485534 5.148477  5.1477504\n",
      " 5.1484914 5.1485167 5.1480756 5.1492977 5.1485553 5.148496  5.1474676\n",
      " 5.1481347 5.148849  5.1483397 5.1485415], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 9s 88ms/step - loss: 2.0558e-05 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[5.286709  5.286145  5.286079  5.286424  5.286479  5.2863965 5.28681\n",
      " 5.2862353 5.285926  5.2860894 5.286915  5.2863297 5.286951  5.286032\n",
      " 5.2857585 5.286688  5.2868376 5.285643  5.286452  5.2863383 5.2852383\n",
      " 5.2862744 5.287098  5.285442  5.286671  5.2870936 5.286222  5.2860565\n",
      " 5.286688  5.2863216 5.285452  5.2854214], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 9s 89ms/step - loss: 1.8623e-05 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[5.4355106 5.4354897 5.4344354 5.4343896 5.435127  5.4343305 5.434534\n",
      " 5.435056  5.4357257 5.435544  5.4347944 5.4350004 5.434475  5.4351377\n",
      " 5.435216  5.435358  5.4351873 5.435355  5.435031  5.434372  5.4356422\n",
      " 5.435534  5.434834  5.4347105 5.435592  5.4347687 5.4346557 5.435213\n",
      " 5.435278  5.4358144 5.435541  5.43519  ], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 9s 86ms/step - loss: 1.7044e-05 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[5.5470824 5.547018  5.546757  5.547301  5.546446  5.5474715 5.5469775\n",
      " 5.5462856 5.5469475 5.546311  5.546585  5.546951  5.546397  5.5475554\n",
      " 5.546851  5.547221  5.5474763 5.546194  5.545277  5.5461345 5.5475163\n",
      " 5.5471077 5.5476055 5.546589  5.5463524 5.546147  5.5461826 5.5458503\n",
      " 5.546718  5.5473833 5.547083  5.546378 ], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 8s 84ms/step - loss: 1.6647e-05 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[5.7043495 5.704768  5.7044    5.7053156 5.704231  5.7044697 5.703583\n",
      " 5.705475  5.704236  5.7043715 5.7052445 5.704375  5.704443  5.7053895\n",
      " 5.7046494 5.7037263 5.7047462 5.7049465 5.704586  5.704441  5.7042255\n",
      " 5.705595  5.704709  5.70534   5.704581  5.7043056 5.7050915 5.7042494\n",
      " 5.705086  5.705045  5.704102  5.703945 ], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 8s 84ms/step - loss: 1.5613e-05 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[5.843657  5.844914  5.8454614 5.8448405 5.8451457 5.8442397 5.8445873\n",
      " 5.8450847 5.8445024 5.845733  5.845115  5.8447948 5.844911  5.8449078\n",
      " 5.844609  5.8447323 5.844118  5.8439007 5.843924  5.8442063 5.8446674\n",
      " 5.8442287 5.8450036 5.844058  5.844886  5.844565  5.8446765 5.8444414\n",
      " 5.844234  5.8442287 5.845163  5.8455114], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 9s 87ms/step - loss: 1.4401e-05 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[5.985077  5.9840865 5.984055  5.9840336 5.9840655 5.983529  5.9828467\n",
      " 5.9843683 5.984888  5.9835377 5.983889  5.9844074 5.9843836 5.9834356\n",
      " 5.983878  5.983691  5.9846873 5.984024  5.983976  5.9844456 5.9840856\n",
      " 5.984615  5.9833894 5.984966  5.9849772 5.984739  5.985038  5.984878\n",
      " 5.9838696 5.98361   5.983981  5.98453  ], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 8s 84ms/step - loss: 1.3682e-05 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[6.1294184 6.1290326 6.1291213 6.1295457 6.130156  6.1294775 6.129606\n",
      " 6.1299815 6.1295996 6.1292486 6.1294146 6.130401  6.130284  6.129404\n",
      " 6.129096  6.129221  6.1295643 6.1293488 6.1301074 6.129833  6.129027\n",
      " 6.1291423 6.1291823 6.129411  6.129081  6.1301613 6.1291447 6.130095\n",
      " 6.129757  6.128727  6.1301517 6.1294665], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 9s 87ms/step - loss: 1.2088e-05 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[6.2502217 6.2512016 6.2515087 6.2506123 6.2509813 6.2511225 6.2503705\n",
      " 6.2507844 6.2505946 6.251244  6.2500196 6.2500453 6.2509904 6.2506166\n",
      " 6.251366  6.250717  6.2506194 6.25125   6.250476  6.2504287 6.2506604\n",
      " 6.251138  6.2506275 6.251922  6.250553  6.249715  6.2512236 6.2506924\n",
      " 6.2504854 6.2493267 6.250922  6.2496734], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 9s 87ms/step - loss: 1.1243e-05 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[6.404096  6.4036117 6.4042964 6.4033155 6.4042444 6.4039693 6.4034495\n",
      " 6.403848  6.4034996 6.403255  6.4034004 6.403846  6.4037957 6.403826\n",
      " 6.4036007 6.40394   6.4041286 6.4039345 6.4036098 6.403893  6.4047112\n",
      " 6.402902  6.404247  6.403982  6.403283  6.403853  6.404232  6.404105\n",
      " 6.404505  6.403924  6.404036  6.404318 ], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 9s 87ms/step - loss: 1.1144e-05 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[6.5898075 6.589054  6.590184  6.589076  6.5896606 6.5898604 6.589389\n",
      " 6.59021   6.589144  6.590099  6.5895405 6.588217  6.5894732 6.589267\n",
      " 6.5893526 6.590055  6.5893    6.5890303 6.5905757 6.5889435 6.5895457\n",
      " 6.5898566 6.5898204 6.5894465 6.5891814 6.5895205 6.589622  6.5889215\n",
      " 6.5896196 6.5895815 6.5901575 6.5895605], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 9s 89ms/step - loss: 1.0009e-05 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[6.7841825 6.7837453 6.7836285 6.7836676 6.78413   6.7840204 6.784034\n",
      " 6.7833986 6.7836347 6.7845435 6.783323  6.782782  6.783439  6.7845726\n",
      " 6.7837095 6.7838182 6.7843957 6.783235  6.7835712 6.783872  6.78321\n",
      " 6.782587  6.78309   6.783242  6.7832994 6.78471   6.7840223 6.7834826\n",
      " 6.7836275 6.7833333 6.782922  6.7840514], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 9s 88ms/step - loss: 9.5248e-06 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[6.918729  6.9188056 6.9185276 6.919646  6.918248  6.9197435 6.919217\n",
      " 6.918526  6.9197674 6.9188337 6.9191136 6.918653  6.918659  6.918509\n",
      " 6.9191    6.920061  6.919516  6.919804  6.9186816 6.9201503 6.918031\n",
      " 6.9195466 6.919073  6.9188957 6.91873   6.917702  6.9191437 6.9188933\n",
      " 6.918398  6.918338  6.919217  6.9192715], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 9s 87ms/step - loss: 8.6189e-06 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[7.0851483 7.084266  7.0848656 7.0844355 7.084569  7.083949  7.084345\n",
      " 7.0846405 7.0843196 7.0848737 7.083986  7.085323  7.0855103 7.0852575\n",
      " 7.0849047 7.0852923 7.085391  7.084952  7.0849624 7.0844936 7.0848255\n",
      " 7.0846753 7.085146  7.0849977 7.085213  7.0850782 7.0850163 7.085097\n",
      " 7.08534   7.0842547 7.08497   7.0845213], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 7.9360e-06 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[7.237428  7.238018  7.2384295 7.237968  7.2381334 7.2389956 7.238528\n",
      " 7.23819   7.23769   7.238772  7.237767  7.2377386 7.238578  7.237631\n",
      " 7.237862  7.237764  7.2378263 7.238296  7.2381945 7.238124  7.2380247\n",
      " 7.23839   7.2384667 7.239315  7.23846   7.23808   7.238279  7.237887\n",
      " 7.2381816 7.237873  7.238466  7.237677 ], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 9s 93ms/step - loss: 7.7321e-06 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[7.416905  7.416593  7.416243  7.4170866 7.4184484 7.4172163 7.4170866\n",
      " 7.4165907 7.416341  7.4177494 7.417234  7.416586  7.4176326 7.4168596\n",
      " 7.4171333 7.4179583 7.4174213 7.417078  7.416412  7.4169617 7.4167147\n",
      " 7.417916  7.417361  7.416831  7.4174623 7.416729  7.4172254 7.4176855\n",
      " 7.4167843 7.4173846 7.416569  7.416974 ], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 7.2119e-06 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[7.595518  7.595997  7.5959454 7.5960417 7.5980144 7.596638  7.5963383\n",
      " 7.596567  7.596433  7.596487  7.596964  7.5965366 7.597033  7.59612\n",
      " 7.5970745 7.597111  7.596738  7.5962806 7.596904  7.5956635 7.596331\n",
      " 7.5964155 7.5964837 7.596606  7.596887  7.596202  7.596209  7.597417\n",
      " 7.5962615 7.5957537 7.5968013 7.5963273], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 6.7261e-06 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[7.771435  7.770736  7.7704973 7.7710075 7.770992  7.771264  7.7706475\n",
      " 7.7716427 7.7709246 7.771617  7.7710505 7.7713976 7.7706532 7.7709947\n",
      " 7.770429  7.770816  7.770481  7.771288  7.7718806 7.7704105 7.7711124\n",
      " 7.771438  7.7704797 7.7708354 7.7702856 7.771055  7.77094   7.7706866\n",
      " 7.771109  7.770833  7.7712646 7.7709193], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 6.2432e-06 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[7.9706316 7.9711742 7.9714885 7.9711137 7.970274  7.970344  7.9705772\n",
      " 7.9709272 7.9718337 7.971097  7.9708943 7.970947  7.9706635 7.971015\n",
      " 7.971491  7.970902  7.970145  7.970611  7.9708714 7.970891  7.9705634\n",
      " 7.9706254 7.970834  7.970138  7.9699616 7.970573  7.971123  7.970997\n",
      " 7.9710994 7.9707026 7.97081   7.971418 ], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 9s 93ms/step - loss: 5.7807e-06 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[8.0979805 8.097284  8.097235  8.098333  8.096885  8.09794   8.097942\n",
      " 8.097343  8.097616  8.096823  8.09769   8.098908  8.097869  8.097742\n",
      " 8.09776   8.098085  8.097111  8.097408  8.097867  8.098246  8.097154\n",
      " 8.097415  8.097091  8.097189  8.097709  8.09808   8.096753  8.096953\n",
      " 8.097818  8.097629  8.097562  8.098008 ], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 5.4432e-06 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[8.297808  8.298236  8.298269  8.29788   8.297571  8.298046  8.297704\n",
      " 8.29734   8.298597  8.297772  8.297668  8.297756  8.29809   8.298044\n",
      " 8.299305  8.298192  8.298329  8.297952  8.296954  8.297705  8.298443\n",
      " 8.297885  8.29908   8.297064  8.297482  8.29891   8.297948  8.298142\n",
      " 8.297962  8.297716  8.297708  8.2975445], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 5.0235e-06 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[8.448201 8.449297 8.448093 8.448438 8.449063 8.448073 8.44865  8.448611\n",
      " 8.448694 8.448575 8.446909 8.448345 8.448597 8.449246 8.448348 8.449094\n",
      " 8.448886 8.448766 8.44857  8.448348 8.448797 8.449041 8.448354 8.448341\n",
      " 8.448346 8.448195 8.449318 8.448706 8.448036 8.448229 8.448194 8.448116], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 9s 91ms/step - loss: 4.7241e-06 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[8.639935 8.641027 8.640614 8.640426 8.639792 8.640236 8.640475 8.640119\n",
      " 8.639515 8.640158 8.639898 8.640218 8.639851 8.641395 8.639776 8.640716\n",
      " 8.639432 8.639314 8.640414 8.640146 8.640576 8.640775 8.640724 8.640772\n",
      " 8.641177 8.640442 8.640519 8.640476 8.640285 8.641136 8.640254 8.640754], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 4.5416e-06 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[8.8070755 8.807025  8.807389  8.806367  8.807725  8.807016  8.806396\n",
      " 8.807215  8.806232  8.807537  8.807185  8.806862  8.807315  8.808136\n",
      " 8.807369  8.808056  8.806947  8.807438  8.807512  8.807326  8.807566\n",
      " 8.807857  8.806577  8.807526  8.806886  8.80715   8.806873  8.806889\n",
      " 8.806831  8.807705  8.80716   8.8068075], shape=(32,), dtype=float32)\n",
      "100/100 [==============================] - 9s 91ms/step - loss: 4.1618e-06 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "train_loss: tf.Tensor(\n",
      "[8.999931  9.000691  9.000287  9.000733  9.0005865 9.00034   9.000789\n",
      " 9.001139  9.000045  9.000665  9.000209  8.999767  8.999683  9.000729\n",
      " 9.001014  9.00072   8.99943   9.000107  9.000606  9.000911  9.000538\n",
      " 9.00072   8.999708  9.000627  9.000622  9.000398  8.999963  9.001278\n",
      " 9.000557  9.000573  9.000436  8.999773 ], shape=(32,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "oset1 = OpenSetv1(4, log_classifier, 16, 0.05) \n",
    "oset1.train(train_data, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba73a43f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0daf3023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oset1.delta_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f9ec4e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 0.          6.259966    0.         11.224113    2.5894656   0.\n",
      "  6.1635675   0.22290488  5.6645617   3.8178217   0.          0.\n",
      "  0.         15.476871    6.0169325   3.0685852 ], shape=(16,), dtype=float32)\n",
      "(32, 16)\n",
      "tf.Tensor(\n",
      "[0.7856829  0.7856829  0.7856829  0.7856829  0.7883268  0.7883268\n",
      " 0.7856829  0.75074387 0.75074387 0.7856829  0.7856829  0.7883268\n",
      " 0.7811786  0.7811786  0.75074387 0.7883268  0.75074387 0.75074387\n",
      " 0.7883268  0.7856829  0.7811786  0.7811786  0.7883268  0.7811786\n",
      " 0.7856829  0.7856829  0.7856829  0.7856829  0.7811786  0.7856829\n",
      " 0.7811786  0.75074387], shape=(32,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[32.91107   30.6134    32.908154  32.138863  13.5897665 39.78266\n",
      " 29.866282  33.700134  26.544718  32.53153   32.89298   30.3351\n",
      " 31.98238   33.19979   35.272285  36.403904  32.645847  23.419264\n",
      " 36.30073   32.34609   32.280228  33.19979   35.581886  32.180763\n",
      " 32.908154  30.674261  30.227377  32.55604   33.19979   32.908154\n",
      " 32.15925   39.581768 ], shape=(32,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True], shape=(32,), dtype=bool)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
       "array([1, 1, 1, 1, 3, 3, 1, 2, 2, 1, 1, 3, 0, 0, 2, 3, 2, 2, 3, 1, 0, 0,\n",
       "       3, 0, 1, 1, 1, 1, 0, 1, 0, 2], dtype=int64)>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_batch = oset.model(t_batch_x, extract_feature=True)\n",
    "logits = euclidean_metric(features_batch, oset.centroids)\n",
    "smax = tf.nn.softmax(logits, )\n",
    "preds = tf.math.argmax(smax, axis=1)\n",
    "probs = tf.reduce_max(smax, 1)\n",
    "c = tf.gather(oset.centroids, indices=preds)\n",
    "print(c[0])\n",
    "print(c.shape)\n",
    "d = tf.gather(oset.delta, indices=preds)\n",
    "d = tf.reshape(d, d.shape[0])\n",
    "print(d)\n",
    "euc_dis = tf.norm(features_batch - c, ord='euclidean', axis=1) \n",
    "print(euc_dis)\n",
    "print(euc_dis >=d)\n",
    "preds[euc_dis >=d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e97895d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'boundary_loss_1/Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
      "array([[0.71801656],\n",
      "       [0.7913795 ],\n",
      "       [0.697228  ],\n",
      "       [0.76913875]], dtype=float32)>, <tf.Variable 'boundary_loss_2/Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
      "array([[0.7494542],\n",
      "       [0.7539585],\n",
      "       [0.7190195],\n",
      "       [0.7566024]], dtype=float32)>, <tf.Variable 'boundary_loss_2/Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
      "array([[0.76799905],\n",
      "       [0.7725034 ],\n",
      "       [0.73756427],\n",
      "       [0.77514726]], dtype=float32)>, <tf.Variable 'boundary_loss_2/Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
      "array([[0.7811786 ],\n",
      "       [0.7856829 ],\n",
      "       [0.75074387],\n",
      "       [0.7883268 ]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(oset.delta_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fbb71b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab8a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4bb2d4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d0e43348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8e91f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172f490f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff889a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b28bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d762dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "acb11537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       " array([1.6548097, 1.6548097, 1.6548097, 1.6548097, 1.6931441, 1.6931441,\n",
       "        1.6548097, 1.6362237, 1.6362237, 1.6548097, 1.6548097, 1.6931441,\n",
       "        1.6754997, 1.6754997, 1.6362237, 1.6931441, 1.6362237, 1.6362237,\n",
       "        1.6931441, 1.6548097, 1.6754997, 1.6754997, 1.6931441, 1.6754997,\n",
       "        1.6548097, 1.6548097, 1.6548097, 1.6548097, 1.6754997, 1.6548097,\n",
       "        1.6754997, 1.6362237], dtype=float32)>,\n",
       " <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       " array([1.6548097, 1.6548097, 1.6548097, 1.6548097, 1.6931441, 1.6931441,\n",
       "        1.6548097, 1.6362237, 1.6362237, 1.6548097, 1.6548097, 1.6931441,\n",
       "        1.6754997, 1.6754997, 1.6362237, 1.6931441, 1.6362237, 1.6362237,\n",
       "        1.6931441, 1.6548097, 1.6754997, 1.6754997, 1.6931441, 1.6754997,\n",
       "        1.6548097, 1.6548097, 1.6548097, 1.6548097, 1.6754997, 1.6548097,\n",
       "        1.6754997, 1.6362237], dtype=float32)>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @tf.function\n",
    "def check_gradient(t_batch_x, t_batch_y, centroids,tr_loss=0):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "    with tf.GradientTape() as tape:                \n",
    "        features_batch = log_classifier(t_batch_x, extract_feature=True)\n",
    "        loss, delta = criterion_boundary(features_batch, centroids, t_batch_y)\n",
    "        tr_loss += loss\n",
    "        gradients = tape.gradient(loss, [delta])    \n",
    "        optimizer.apply_gradients(zip(gradients, [delta]))\n",
    "#     optimizer.minimize(gradients, var_list=[delta])\n",
    "    return loss, tr_loss\n",
    "check_gradient(t_batch_x, t_batch_y, centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9728ac02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
       " array([[-0.05825601],\n",
       "        [-0.01609268],\n",
       "        [ 0.02103348],\n",
       "        [-0.09494998]], dtype=float32)>]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion_boundary.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "806cecd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
       "array([[-0.05825601],\n",
       "       [-0.01609268],\n",
       "       [ 0.02103348],\n",
       "       [-0.09494998]], dtype=float32)>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion_boundary.delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "38cb07de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
       " array([[-0.05825601],\n",
       "        [-0.01609268],\n",
       "        [ 0.02103348],\n",
       "        [-0.09494998]], dtype=float32)>]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion_boundary.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f97e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff179e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e356361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a824bd50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9012db5",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "NotImplementedError                       Traceback (most recent call last)\n",
    "C:\\Users\\BHUJAY~1\\AppData\\Local\\Temp/ipykernel_24404/2131960519.py in <module>\n",
    "----> 1 oset.train(train_data)\n",
    "\n",
    "C:\\Users\\BHUJAY~1\\AppData\\Local\\Temp/ipykernel_24404/3835535165.py in train(self, data_train, epochs)\n",
    "     35                 logseq_batch, label_batch = batch\n",
    "     36                 ## (32, 32, 64), (32, 4)\n",
    "---> 37                 batch_loss, t_loss = self.train_step(criterion_boundary, \n",
    "     38                                                      logseq_batch, label_batch)\n",
    "     39                 tr_loss += t_loss\n",
    "\n",
    "~\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\n",
    "    778       else:\n",
    "    779         compiler = \"nonXla\"\n",
    "--> 780         result = self._call(*args, **kwds)\n",
    "    781 \n",
    "    782       new_tracing_count = self._get_tracing_count()\n",
    "\n",
    "~\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in _call(self, *args, **kwds)\n",
    "    821       # This is the first call of __call__, so we have to initialize.\n",
    "    822       initializers = []\n",
    "--> 823       self._initialize(args, kwds, add_initializers_to=initializers)\n",
    "    824     finally:\n",
    "    825       # At this point we know that the initialization is complete (or less\n",
    "\n",
    "~\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in _initialize(self, args, kwds, add_initializers_to)\n",
    "    694     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)\n",
    "    695     self._concrete_stateful_fn = (\n",
    "--> 696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n",
    "    697             *args, **kwds))\n",
    "    698 \n",
    "\n",
    "~\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\n",
    "   2853       args, kwargs = None, None\n",
    "   2854     with self._lock:\n",
    "-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)\n",
    "   2856     return graph_function\n",
    "   2857 \n",
    "\n",
    "~\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _maybe_define_function(self, args, kwargs)\n",
    "   3211 \n",
    "   3212       self._function_cache.missed.add(call_context_key)\n",
    "-> 3213       graph_function = self._create_graph_function(args, kwargs)\n",
    "   3214       self._function_cache.primary[cache_key] = graph_function\n",
    "   3215       return graph_function, args, kwargs\n",
    "\n",
    "~\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\n",
    "   3063     arg_names = base_arg_names + missing_arg_names\n",
    "   3064     graph_function = ConcreteFunction(\n",
    "-> 3065         func_graph_module.func_graph_from_py_func(\n",
    "   3066             self._name,\n",
    "   3067             self._python_function,\n",
    "\n",
    "~\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\n",
    "    984         _, original_func = tf_decorator.unwrap(python_func)\n",
    "    985 \n",
    "--> 986       func_outputs = python_func(*func_args, **func_kwargs)\n",
    "    987 \n",
    "    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\n",
    "\n",
    "~\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in wrapped_fn(*args, **kwds)\n",
    "    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give\n",
    "    599         # the function a weak reference to itself to avoid a reference cycle.\n",
    "--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
    "    601     weak_wrapped_fn = weakref.ref(wrapped_fn)\n",
    "    602 \n",
    "\n",
    "~\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in bound_method_wrapper(*args, **kwargs)\n",
    "   3733     # However, the replacer is still responsible for attaching self properly.\n",
    "   3734     # TODO(mdan): Is it possible to do it here instead?\n",
    "-> 3735     return wrapped_fn(*args, **kwargs)\n",
    "   3736   weak_bound_method_wrapper = weakref.ref(bound_method_wrapper)\n",
    "   3737 \n",
    "\n",
    "~\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py in wrapper(*args, **kwargs)\n",
    "    971           except Exception as e:  # pylint:disable=broad-except\n",
    "    972             if hasattr(e, \"ag_error_metadata\"):\n",
    "--> 973               raise e.ag_error_metadata.to_exception(e)\n",
    "    974             else:\n",
    "    975               raise\n",
    "\n",
    "NotImplementedError: in user code:\n",
    "\n",
    "    C:\\Users\\BHUJAY~1\\AppData\\Local\\Temp/ipykernel_24404/3835535165.py:50 train_step  *\n",
    "        loss, self.delta = criterion_boundary(features_batch,\n",
    "    C:\\Users\\Bhujay_ROG\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__  **\n",
    "        outputs = call_fn(inputs, *args, **kwargs)\n",
    "    C:\\Users\\Bhujay_ROG\\MyDev\\OCLog\\oclog\\boundary_loss.py:34 call  **\n",
    "        logits =  euclidean_metric(features, centroids)\n",
    "    C:\\Users\\Bhujay_ROG\\MyDev\\OCLog\\oclog\\boundary_loss.py:12 euclidean_metric\n",
    "        a = np.expand_dims(a, 1)\n",
    "    <__array_function__ internals>:5 expand_dims\n",
    "        \n",
    "    C:\\Users\\Bhujay_ROG\\anaconda3\\envs\\env3\\lib\\site-packages\\numpy\\lib\\shape_base.py:591 expand_dims\n",
    "        a = asanyarray(a)\n",
    "    C:\\Users\\Bhujay_ROG\\anaconda3\\envs\\env3\\lib\\site-packages\\numpy\\core\\_asarray.py:136 asanyarray\n",
    "        return array(a, dtype, copy=False, order=order, subok=True)\n",
    "    C:\\Users\\Bhujay_ROG\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:845 __array__\n",
    "        raise NotImplementedError(\n",
    "\n",
    "    NotImplementedError: Cannot convert a symbolic Tensor (log_classifier/log_seq_encoder/dense/Relu:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "var1 = tf.Variable(10.0)\n",
    "loss = lambda: (var1 ** 2)/2.0       # d(loss)/d(var1) == var1\n",
    "step_count = opt.minimize(loss, [var1]).numpy()\n",
    "# The first step is `-learning_rate*sign(grad)`\n",
    "var1.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3412f7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tqdm() Progress Bar: 100%|██████████| 20/20 [00:10<00:00,  1.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "for i in tqdm(range(20), desc = 'tqdm() Progress Bar'):\n",
    "    time.sleep(0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
