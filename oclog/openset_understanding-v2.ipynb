{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9cfd3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(123)\n",
    "from BGL.bglog import BGLog, get_embedding_layer\n",
    "from pretraining import LogLineEncoder, LogSeqEncoder, LogClassifier\n",
    "from boundary_loss import euclidean_metric, BoundaryLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b434fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "bglog = BGLog(save_padded_num_sequences=False, load_from_pkl=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76718f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded_num_seq_df loaded from data\\bgl_padded_num_seq_df.pkl\n",
      "trained tokenizer, tk, loaded from data\\bgltk.pkl\n",
      "train_0:, 800\n",
      "test_0:, 200\n",
      "train_1:, 800\n",
      "test_1:, 200\n",
      "train_2:, 800\n",
      "test_2:, 200\n",
      "train_3:, 800\n",
      "test_3:, 102\n",
      "4 class does not have 800 records, it has only 628 records\n",
      "test_4:, 0\n",
      "5 class does not have 800 records, it has only 165 records\n",
      "5 class does not have 200 records, it has only 165 records\n",
      "6 class does not have 800 records, it has only 75 records\n",
      "6 class does not have 200 records, it has only 75 records\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "<BatchDataset shapes: ((32, 32, 64), (32, 4)), types: (tf.int32, tf.float32)>\n",
      "<BatchDataset shapes: ((32, 32, 64), (32, 4)), types: (tf.int32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "train_test = bglog.get_tensor_train_test(ablation=1000)\n",
    "train_data, test_data = train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e43a4559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 50\n",
      "sample_x_train.shape: (32, 32, 64)\n",
      "loglineEmbedding.shape: (32, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "line_encoder =   LogLineEncoder(bglog, chars_in_line=64)\n",
    "# the model doesn't have a state unless it is called at least once\n",
    "# in order to initialize the model we need a sample data \n",
    "sample_train_data = next(iter(train_data))\n",
    "sample_x_train = sample_train_data[0]\n",
    "print('sample_x_train.shape:', sample_x_train.shape)\n",
    "# now we will initialize the model with the sample data\n",
    "loglineEmbedding = line_encoder(sample_x_train)\n",
    "print('loglineEmbedding.shape:', loglineEmbedding.shape)\n",
    "# Now the model have a state and can be inspected        \n",
    "# line_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94d72225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logSeqEmbedding.shape: (32, 16)\n"
     ]
    }
   ],
   "source": [
    "logSeqencer =   LogSeqEncoder(line_in_seq=32)\n",
    "# the model doesn't have a state unless it is called at least once\n",
    "logSeqEmbedding = logSeqencer(loglineEmbedding)\n",
    "print('logSeqEmbedding.shape:', logSeqEmbedding.shape)\n",
    "# Now the model have a state and can be inspected        \n",
    "# logSeqencer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "081a4660",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_classifier = LogClassifier(line_encoder=line_encoder, seq_encoder=logSeqencer, num_classes=4)\n",
    "# log_classifier(sample_x_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae415ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df1d7cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method LogLineEncoder.call of <pretraining.LogLineEncoder object at 0x000001D072D54220>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method LogLineEncoder.call of <pretraining.LogLineEncoder object at 0x000001D072D54220>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method LogSeqEncoder.call of <pretraining.LogSeqEncoder object at 0x000001D072D54130>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method LogSeqEncoder.call of <pretraining.LogSeqEncoder object at 0x000001D072D54130>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['log_seq_encoder/conv1d_3/kernel:0', 'log_seq_encoder/conv1d_3/bias:0', 'log_seq_encoder/conv1d_4/kernel:0', 'log_seq_encoder/conv1d_4/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['log_seq_encoder/conv1d_3/kernel:0', 'log_seq_encoder/conv1d_3/bias:0', 'log_seq_encoder/conv1d_4/kernel:0', 'log_seq_encoder/conv1d_4/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['log_seq_encoder/conv1d_3/kernel:0', 'log_seq_encoder/conv1d_3/bias:0', 'log_seq_encoder/conv1d_4/kernel:0', 'log_seq_encoder/conv1d_4/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['log_seq_encoder/conv1d_3/kernel:0', 'log_seq_encoder/conv1d_3/bias:0', 'log_seq_encoder/conv1d_4/kernel:0', 'log_seq_encoder/conv1d_4/bias:0'] when minimizing the loss.\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 0.3961 - accuracy: 0.8656 - precision: 0.9614 - recall: 0.7619 - val_loss: 0.0231 - val_accuracy: 0.9985 - val_precision: 0.9985 - val_recall: 0.9940\n"
     ]
    }
   ],
   "source": [
    "log_classifier.compile(optimizer='adam', \n",
    "                  loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "hist = log_classifier.fit(train_data, validation_data=test_data, epochs=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95ea3e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_classifier(sample_x_train)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aa507f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4decad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f60bd0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenSet:\n",
    "    ''' \n",
    "    self.num_labels = number of classes\n",
    "    self.embedding_size = number of neurons in the logits layers of the pretrained model'''\n",
    "    def __init__(self, num_labels, pretrained_model, embedding_size,\n",
    "                lr_boundary):\n",
    "#         super().__init__():\n",
    "        self.model = pretrained_model        \n",
    "        self.centroids = None\n",
    "        self.num_labels = num_labels\n",
    "        self.embedding_size = embedding_size\n",
    "        self.delta = None\n",
    "        self.lr_boundary = lr_boundary\n",
    "        self.delta_points = []\n",
    "        \n",
    "    \n",
    "    def train(self, data_train, epochs=1):\n",
    "        criterion_boundary = BoundaryLoss(num_labels=self.num_labels)\n",
    "        # delta is getting calculated inside the  BoundaryLoss class as well\n",
    "        # however that calculated delta is used for calculating the loss \n",
    "        # that delta is not updating the criterion_boundary.delta which is \n",
    "        # a randomly initialized parameter. \n",
    "        # Hence the following softplus is on randomly initialized trainable parameters\n",
    "        # and not softplus on softplus\n",
    "        self.delta = tf.nn.softplus(criterion_boundary.delta)\n",
    "        self.centroids = self.centroids_cal(data_train)        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.lr_boundary) # does it take criterion_boundary.parameters() ??\n",
    "        wait = 0\n",
    "        best_delta, best_centroids = None, None\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            tr_loss = 0\n",
    "            nb_tr_examples, nb_tr_steps = 0, 0\n",
    "            for batch in data_train:\n",
    "                logseq_batch, label_batch = batch\n",
    "                ## (32, 32, 64), (32, 4)\n",
    "                batch_loss, t_loss = self.train_step(criterion_boundary, \n",
    "                                                     logseq_batch, label_batch, optimizer)\n",
    "                tr_loss += t_loss\n",
    "                nb_tr_steps += 1\n",
    "                \n",
    "            self.delta_points.append(self.delta)\n",
    "            loss = tr_loss / nb_tr_steps\n",
    "            print('train_loss:', loss)  \n",
    "                    \n",
    "#     @tf.function                \n",
    "    def train_step(self, criterion_boundary, logseq_batch, label_batch, optimizer):\n",
    "#         print('within train_step')\n",
    "        tr_loss = 0\n",
    "        with tf.GradientTape() as tape:                \n",
    "            features_batch = self.model(logseq_batch, extract_feature=True)\n",
    "            loss, self.delta = criterion_boundary(features_batch, \n",
    "                                                  self.centroids, \n",
    "                                                  label_batch)\n",
    "        tr_loss += loss\n",
    "        gradients = tape.gradient(loss, [self.delta])\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, [self.delta]))\n",
    "        return loss, tr_loss                    \n",
    "        \n",
    "        \n",
    "        \n",
    "    def centroids_cal(self, data):\n",
    "        centroids = tf.zeros((self.num_labels, self.embedding_size))\n",
    "        total_labels = tf.zeros(self.num_labels)\n",
    "        for batch in data:\n",
    "            logseq_batch, label_batch = batch\n",
    "            ## (32, 32, 64), (32, 4)\n",
    "            features = self.model(logseq_batch, extract_feature=True)\n",
    "            ## (32, 16) features - 32 sequence of line each haaving 64 characrers\n",
    "            ## produces a feaure vector of dimension 16. \n",
    "            for i in range(len(label_batch)): # (32, 4) --> here length is 32\n",
    "                label = label_batch[i] # label looks like [0 0 0 1]\n",
    "                numeric_label = np.argmax(label) # index position of the label = 3 , so it is actually class =3\n",
    "                ##total_labels = [0 0 0 0] each col representing a class \n",
    "                ## count the number for each class\n",
    "                total_labels_lst = tf.unstack(total_labels)\n",
    "                total_labels_lst[numeric_label] += 1 \n",
    "                total_labels = tf.stack(total_labels_lst)\n",
    "                centroids_lst = tf.unstack(centroids)\n",
    "                centroids_lst[numeric_label] += features[i]\n",
    "                centroids = tf.stack(centroids_lst)\n",
    "                # each row index in the centroid array is a class\n",
    "                # we add first identify the feature belonging to which class by the numeric_label\n",
    "                # Then add all the features belonging to the class in the corresponding row of the centroid arr\n",
    "        ### shape of centroids is (4, 16) whereas shape of total_labels is (1, 4)\n",
    "        ### reshape the total_labels as 4,1 ==> [[0], [0], [0], [0]]==> 4 rows \n",
    "        ## so that we can divide the centroids array by the total_labels\n",
    "        total_label_reshaped = tf.reshape(total_labels, (self.num_labels, 1))\n",
    "        centroids /= total_label_reshaped\n",
    "        return centroids  \n",
    "        \n",
    "        def openpredict(self, features):\n",
    "            logits = euclidean_metric(features, self.centroids)\n",
    "            ####original line in pytorch ###probs, preds = F.softmax(logits.detach(), dim = 1).max(dim = 1)\n",
    "            smax = tf.nn.softmax(logits, )\n",
    "            preds = tf.math.argmax(smax, axis=1)\n",
    "            probs = tf.reduce_max(smax, 1)            \n",
    "            #######euc_dis = torch.norm(features - self.centroids[preds], 2, 1).view(-1)\n",
    "            euc_dis = tf.norm(features - self.centroids[preds], ord='euclidean', axis=1) \n",
    "            \n",
    "            \n",
    "            #preds[euc_dis >= self.delta[preds]] = data.unseen_token_id\n",
    "\n",
    "            return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaa85993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, self.delta = criterion_boundary(features_batch,\n",
    "# logits =  euclidean_metric(features, centroids)\n",
    "# NotImplementedError: Cannot convert a symbolic Tensor (log_classifier/log_seq_encoder/dense/Relu:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported \n",
    "# it looks like the numpy arrays to be converted to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fdaa97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "oset = OpenSet(4, log_classifier, 16, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8c019d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "031f6124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.apply_gradients(zip(gradients, self.delta))\n",
    "# TypeError: 'IndexedSlices' object is not iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36e9373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_batch = next(iter(train_data))\n",
    "t_batch_x, t_batch_y = t_batch\n",
    "t_batch_x.shape\n",
    "centroids = oset.centroids_cal(train_data)\n",
    "features_batch = log_classifier(t_batch_x, extract_feature=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb1e0b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_boundary = BoundaryLoss(num_labels=4)\n",
    "loss, delta = criterion_boundary(features_batch, centroids, t_batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "589efef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       "array([1.6548097, 1.6548097, 1.6548097, 1.6548097, 1.6931441, 1.6931441,\n",
       "       1.6548097, 1.6362237, 1.6362237, 1.6548097, 1.6548097, 1.6931441,\n",
       "       1.6754997, 1.6754997, 1.6362237, 1.6931441, 1.6362237, 1.6362237,\n",
       "       1.6931441, 1.6548097, 1.6754997, 1.6754997, 1.6931441, 1.6754997,\n",
       "       1.6548097, 1.6548097, 1.6548097, 1.6548097, 1.6754997, 1.6548097,\n",
       "       1.6754997, 1.6362237], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8b5b838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'boundary_loss/Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
       "array([[0.6644433],\n",
       "       [0.6851332],\n",
       "       [0.7037192],\n",
       "       [0.6467987]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8757006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_loss=0\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "with tf.GradientTape() as tape:                \n",
    "    features_batch = log_classifier(t_batch_x, extract_feature=True)\n",
    "    loss, delta = criterion_boundary(features_batch, centroids, t_batch_y)\n",
    "tr_loss += loss\n",
    "gradients = tape.gradient(loss, [delta])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57079707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
       "array([[0.6644433],\n",
       "       [0.6851332],\n",
       "       [0.7037192],\n",
       "       [0.6467987]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = tf.Variable(delta)\n",
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21c6e9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.framework.indexed_slices.IndexedSlices at 0x1d06eb84760>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed344f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.minimize(loss, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fbf7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.apply_gradients(zip(gradients, delta))\n",
    "#### AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_in_graph_mode'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2edc124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def check_gradient(t_batch_x, t_batch_y, centroids):\n",
    "    tr_loss=0\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "    with tf.GradientTape() as tape:                \n",
    "        features_batch = log_classifier(t_batch_x, extract_feature=True)\n",
    "        loss, delta = criterion_boundary(features_batch, centroids, t_batch_y)\n",
    "    tr_loss += loss\n",
    "    gradients = tape.gradient(loss, [delta])    \n",
    "    optimizer.apply_gradients(zip(gradients, [delta]))\n",
    "#     optimizer.minimize(gradients, var_list=[delta])\n",
    "    return loss, tr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a412462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       " array([1.6548097, 1.6548097, 1.6548097, 1.6548097, 1.6931441, 1.6931441,\n",
       "        1.6548097, 1.6362237, 1.6362237, 1.6548097, 1.6548097, 1.6931441,\n",
       "        1.6754997, 1.6754997, 1.6362237, 1.6931441, 1.6362237, 1.6362237,\n",
       "        1.6931441, 1.6548097, 1.6754997, 1.6754997, 1.6931441, 1.6754997,\n",
       "        1.6548097, 1.6548097, 1.6548097, 1.6548097, 1.6754997, 1.6548097,\n",
       "        1.6754997, 1.6362237], dtype=float32)>,\n",
       " <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       " array([1.6548097, 1.6548097, 1.6548097, 1.6548097, 1.6931441, 1.6931441,\n",
       "        1.6548097, 1.6362237, 1.6362237, 1.6548097, 1.6548097, 1.6931441,\n",
       "        1.6754997, 1.6754997, 1.6362237, 1.6931441, 1.6362237, 1.6362237,\n",
       "        1.6931441, 1.6548097, 1.6754997, 1.6754997, 1.6931441, 1.6754997,\n",
       "        1.6548097, 1.6548097, 1.6548097, 1.6548097, 1.6754997, 1.6548097,\n",
       "        1.6754997, 1.6362237], dtype=float32)>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_gradient(t_batch_x, t_batch_y, centroids)\n",
    "###AttributeError: 'Tensor' object has no attribute '_in_graph_mode'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83df12ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: tf.Tensor(\n",
      "[1.7890495 1.7961293 1.7964641 1.8004185 1.7929924 1.7974381 1.795191\n",
      " 1.7931007 1.7890009 1.7863306 1.7911556 1.7913154 1.7929295 1.7916319\n",
      " 1.793261  1.7944844 1.7919563 1.7936623 1.7925035 1.79865   1.7968493\n",
      " 1.8006352 1.7996067 1.7871317 1.7890861 1.7963542 1.7880127 1.7968636\n",
      " 1.7952195 1.7908095 1.7912313 1.7910193], shape=(32,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "oset.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89662ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'boundary_loss_1/Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
       " array([[0.71801656],\n",
       "        [0.7913795 ],\n",
       "        [0.697228  ],\n",
       "        [0.76913875]], dtype=float32)>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oset.delta_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca6a839f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ -67.01952     -3.525845  -109.583534  -160.43402  ]\n",
      " [ -79.82242     -2.1222196  -93.719376  -131.56161  ]\n",
      " [-184.88982    -24.641348  -132.53224   -195.33246  ]\n",
      " [ -76.3061      -2.9903688 -107.254425  -152.44286  ]\n",
      " [-119.22389   -140.64474    -50.575027   -47.487274 ]\n",
      " [-222.22762   -185.70552   -101.49874     -6.754195 ]\n",
      " [ -70.32373     -1.5167881  -95.82221   -140.16188  ]\n",
      " [-140.18436    -95.62632     -3.5865545  -59.446598 ]\n",
      " [-119.264595   -92.42897     -4.27982    -86.548355 ]\n",
      " [ -65.43387     -3.5297403 -108.33574   -157.64508  ]\n",
      " [ -65.415955    -3.8230033 -111.58875   -161.621    ]\n",
      " [-180.8993    -161.67134    -77.23272     -0.7322664]\n",
      " [  -2.8020918  -88.394     -161.12628   -210.3276   ]\n",
      " [  -4.171529   -83.65649    -98.34264   -135.52994  ]\n",
      " [-154.31055   -103.04908     -7.6279254  -41.967087 ]\n",
      " [-209.51389   -179.3525     -93.75351     -4.171521 ]\n",
      " [-135.17406    -90.617714    -0.8169305  -77.32392  ]\n",
      " [-141.30368   -106.51258    -15.940049   -23.444386 ]\n",
      " [-202.55185   -177.32387    -95.83757     -2.9559226]\n",
      " [ -65.92119     -3.5019805 -109.425064  -159.5612   ]\n",
      " [  -2.9463463  -88.81685   -162.48105   -211.64554  ]\n",
      " [  -4.171529   -83.65649    -98.34264   -135.52994  ]\n",
      " [-208.68044   -177.49672    -94.12228     -3.7914007]\n",
      " [  -2.938864   -89.27342   -162.53915   -210.3732   ]\n",
      " [-184.88982    -24.641348  -132.53224   -195.33246  ]\n",
      " [ -76.804245    -2.3747244 -112.30319   -162.9024   ]\n",
      " [ -67.77647     -1.7361434  -93.95012   -137.69157  ]\n",
      " [ -64.39933     -3.5298269 -108.14549   -158.5847   ]\n",
      " [  -4.171529   -83.65649    -98.34264   -135.52994  ]\n",
      " [-184.88982    -24.641348  -132.53224   -195.33246  ]\n",
      " [  -4.2661357  -86.24663   -107.086655  -133.88907  ]\n",
      " [-146.59216    -97.25247     -5.1762     -77.54037  ]], shape=(32, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "logits = euclidean_metric(features_batch, oset.centroids)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b05553d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "smax = tf.nn.softmax(logits, )\n",
    "preds = tf.math.argmax(smax, axis=1)\n",
    "probs = tf.reduce_max(smax, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "838c6504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: tf.Tensor([1 1 1 1 3 3 1 2 2 1 1 3 0 0 2 3 2 2 3 1 0 0 3 0 1 1 1 1 0 1 0 2], shape=(32,), dtype=int64)\n",
      "preds: tf.Tensor(\n",
      "[1.         1.         1.         1.         0.9563847  1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99944955\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.        ], shape=(32,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('preds:', preds)\n",
    "print('preds:', probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "05ae4633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oset.centroids: tf.Tensor(\n",
      "[[0.0000000e+00 4.2018835e-02 0.0000000e+00 7.5073738e+00 2.9848788e+00\n",
      "  0.0000000e+00 5.9337478e+00 3.0970883e+00 4.7078938e+00 5.1352601e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.4536023e+01 3.4672394e+00\n",
      "  6.4453125e+00]\n",
      " [0.0000000e+00 6.2599664e+00 0.0000000e+00 1.1224109e+01 2.5894647e+00\n",
      "  0.0000000e+00 6.1635613e+00 2.2290489e-01 5.6645598e+00 3.8178229e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.5476877e+01 6.0169353e+00\n",
      "  3.0685852e+00]\n",
      " [0.0000000e+00 4.1064219e+00 0.0000000e+00 1.0502010e+01 2.1169436e+00\n",
      "  0.0000000e+00 6.6549835e+00 1.9764199e-03 3.4076446e-01 6.0818281e-02\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.7405222e+01 8.4182948e-02\n",
      "  9.9504930e-01]\n",
      " [0.0000000e+00 1.7120099e+00 0.0000000e+00 1.5485305e+01 6.2190514e-02\n",
      "  0.0000000e+00 2.0054388e+00 3.6714032e-02 1.1385172e-01 1.2439347e-01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.7535822e+01 1.5839231e-01\n",
      "  4.3307877e+00]], shape=(4, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('oset.centroids:',oset.centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d099a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroids are having only 4 rows , whereas labels are rows equivallent to batch\n",
    "        # pick-up the centroid for each class \n",
    "        # label_index from the data set will have all the classes, 32 for a batch\n",
    "        # for each class cetroid[class_index] will give the centroid of the calss\n",
    "        # it is basically : [centroids[class_idx] for class_idx in label_indexes]\n",
    "#         c = centroids[label_indexs]\n",
    "#         c = tf.gather(centroids, indices=label_indexs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b2d1980",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.gather(oset.centroids, indices=preds)\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bc94f833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[1.8777233 1.456784  4.964005  1.7292683 6.891102  2.5988834 1.2315795\n",
      " 1.89382   2.0687726 1.8787605 1.9552501 0.8557257 1.673945  2.042432\n",
      " 2.76187   2.0424302 0.903842  3.992499  1.7192796 1.8713579 1.7164923\n",
      " 2.042432  1.9471519 1.7143115 4.964005  1.5410141 1.3176279 1.8787833\n",
      " 2.042432  4.964005  2.0654626 2.2751262], shape=(32,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "euc_dis = tf.norm(features_batch - c, ord='euclidean', axis=1) \n",
    "print(euc_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0d3f1b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = tf.gather(oset.delta_points, indices=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "67bb0615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 32), dtype=bool, numpy=\n",
       "array([[[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True]]])>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euc_dis >=oset.delta_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb07526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6972376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb73cdd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e5a15f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab35f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917a0bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0644ea78",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "NotImplementedError                       Traceback (most recent call last)\n",
    "C:\\Users\\BHUJAY~1\\AppData\\Local\\Temp/ipykernel_24404/2131960519.py in <module>\n",
    "----> 1 oset.train(train_data)\n",
    "\n",
    "C:\\Users\\BHUJAY~1\\AppData\\Local\\Temp/ipykernel_24404/3835535165.py in train(self, data_train, epochs)\n",
    "     35                 logseq_batch, label_batch = batch\n",
    "     36                 ## (32, 32, 64), (32, 4)\n",
    "---> 37                 batch_loss, t_loss = self.train_step(criterion_boundary, \n",
    "     38                                                      logseq_batch, label_batch)\n",
    "     39                 tr_loss += t_loss\n",
    "\n",
    "~\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\n",
    "    778       else:\n",
    "    779         compiler = \"nonXla\"\n",
    "--> 780         result = self._call(*args, **kwds)\n",
    "    781 \n",
    "    782       new_tracing_count = self._get_tracing_count()\n",
    "\n",
    "~\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in _call(self, *args, **kwds)\n",
    "    821       # This is the first call of __call__, so we have to initialize.\n",
    "    822       initializers = []\n",
    "--> 823       self._initialize(args, kwds, add_initializers_to=initializers)\n",
    "    824     finally:\n",
    "    825       # At this point we know that the initialization is complete (or less\n",
    "\n",
    "~\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in _initialize(self, args, kwds, add_initializers_to)\n",
    "    694     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)\n",
    "    695     self._concrete_stateful_fn = (\n",
    "--> 696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n",
    "    697             *args, **kwds))\n",
    "    698 \n",
    "\n",
    "~\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\n",
    "   2853       args, kwargs = None, None\n",
    "   2854     with self._lock:\n",
    "-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)\n",
    "   2856     return graph_function\n",
    "   2857 \n",
    "\n",
    "~\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _maybe_define_function(self, args, kwargs)\n",
    "   3211 \n",
    "   3212       self._function_cache.missed.add(call_context_key)\n",
    "-> 3213       graph_function = self._create_graph_function(args, kwargs)\n",
    "   3214       self._function_cache.primary[cache_key] = graph_function\n",
    "   3215       return graph_function, args, kwargs\n",
    "\n",
    "~\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\n",
    "   3063     arg_names = base_arg_names + missing_arg_names\n",
    "   3064     graph_function = ConcreteFunction(\n",
    "-> 3065         func_graph_module.func_graph_from_py_func(\n",
    "   3066             self._name,\n",
    "   3067             self._python_function,\n",
    "\n",
    "~\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\n",
    "    984         _, original_func = tf_decorator.unwrap(python_func)\n",
    "    985 \n",
    "--> 986       func_outputs = python_func(*func_args, **func_kwargs)\n",
    "    987 \n",
    "    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\n",
    "\n",
    "~\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in wrapped_fn(*args, **kwds)\n",
    "    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give\n",
    "    599         # the function a weak reference to itself to avoid a reference cycle.\n",
    "--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
    "    601     weak_wrapped_fn = weakref.ref(wrapped_fn)\n",
    "    602 \n",
    "\n",
    "~\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in bound_method_wrapper(*args, **kwargs)\n",
    "   3733     # However, the replacer is still responsible for attaching self properly.\n",
    "   3734     # TODO(mdan): Is it possible to do it here instead?\n",
    "-> 3735     return wrapped_fn(*args, **kwargs)\n",
    "   3736   weak_bound_method_wrapper = weakref.ref(bound_method_wrapper)\n",
    "   3737 \n",
    "\n",
    "~\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py in wrapper(*args, **kwargs)\n",
    "    971           except Exception as e:  # pylint:disable=broad-except\n",
    "    972             if hasattr(e, \"ag_error_metadata\"):\n",
    "--> 973               raise e.ag_error_metadata.to_exception(e)\n",
    "    974             else:\n",
    "    975               raise\n",
    "\n",
    "NotImplementedError: in user code:\n",
    "\n",
    "    C:\\Users\\BHUJAY~1\\AppData\\Local\\Temp/ipykernel_24404/3835535165.py:50 train_step  *\n",
    "        loss, self.delta = criterion_boundary(features_batch,\n",
    "    C:\\Users\\Bhujay_ROG\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__  **\n",
    "        outputs = call_fn(inputs, *args, **kwargs)\n",
    "    C:\\Users\\Bhujay_ROG\\MyDev\\OCLog\\oclog\\boundary_loss.py:34 call  **\n",
    "        logits =  euclidean_metric(features, centroids)\n",
    "    C:\\Users\\Bhujay_ROG\\MyDev\\OCLog\\oclog\\boundary_loss.py:12 euclidean_metric\n",
    "        a = np.expand_dims(a, 1)\n",
    "    <__array_function__ internals>:5 expand_dims\n",
    "        \n",
    "    C:\\Users\\Bhujay_ROG\\anaconda3\\envs\\env3\\lib\\site-packages\\numpy\\lib\\shape_base.py:591 expand_dims\n",
    "        a = asanyarray(a)\n",
    "    C:\\Users\\Bhujay_ROG\\anaconda3\\envs\\env3\\lib\\site-packages\\numpy\\core\\_asarray.py:136 asanyarray\n",
    "        return array(a, dtype, copy=False, order=order, subok=True)\n",
    "    C:\\Users\\Bhujay_ROG\\anaconda3\\envs\\env3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:845 __array__\n",
    "        raise NotImplementedError(\n",
    "\n",
    "    NotImplementedError: Cannot convert a symbolic Tensor (log_classifier/log_seq_encoder/dense/Relu:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5688e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "var1 = tf.Variable(10.0)\n",
    "loss = lambda: (var1 ** 2)/2.0       # d(loss)/d(var1) == var1\n",
    "step_count = opt.minimize(loss, [var1]).numpy()\n",
    "# The first step is `-learning_rate*sign(grad)`\n",
    "var1.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f41b763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
